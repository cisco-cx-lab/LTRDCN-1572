{
    "docs": [
        {
            "location": "/", 
            "text": "LTRDCN-1572\n\n\nWelcome to Cisco Live LTRDCN-1572: VXLAN EVPN Fabric and automation using Ansible.\n\n\nFor full documentation visit \nCisco Live\n.\n\n\nSpeakers\n\n\n\n\nFaisal Chaudhry \nPrincipal Architect, Cisco Advanced Services\n\n\nLei Tian \nSolutions Architect, Cisco Advanced Services", 
            "title": "Home"
        }, 
        {
            "location": "/#ltrdcn-1572", 
            "text": "Welcome to Cisco Live LTRDCN-1572: VXLAN EVPN Fabric and automation using Ansible.  For full documentation visit  Cisco Live .", 
            "title": "LTRDCN-1572"
        }, 
        {
            "location": "/#speakers", 
            "text": "Faisal Chaudhry  Principal Architect, Cisco Advanced Services  Lei Tian  Solutions Architect, Cisco Advanced Services", 
            "title": "Speakers"
        }, 
        {
            "location": "/intro/", 
            "text": "VXLAN\n\n\nVXLAN stands for Virtual Extensible Local Area Network. VXLAN is a L2 overlay scheme on top of L3 network or we can say it is a L2 in layer 3 tunnel. It runs over the existing networks and provides the means to stretch the L2 network. Only VMs within the same VXLAN segment can communicate with each other. Each VXLAN segment is identified by a 24 bit segment ID called \u201cVXLAN Network Identifier (VNI)\u201d.  This help overcome 4094 VLAN scale limitation and able to extend it to 224 segments.\n\n\nVXLAN uses BGP as its control plane for Overlay. It makes it forwarding decisions at VTEPs (Virtual tunnel end points) for layer-2 and layer-3. Forwarding happens based on MAC or IP learnt via control plane (MP-BGP EVPN) . VXLAN uses IGP, PIM and BGP as its underlay in the fabric. \n\n\nBelow are some of the terminologies that will be used in the lab:\n\n\n\n\nVNI / VNID\n \u2013 VXLAN Network Identifier. This replaces VLAN ID \n\n\nVTEP\n \u2013 VXLAN Tunnel End Point.\n\n\nThis is the end point where the box performs VXLAN encap / decap\nThis could be physical HW (Nexus9k) or Virtual (Nexus 1000v, Nexus 9000v)\n\n\n\n\n\n\nVXLAN Segment\n -  The resulting layer 2 overlay network\n\n\nVXLAN Gateway\n \u2013 It is a device that forwards traffic between VXLANS. It can be both L2 and L3 forwarding\n\n\nNVE\n \u2013 Network Virtualization Edge\n\n\nNVE is tunnel interface. It represents VTEP\n\n\n\n\n\n\n\n\nAnsible\n\n\nAnsible is an agentless open source software that can be used for configuration management, deployment and orchestration of deployment. The scripts in Ansible are called playbooks; playbook is in YAML format that was desgiened to be easy for humans to read and write. Playbooks include one or more plays, each play include one or more tasks. Each task is associated with one module, which is what gets executed in the playbook. Modules are python scripts that ship with Ansible installation. During the lab, you will be introduced to multiple NXOS modules and ansible template module. \n\n\nYou can find all Ansible modules documentation at below url:\n\nhttp://docs.ansible.com/ansible/latest/list_of_all_modules.html\n\n\nBelow are some of the terminologies that will be used in the lab:\n\n\n\n\nHost\n: remote machines that Ansible manages  \n\n\nGroup\n: several hosts that can be configured together and share common verables \n\n\nInventory\n: file descripts hosts and groups in Ansible.\n\n\nVariable\n: names of value (int, str, dic, list) referenced in playbook or template\n\n\nYAML\n: data format for Playbook or Variables in Ansible \n\n\nPlaybook\n: the script to orchestrate, automate, deploy system in Ansible. One playbook can include multiple plays. \n\n\nRoles\n: group of tasks, templates to implement specific behavior\n\n\nJinja2\n: a Python based tempting language\n\n\n\n\n\n\nAbout this lab\n\n\nAs a standardized overlay technology, multiple vendors have adopted VXLAN as datacenter solution to provide scalability and allow layer 2 across IP network. MP-BPG EVPN as VXLAN control plane protocol provides a robust scalable solution to overcome the limitation in VXLAN flood and learn mode.\n\n\nAs an open source automation tool, Ansible provides the same framework for network administrators to automate network infrastructure as the rest IT organization. \n\n\nThis lab demostates the possibility of using Ansible to automate datacenter VXLAN fabric day 1 provisiong and day 2 operations. \n\n\nLab Flow\n\n\nLab guide will walk the attendees through the below activities:\n\n\n\n\nAnsible installation \n\n\nAnsible playbook \n\n\nDay 1 automation using Ansible \n\n\nDay 2 automation using Ansible \n\n\nDay 0 automation \n\n\nCode upgrade using Ansible\n\n\n\n\nLab Access\n\n\nBelow table provides the IP addresses and credentials for the devices used in this lab: \n\n\n\n\n\n\n\n\nSpine-1\n\n\n198.18.133.33:1030\n\n\nadmin/C1sco12345\n\n\n\n\n\n\n\n\n\n\nSpine-2\n\n\n198.18.133.33:1040\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nLeaf-1\n\n\n198.18.133.33:1050\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nLeaf-3\n\n\n198.18.133.33:1070\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nLeaf-4\n\n\n198.18.1333.33:1080\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nServer-1\n\n\n198.18.134.50\n\n\nroot/C1sco12345\n\n\n\n\n\n\nServer-3\n\n\n198.18.134.52\n\n\nroot/C1sco12345\n\n\n\n\n\n\nServer-4\n\n\n198.18.134.53\n\n\nroot/C1sco12345\n\n\n\n\n\n\nAnsible Server\n\n\n198.18.134.150\n\n\nroot/C1sco12345\n\n\n\n\n\n\nDCNM\n\n\n198.18.134.200\n\n\nadmin/C1sco12345\n\n\n\n\n\n\nRemote Workstation\n\n\n198.18.133.36\n\n\ndemouser/C1sco12345\n\n\n\n\n\n\n\n\nLab topology\n\n\nBelow picture shows the lab topology:", 
            "title": "Introduction"
        }, 
        {
            "location": "/intro/#vxlan", 
            "text": "VXLAN stands for Virtual Extensible Local Area Network. VXLAN is a L2 overlay scheme on top of L3 network or we can say it is a L2 in layer 3 tunnel. It runs over the existing networks and provides the means to stretch the L2 network. Only VMs within the same VXLAN segment can communicate with each other. Each VXLAN segment is identified by a 24 bit segment ID called \u201cVXLAN Network Identifier (VNI)\u201d.  This help overcome 4094 VLAN scale limitation and able to extend it to 224 segments.  VXLAN uses BGP as its control plane for Overlay. It makes it forwarding decisions at VTEPs (Virtual tunnel end points) for layer-2 and layer-3. Forwarding happens based on MAC or IP learnt via control plane (MP-BGP EVPN) . VXLAN uses IGP, PIM and BGP as its underlay in the fabric.   Below are some of the terminologies that will be used in the lab:   VNI / VNID  \u2013 VXLAN Network Identifier. This replaces VLAN ID   VTEP  \u2013 VXLAN Tunnel End Point.  This is the end point where the box performs VXLAN encap / decap\nThis could be physical HW (Nexus9k) or Virtual (Nexus 1000v, Nexus 9000v)    VXLAN Segment  -  The resulting layer 2 overlay network  VXLAN Gateway  \u2013 It is a device that forwards traffic between VXLANS. It can be both L2 and L3 forwarding  NVE  \u2013 Network Virtualization Edge  NVE is tunnel interface. It represents VTEP", 
            "title": "VXLAN"
        }, 
        {
            "location": "/intro/#ansible", 
            "text": "Ansible is an agentless open source software that can be used for configuration management, deployment and orchestration of deployment. The scripts in Ansible are called playbooks; playbook is in YAML format that was desgiened to be easy for humans to read and write. Playbooks include one or more plays, each play include one or more tasks. Each task is associated with one module, which is what gets executed in the playbook. Modules are python scripts that ship with Ansible installation. During the lab, you will be introduced to multiple NXOS modules and ansible template module.   You can find all Ansible modules documentation at below url: http://docs.ansible.com/ansible/latest/list_of_all_modules.html  Below are some of the terminologies that will be used in the lab:   Host : remote machines that Ansible manages    Group : several hosts that can be configured together and share common verables   Inventory : file descripts hosts and groups in Ansible.  Variable : names of value (int, str, dic, list) referenced in playbook or template  YAML : data format for Playbook or Variables in Ansible   Playbook : the script to orchestrate, automate, deploy system in Ansible. One playbook can include multiple plays.   Roles : group of tasks, templates to implement specific behavior  Jinja2 : a Python based tempting language", 
            "title": "Ansible"
        }, 
        {
            "location": "/intro/#about-this-lab", 
            "text": "As a standardized overlay technology, multiple vendors have adopted VXLAN as datacenter solution to provide scalability and allow layer 2 across IP network. MP-BPG EVPN as VXLAN control plane protocol provides a robust scalable solution to overcome the limitation in VXLAN flood and learn mode.  As an open source automation tool, Ansible provides the same framework for network administrators to automate network infrastructure as the rest IT organization.   This lab demostates the possibility of using Ansible to automate datacenter VXLAN fabric day 1 provisiong and day 2 operations.", 
            "title": "About this lab"
        }, 
        {
            "location": "/intro/#lab-flow", 
            "text": "Lab guide will walk the attendees through the below activities:   Ansible installation   Ansible playbook   Day 1 automation using Ansible   Day 2 automation using Ansible   Day 0 automation   Code upgrade using Ansible", 
            "title": "Lab Flow"
        }, 
        {
            "location": "/intro/#lab-access", 
            "text": "Below table provides the IP addresses and credentials for the devices used in this lab:      Spine-1  198.18.133.33:1030  admin/C1sco12345      Spine-2  198.18.133.33:1040  admin/C1sco12345    Leaf-1  198.18.133.33:1050  admin/C1sco12345    Leaf-3  198.18.133.33:1070  admin/C1sco12345    Leaf-4  198.18.1333.33:1080  admin/C1sco12345    Server-1  198.18.134.50  root/C1sco12345    Server-3  198.18.134.52  root/C1sco12345    Server-4  198.18.134.53  root/C1sco12345    Ansible Server  198.18.134.150  root/C1sco12345    DCNM  198.18.134.200  admin/C1sco12345    Remote Workstation  198.18.133.36  demouser/C1sco12345", 
            "title": "Lab Access"
        }, 
        {
            "location": "/intro/#lab-topology", 
            "text": "Below picture shows the lab topology:", 
            "title": "Lab topology"
        }, 
        {
            "location": "/Task1-ansible-node/", 
            "text": "Your first task will be to build an Ansible node on a server running redhat CentOS operating system.  At the end of this task, you will have a fully operational Ansible node.\n\n\nStep 1: Connect to lab using anyconnect VPN\n\n\nYou will connect to \ndcloud-rtp-anyconnect.cisco.com\n using Cisco VPN AnyConnect client, as shown in below picture, with the username and password provided by the lab admin.\n\n\nNote:\n lab admin will furnish the credentials information to the participant.  If you don't have this information please ask the lab speakers.\n\n\n\n\nStep 2: Enter VPN credentials\n\n\nAfter prompted for credentials, use the credentials provided by the lab admin.    \n\n\u2022   Below is an example of user logging into a reference POD:\n\n\n\n\n\n\nHit accept when the prompt appears to accept the VPN connection login    \n\n\n\n\n\n\nStep 3: RDP to workstation\n\n\nIn this step, you will connect to the workstation with RDP client on your machines.  Use below details for this RDP session:\n\n\n\n\nWorkstation: \n198.18.133.36\n\n\nUsername: \ndcloud\\demouser\n\n\nPassword: \nC1sco12345\n\n\n\n\nBelow screenshot is only an example for this RDP connection:\n\n\n\n\nStep 4: MTputty\n\n\nOnce you have the RDP session to the remote workstation, then you will use MTputty client to connect to all devices in this lab.  \n\n\nMTputty is already installed on the Desktop of the workstation where you connected using RDP.  Run this application by clicking on the icon on the desktop:\n\n\n\n\nStep 5: SSH into Ansible node\n\n\nSSH into Ansible node (198.18.134.150) by double clicking the Ansible icon on the left pan with username \nroot\n and password \nC1sco12345\n\n\n\n\nStep 6: Verify Python\n\n\nOnce successfully SSH into the ansible node, the very first thing we are going to do after logging into Ansible server is verify the python version by running \npython --version\n command - as shown below:\n\n\n[root@rhel7-tools ~]# python --version\nPython 2.7.5\n\n\n\nIt is an important step as we need minimum 2.7.5 version of python in order to install some features for ansbile.  The output of above command confirms this version. \n\n\nAnsible can be run from any machine with Python 2 (versions 2.6 or 2.7) or Python 3 (versions 3.5 and higher) installed. \n\n\n\n\nStep 7: Install PIP\n\n\nAfter verifying we have the minimum version of python installed, we are now going to Install PIP python package using \neasy_install pip\n command as shown below:\n\n\n[root@rhel7-tools ~]# easy_install pip\nSearching for pip\nBest match: pip 8.1.1\nAdding pip 8.1.1 to easy-install.pth file\nInstalling pip script to /usr/bin\nInstalling pip3.5 script to /usr/bin\nInstalling pip3 script to /usr/bin\n\nUsing /usr/lib/python2.7/site-packages\nProcessing dependencies for pip\nFinished processing dependencies for pip\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\nNext, update pip to latest version by executing below command:\n\n\n[root@rhel7-tools ~]# pip install --upgrade pip\n\n\n\nBelow screenshot shows the exection of above command:\n\n\n\n\nAfter installing PIP package, we are going to add the relevant packages that are needed for this Ansible based VXLAN lab. Below are the packages required for this lab.  \n\n\n\n\nParamiko\n\n\nPyYAML\n\n\nJinj2\n\n\nHttplib2\n\n\n\n\nRun below command to install these packages:\n\n\n[root@rhel7-tools ~]# pip install paramiko PyYAML jinja2 httplib2\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\nAs a final step, we are going to install Ansible on this RHEL. Once the install is initiated with the below command, it will take few minutes for it to download and install.\n\n\n[root@rhel7-tools ~]# pip install ansible==2.8.0b1\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\nStep 8: Verify Ansible\n\n\nAfter installation is complete, check Ansible version by executing command \nansible --version\n, as shown below:\n\n\n[root@rhel7-tools ~]# ansible --version\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\nStep 9: Create Ansible Inventory\n\n\nNow, we are going to create inventory, host variables and Configuration file. This is important as Ansible works  against multiple systems in the system by selecting portions of systems listed in Ansible inventory. Similarly, configuration settings in Ansible are adjustable via configuration file.\n\n\nCreate folder named LTRDCN-1572 as working environment and verify that it\u2019s empty:\n\n\n[root@rhel7-tools ~]# mkdir LTRDCN-1572 \n cd LTRDCN-1572\n[root@rhel7-tools LTRDCN-1572]# ls\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\nNext:\n\n\n\n\nCreate Ansible inventory file to include Spine and Leaf switches. \n\n\nBy default Ansible has inventory file saved in location /etc/ansible/hosts. \n\n\n\n\nIn this lab we will create hosts file in the working environment. You may use \nvi\n or \nvim\n to create inventory file \nhosts\n.  Below this file is created from ansible host prompt using \ncat\n command.  \nNote:\n you can copy and paste starting from cat till EOF (as shown in below screenshot):\n\n\n~~~\ncat \n EOF \n hosts\n\n\ndefine global variables, groups and host variables\n\n\n[all:vars]\nansible_connection = local\nuser=admin\npwd=C1sco12345\ngather_fact=no\n[jinja2_spine]\n198.18.4.202\n[jinja2_leaf]\n198.18.4.104\n[spine]\n198.18.4.201\n[leaf]\n198.18.4.101\n198.18.4.103\n[server]\n198.18.134.50 eth1=172.21.140.10 gw=172.21.140.1\n198.18.134.52 eth1=172.21.140.11 gw=172.21.140.1\n198.18.134.53 eth1=172.21.141.11 gw=172.21.141.1\nEOF\n~~~\n\n\n\n\n\n\nBelow screenshot shows the output of above command:\n\n\n\n\n\n\nNow you may verify the content of this file using below command:\n[root@rhel7-tools LTRDCN-1572]# more hosts\n\n\n\n\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\n\n\n\nCreate Ansible config (ansible.cfg) using the same steps as above.  \nNote:\n you can copy and paste starting from cat till EOF (as shown in below screenshot):\n\n\ncat \n EOF \n ansible.cfg\n[defaults]\ninventory = hosts\nhost_key_checking = false\nrecord_host_key = true\nstdout_callback = debug\ndeprecation_warnings = False\nEOF\n\n\n\n\n\n\n\nNow you may verify the content of this file using below command:\n\n\n[root@rhel7-tools LTRDCN-1572]# more ansible.cfg\n\n\n\n\n\n\n\nBelow screenshot shows the output of above commands:\n\n\n\n\n\n\n\n\nDo \nls\n to verify the file that you just created under project folder LTRDCN-1572.   \n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\n\n\n\n\nCreate host variable folder named host_vars in folder LTRDCN-1572. Host varilables can be placed in different places in Ansible. In this lab, we will use host_vars for host variables:\n\n\n[root@rhel7-tools LTRDCN-1572]# mkdir host_vars \n cd host_vars\n\n\n\n\n\n\n\nCreate host variable file for each host in inventory by using the cat command.  The variable file for a switch is created using the below cat command.  \nNote:\n you can copy and paste starting from cat till EOF (as shown in below screenshot).  \nNote:\n the spaces in the file are important so do not remove those:\n\n\n~~~\ncat \n EOF \n 198.18.4.101.yml\n\n\n\n\nhostname: leaf_1\n  loopback0: 192.168.0.8\n  loopback1: 192.168.0.18\n  router_id: 192.168.0.8\nEOF\n~~~\n\n\n\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\n\n\nNow you may verify the content of this file using below command:\n[root@rhel7-tools LTRDCN-1572]# more 198.18.4.101.yml\n\n\n\n\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\n\n\n\n\nCreate a new host variable file for next host in inventory.   The variable file for a switch is created using the below cat command.  \nNote:\n you can copy and paste starting from cat till EOF (as shown in below screenshot).  \nNote:\n the spaces in the file are important so do not remove those:\n\n\n~~~ YAML\ncat \n EOF \n 198.18.4.103.yml\n\n\n\n\nhostname: leaf_3\n  loopback0: 192.168.0.10\n  loopback1: 192.168.0.110\n  router_id: 192.168.0.10\nEOF\n~~~\n\n\n\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\n\n\nNow you may verify the content of this file using below command:\n[root@rhel7-tools LTRDCN-1572]# more 198.18.4.103.yml\n\n\n\n\n\n\n\nBelow screenshot shows the execution of above command:\n\n\n\n\n\n\n\n\nCreate a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command.  \nNote:\n you can copy and paste starting from cat till EOF (as shown in below screenshot).  \nNote:\n the spaces in the file are important so do not remove those:\n\n\n~~~ YAML\ncat \n EOF \n 198.18.4.104.yml\n\n\n\n\nhostname: leaf_4\n  loopback0: 192.168.0.11\n  loopback1: 192.168.0.111\n  router_id: 192.168.0.11\nEOF\n~~~\n\n\n\n\n\n\nNow you may verify the content of this file using below command:\n\n\n[root@rhel7-tools LTRDCN-1572]# more 198.18.4.104.yml\n\n\n\n\n\n\n\nBelow screenshot shows the execution of above commands:\n\n\n\n\n\n\n\n\nCreate a new host variable file for next host in inventory.  The variable file for a switch is created using the below cat command.  \nNote:\n you can copy and paste starting from cat till EOF (as shown in below screenshot).  \nNote:\n the spaces in the file are important so do not remove those:\n\n\n~~~ YAML\ncat \n EOF \n 198.18.4.201.yml\n\n\n\n\nhostname: spine-1\n  loopback0: 192.168.0.6\n  loopback1: 192.168.0.100\n  router_id: 192.168.0.6\nEOF\n~~~\n\n\n\n\n\n\nNow you may verify the content of this file using below command:\n\n\n[root@rhel7-tools LTRDCN-1572]# more 198.18.4.201.yml\n\n\n\n\n\n\n\nBelow screenshot shows the execution of above commands:\n\n\n\n\n\n\n\n\nCreate a new host variable file for next host in inventory.  The variable file for a switch is created using the below cat command.  \nNote:\n you can copy and paste starting from cat till EOF (as shown in below screenshot).  \nNote:\n the spaces in the file are important so do not remove those:\n\n\n~~~ YAML\ncat \n EOF \n 198.18.4.202.yml\n\n\n\n\nhostname: spine-2\n  loopback0: 192.168.0.7\n  loopback1: 192.168.0.100\n  router_id: 192.168.0.7\nEOF\n~~~\n\n\n\n\n\n\nNow you may verify the content of this file using below command:\n\n\n[root@rhel7-tools LTRDCN-1572]# more 198.18.4.202.yml\n\n\n\n\n\n\n\nBelow screenshot shows the execution of above commands:\n\n\n\n\nStep 10: Ansible role structure\n\n\nRole is very useful technique to manage a set of playbooks in Ansible. \nIn this lab, we will use two different playbooks to manage configuration for Spine and Leaf switches. \n\n\nWe will use role structure and manage the two plays into single playbook. \nA role directory structure contains several directories of defaults, vars, files, handlers, meta, tasks and templates. \n\n\nIn this lab:\n\n\n\n\nwe will use vars, templates and tasks folders \n/vars\n \n\n\nmain.yml file in vars folder contains dictionary of variables for this role \n/tasks\n\n\nmain.yml file in tasks folder contains the Ansible playbook for this role\n\n\n\n\nTo proceed further in the new:\n\n\n\u2022   Create roles directory in folder LTRDCN-1572 by issuing below commands:\n\n\n[root@rhel7-tools LTRDCN-1572]# cd /root/LTRDCN-1572\n[root@rhel7-tools LTRDCN-1572]# mkdir roles\n\n\n\nThis will be used in the lab later.", 
            "title": "Installation"
        }, 
        {
            "location": "/Task1-ansible-node/#step-1-connect-to-lab-using-anyconnect-vpn", 
            "text": "You will connect to  dcloud-rtp-anyconnect.cisco.com  using Cisco VPN AnyConnect client, as shown in below picture, with the username and password provided by the lab admin.  Note:  lab admin will furnish the credentials information to the participant.  If you don't have this information please ask the lab speakers.", 
            "title": "Step 1: Connect to lab using anyconnect VPN"
        }, 
        {
            "location": "/Task1-ansible-node/#step-2-enter-vpn-credentials", 
            "text": "After prompted for credentials, use the credentials provided by the lab admin.     \n\u2022   Below is an example of user logging into a reference POD:    Hit accept when the prompt appears to accept the VPN connection login", 
            "title": "Step 2: Enter VPN credentials"
        }, 
        {
            "location": "/Task1-ansible-node/#step-3-rdp-to-workstation", 
            "text": "In this step, you will connect to the workstation with RDP client on your machines.  Use below details for this RDP session:   Workstation:  198.18.133.36  Username:  dcloud\\demouser  Password:  C1sco12345   Below screenshot is only an example for this RDP connection:", 
            "title": "Step 3: RDP to workstation"
        }, 
        {
            "location": "/Task1-ansible-node/#step-4-mtputty", 
            "text": "Once you have the RDP session to the remote workstation, then you will use MTputty client to connect to all devices in this lab.    MTputty is already installed on the Desktop of the workstation where you connected using RDP.  Run this application by clicking on the icon on the desktop:", 
            "title": "Step 4: MTputty"
        }, 
        {
            "location": "/Task1-ansible-node/#step-5-ssh-into-ansible-node", 
            "text": "SSH into Ansible node (198.18.134.150) by double clicking the Ansible icon on the left pan with username  root  and password  C1sco12345", 
            "title": "Step 5: SSH into Ansible node"
        }, 
        {
            "location": "/Task1-ansible-node/#step-6-verify-python", 
            "text": "Once successfully SSH into the ansible node, the very first thing we are going to do after logging into Ansible server is verify the python version by running  python --version  command - as shown below:  [root@rhel7-tools ~]# python --version\nPython 2.7.5  It is an important step as we need minimum 2.7.5 version of python in order to install some features for ansbile.  The output of above command confirms this version.   Ansible can be run from any machine with Python 2 (versions 2.6 or 2.7) or Python 3 (versions 3.5 and higher) installed.", 
            "title": "Step 6: Verify Python"
        }, 
        {
            "location": "/Task1-ansible-node/#step-7-install-pip", 
            "text": "After verifying we have the minimum version of python installed, we are now going to Install PIP python package using  easy_install pip  command as shown below:  [root@rhel7-tools ~]# easy_install pip\nSearching for pip\nBest match: pip 8.1.1\nAdding pip 8.1.1 to easy-install.pth file\nInstalling pip script to /usr/bin\nInstalling pip3.5 script to /usr/bin\nInstalling pip3 script to /usr/bin\n\nUsing /usr/lib/python2.7/site-packages\nProcessing dependencies for pip\nFinished processing dependencies for pip  Below screenshot shows the execution of above command:   Next, update pip to latest version by executing below command:  [root@rhel7-tools ~]# pip install --upgrade pip  Below screenshot shows the exection of above command:   After installing PIP package, we are going to add the relevant packages that are needed for this Ansible based VXLAN lab. Below are the packages required for this lab.     Paramiko  PyYAML  Jinj2  Httplib2   Run below command to install these packages:  [root@rhel7-tools ~]# pip install paramiko PyYAML jinja2 httplib2  Below screenshot shows the execution of above command:   As a final step, we are going to install Ansible on this RHEL. Once the install is initiated with the below command, it will take few minutes for it to download and install.  [root@rhel7-tools ~]# pip install ansible==2.8.0b1  Below screenshot shows the execution of above command:", 
            "title": "Step 7: Install PIP"
        }, 
        {
            "location": "/Task1-ansible-node/#step-8-verify-ansible", 
            "text": "After installation is complete, check Ansible version by executing command  ansible --version , as shown below:  [root@rhel7-tools ~]# ansible --version  Below screenshot shows the execution of above command:", 
            "title": "Step 8: Verify Ansible"
        }, 
        {
            "location": "/Task1-ansible-node/#step-9-create-ansible-inventory", 
            "text": "Now, we are going to create inventory, host variables and Configuration file. This is important as Ansible works  against multiple systems in the system by selecting portions of systems listed in Ansible inventory. Similarly, configuration settings in Ansible are adjustable via configuration file.  Create folder named LTRDCN-1572 as working environment and verify that it\u2019s empty:  [root@rhel7-tools ~]# mkdir LTRDCN-1572   cd LTRDCN-1572\n[root@rhel7-tools LTRDCN-1572]# ls  Below screenshot shows the execution of above command:   Next:   Create Ansible inventory file to include Spine and Leaf switches.   By default Ansible has inventory file saved in location /etc/ansible/hosts.    In this lab we will create hosts file in the working environment. You may use  vi  or  vim  to create inventory file  hosts .  Below this file is created from ansible host prompt using  cat  command.   Note:  you can copy and paste starting from cat till EOF (as shown in below screenshot):  ~~~\ncat   EOF   hosts", 
            "title": "Step 9: Create Ansible Inventory"
        }, 
        {
            "location": "/Task1-ansible-node/#define-global-variables-groups-and-host-variables", 
            "text": "[all:vars]\nansible_connection = local\nuser=admin\npwd=C1sco12345\ngather_fact=no\n[jinja2_spine]\n198.18.4.202\n[jinja2_leaf]\n198.18.4.104\n[spine]\n198.18.4.201\n[leaf]\n198.18.4.101\n198.18.4.103\n[server]\n198.18.134.50 eth1=172.21.140.10 gw=172.21.140.1\n198.18.134.52 eth1=172.21.140.11 gw=172.21.140.1\n198.18.134.53 eth1=172.21.141.11 gw=172.21.141.1\nEOF\n~~~    Below screenshot shows the output of above command:    Now you may verify the content of this file using below command: [root@rhel7-tools LTRDCN-1572]# more hosts    Below screenshot shows the execution of above command:    Create Ansible config (ansible.cfg) using the same steps as above.   Note:  you can copy and paste starting from cat till EOF (as shown in below screenshot):  cat   EOF   ansible.cfg\n[defaults]\ninventory = hosts\nhost_key_checking = false\nrecord_host_key = true\nstdout_callback = debug\ndeprecation_warnings = False\nEOF    Now you may verify the content of this file using below command:  [root@rhel7-tools LTRDCN-1572]# more ansible.cfg    Below screenshot shows the output of above commands:     Do  ls  to verify the file that you just created under project folder LTRDCN-1572.     Below screenshot shows the execution of above command:     Create host variable folder named host_vars in folder LTRDCN-1572. Host varilables can be placed in different places in Ansible. In this lab, we will use host_vars for host variables:  [root@rhel7-tools LTRDCN-1572]# mkdir host_vars   cd host_vars    Create host variable file for each host in inventory by using the cat command.  The variable file for a switch is created using the below cat command.   Note:  you can copy and paste starting from cat till EOF (as shown in below screenshot).   Note:  the spaces in the file are important so do not remove those:  ~~~\ncat   EOF   198.18.4.101.yml   hostname: leaf_1\n  loopback0: 192.168.0.8\n  loopback1: 192.168.0.18\n  router_id: 192.168.0.8\nEOF\n~~~    Below screenshot shows the execution of above command:    Now you may verify the content of this file using below command: [root@rhel7-tools LTRDCN-1572]# more 198.18.4.101.yml    Below screenshot shows the execution of above command:     Create a new host variable file for next host in inventory.   The variable file for a switch is created using the below cat command.   Note:  you can copy and paste starting from cat till EOF (as shown in below screenshot).   Note:  the spaces in the file are important so do not remove those:  ~~~ YAML\ncat   EOF   198.18.4.103.yml   hostname: leaf_3\n  loopback0: 192.168.0.10\n  loopback1: 192.168.0.110\n  router_id: 192.168.0.10\nEOF\n~~~    Below screenshot shows the execution of above command:    Now you may verify the content of this file using below command: [root@rhel7-tools LTRDCN-1572]# more 198.18.4.103.yml    Below screenshot shows the execution of above command:     Create a new host variable file for next host in inventory. The variable file for a switch is created using the below cat command.   Note:  you can copy and paste starting from cat till EOF (as shown in below screenshot).   Note:  the spaces in the file are important so do not remove those:  ~~~ YAML\ncat   EOF   198.18.4.104.yml   hostname: leaf_4\n  loopback0: 192.168.0.11\n  loopback1: 192.168.0.111\n  router_id: 192.168.0.11\nEOF\n~~~    Now you may verify the content of this file using below command:  [root@rhel7-tools LTRDCN-1572]# more 198.18.4.104.yml    Below screenshot shows the execution of above commands:     Create a new host variable file for next host in inventory.  The variable file for a switch is created using the below cat command.   Note:  you can copy and paste starting from cat till EOF (as shown in below screenshot).   Note:  the spaces in the file are important so do not remove those:  ~~~ YAML\ncat   EOF   198.18.4.201.yml   hostname: spine-1\n  loopback0: 192.168.0.6\n  loopback1: 192.168.0.100\n  router_id: 192.168.0.6\nEOF\n~~~    Now you may verify the content of this file using below command:  [root@rhel7-tools LTRDCN-1572]# more 198.18.4.201.yml    Below screenshot shows the execution of above commands:     Create a new host variable file for next host in inventory.  The variable file for a switch is created using the below cat command.   Note:  you can copy and paste starting from cat till EOF (as shown in below screenshot).   Note:  the spaces in the file are important so do not remove those:  ~~~ YAML\ncat   EOF   198.18.4.202.yml   hostname: spine-2\n  loopback0: 192.168.0.7\n  loopback1: 192.168.0.100\n  router_id: 192.168.0.7\nEOF\n~~~    Now you may verify the content of this file using below command:  [root@rhel7-tools LTRDCN-1572]# more 198.18.4.202.yml    Below screenshot shows the execution of above commands:", 
            "title": "define global variables, groups and host variables"
        }, 
        {
            "location": "/Task1-ansible-node/#step-10-ansible-role-structure", 
            "text": "Role is very useful technique to manage a set of playbooks in Ansible. \nIn this lab, we will use two different playbooks to manage configuration for Spine and Leaf switches.   We will use role structure and manage the two plays into single playbook. \nA role directory structure contains several directories of defaults, vars, files, handlers, meta, tasks and templates.   In this lab:   we will use vars, templates and tasks folders  /vars    main.yml file in vars folder contains dictionary of variables for this role  /tasks  main.yml file in tasks folder contains the Ansible playbook for this role   To proceed further in the new:  \u2022   Create roles directory in folder LTRDCN-1572 by issuing below commands:  [root@rhel7-tools LTRDCN-1572]# cd /root/LTRDCN-1572\n[root@rhel7-tools LTRDCN-1572]# mkdir roles  This will be used in the lab later.", 
            "title": "Step 10: Ansible role structure"
        }, 
        {
            "location": "/task2-first-ansible/", 
            "text": "In this section, we will create the first Ansible Playbook. In the playbook, we will configure VLANs on leaf switches, and assign VLANs to the server facing port. \n\n\nYou will learn create variables inside playbook, learn simple loop using \u201c\nwith_items\n\u201d, learn simple logical using \u201c\nwhen\n\u201d and learn \u201c\ntags\n\u201d to isolate tasks from whole playbook. \n\n\n\n\nStep 1: Using \"Atom\" text Editor\n\n\n\n\n\n\nOpen \u201cAtom\u201d text editor by double click the icon on desktop. Atom is the recommended text editor for this lab:\n\n\n\n\n\n\n\n\nAfter opening ATOM, \nClick\n \nNo, Never\n to the \u201cRegister as default atom:// URI handlre\u201d message as show below:\n\n\n\n\n\n\n\n\n\n\nStep 2: Atom folder\n\n\n\n\nAfter opening ATOM, there should be a folder in the left pane named \u201cLTRDCN-1572.\u201d\n\n\n\n\nRight click\n the pre-configured project folder \nLTRDCN-1572\n and select \nNew File\n \n\n\n\n\n\n\n\n\nName the new file \nvlan_provision.yml\n and hit enter. This will create the new file:\n\n\n\n\n\n\n\n\nAlso, on the lower bar of the ATOM, verify that  file grammar of \nYAML\n is selected instead of default \"\nPlain Text\n\".  If \nYAML\n is not selected, then you should \nchoose\n it from the listed options.\n\n\n\n\n\n\n\n\nStep 3: Define variables, tasks for playbook\n\n\nIn this step, we are going to define the scope, variable for playbook and tasks \n\n\n\n\nIn the opened window for \u2018vlan_provision.yml\u2019 file, enter the below content.\n\n\nNOTE: YAML is space sensitive. Hence be careful with the spaces in below section. \n\n\n\n\n\n\n\n\n---                     \n#Task 2: Simple playbook assign VLAN to server facing port\n  - hosts: leaf:jinja2_leaf\n    vars:\n      nxos_provider:\n        username: \n{{ user }}\n\n        password: \n{{ pwd }}\n\n        transport: nxapi\n        host: \n{{ inventory_hostname }}\n\n\n\n\n\nNote:\n\n\n\n\n\u201c\nhosts:\n\u201d defines the scope of this playbook applies to all switches in group \u2018leaf\u2019 and \u2018jinja2_leaf\u2019 (within the \n\"hosts\"\n file created in pervious task). \n\n\nNote\n that you can review the IP addresses of the three (2) \u201cleaf\u201d and one (1) \u201cjinja2_leaf\u201d in the \u201c\nhosts\n\u201d file (configured in previous steps).  The IP addresses are:\n\n\njinja2_leaf: 198.18.4.104\n\n\nleaf: 198.18.4.101\n\n\nleaf: 198.18.4.103\n\n\n\n\n\n\n\n\n\n\n\u201c\nvars\n\u201d defines one variable \u201c\nnxos_provider\n\u201d that will be used in this playbook. \n\n\n\u201c\nnxos_provider\n\u201d is variable that includes all value that will be used for connection and autnentication. \n\n\nThis variable will be referenced in playbook via \u201c\nprovider\n\u201d argument (that will be added in next step #4).    \n\n\n\n\n\n\n\n\n\n\nStep 4: VLAN tasks in playbook\n\n\n\n\nContinue to add below tasks on the same  playbook file:\n\n\n\n\n    tasks:\n      - name: provision VLAN\n        nxos_config:\n          lines: \nvlan {{item}}\n\n          provider: \n{{nxos_provider}}\n\n        with_items:\n          -  140\n          -  141\n        tags: add vlans\n\n\n\n\nNote:\n\n\n\n\nMultiple plays can be defined in one playbook under \u201c\ntasks\n\u201d, each starts with \u201c-\u201c . \n\n\n\n\nThis kind of play creates multiple VLANs using \nnxos_config\n module. \n\n\n\n\nThe \u201clines\u201d option is used to pass only one configuration command.  This commands must be the exact same commands as found in the device running-config.\n\n\nThough one or multiple configuration commands can be configured under the \u201clines\u201d option.   For multiple, ordered set of commands can be configured under this \u201cline\u201d section.\n\n\n\n\n\n\n\n\nAt the end of this play, we use \u201c\ntags\n\u201d to name the play \u201c\nadd vlans\n\u201d. This is useful to run a specific part of the configuration without running whole playbook.\n\n\n\n\n\n\nBelow screenshot shows how playbook will look:\n\n\n\n\nNOTE: Formatting is extremely important when working with Ansible. Ansible playbook would return errors if the spaces are not properly aligned or formatting is not correct\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nNOTE: Once the Save button is pressed, then at the lower part of ATOM app, you will see message about connecting to Ansibe host (198.18.134.150) and saving the vlan_provision.yml file\n\n\n\n\n\n\n\n\nStep 5: Execute playbook\n\n\nAfter creating the playbook, it is now time to execute the playbook. \n\n\n\n\nBefore executing the playbook, we will verify the leaf switch if it has any vlan configuration present on it.\n\n\n\n\nLogin\n to leaf-3 switch using the Mputty client, or any leaf switch in previous playbook (remember \nhosts:\n variable in the file), and execute \nshow vlan brief\n command. \n\n\n\n\nThis command will show the vlans that currently exist on the leaf switch.  As you note from below screenshot, only the default VLAN (vlan number 1) is configured:\n\n\n\n\n\n\n\n\n\n\nNow, go to mputty, login or launch a new ssh into Ansible node (198.18.134.150) \n\n\n\n\n\n\nUse command \nansible-playbook vlan_provision.yml --tags \"add vlans\"\n under folder \"LTRDCN-1572\" as shown below:\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml --tags \"add vlans\"\n\n\n\n\n\nNote: If the playbook fails first time, re-run the playbook again. Make sure to save all the changes in the playbook first before executing the playbook in Ansible.\n\n\n\n\n\n\nAfter playbook is run successfully, login into leaf 3 again and check if vlan 140 and vlan 141 appears. There would also be a log message on the screen indicating a configuration change was pushed to the device:\n\n\n\n\n\n\n\n\n\n\nStep 6: Server port VLAN tasks in playbook\n\n\nWe have just tested our first playbook with basic configuration (i.e, by adding 2 VLANS), now we are going to add more tasks in our existing playbook \u201c\nvlan_provision.yml\n\u201d in this step:\n\n\n\n\nwe will add new plays in the playbook to assign VLANs to server facing port. This time, we will configure VLAN towards the server facing ports\n\n\nGo back to \nATOM\n and \nadd\n the following plays to the \nexisting playbook\n\n\n\n\n      - name: configure server facing port to L2\n        nxos_interface:\n          interface: eth1/3\n          mode: layer2\n          provider: \n{{nxos_provider}}\n\n      - name: configure VLAN for server port\n        when: (\n101\n in inventory_hostname) or (\n103\n in inventory_hostname)\n        nxos_switchport:\n          interface: eth1/3\n          mode: access\n          access_vlan: 140\n          provider: \n{{nxos_provider}}\n\n      - name: configure VLAN for server port\n        when: (\n102\n in inventory_hostname) or (\n104\n in inventory_hostname)\n        nxos_switchport:\n          interface: eth1/3\n          mode: access\n          access_vlan: 141\n          provider: \n{{nxos_provider}}\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n on ATOM. This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nNote: In this new play, we used nxos module \u201c\nnxos_interface\n\u201d and \u201c\nnxos_switchport\n\u201d. \n\n\n\n\n\u201c\nnxos_interface\n\u201d provides the capability to manage the physical attributes of an interface\n\n\nIn this example, it is used to configure \u201clayer 2\u201d on interface Ethernet 1/3\n\n\n\n\n\n\n\u201c\nnxos_switchport\n\u201d provides the capability to manage the Layer 2 switchport attributes\n\n\nIn this example, it is used to configuration it is used to configure mode access on Ethernet ports 1/3\n\n\n\n\n\n\nWe used \u201c\nwhen\n\u201d argument to provide little logic of the play. \n\n\nIn our example, the playbook assign VLAN 140 on leaf1 and leaf3 switches; assign VLAN 141 on leaf4 switch. \n\n\n\n\n\n\n\n\n\n\nStep 7: Execute playbook\n\n\nNow, we are going to execute the playbook with the command \nansible-playbook vlan_provision.yml\n\n\n\n\n\n\nBefore excuting the ansible playbook, you may log into switch (leaf1, leaf3 or leaf4) using MTPutty client, and check the existing configuration by executing the below command:\n  \nshow run interface ethernet1/3\n\n\n\n\n\n\nOn MTPuttY, log back into (or launch a new ssh) into \u201cAnsible\u201d node\n\n\n\n\n\n\nExecute command \nansible-playbook vlan_provision.yml\n in the \u201cLTRDCN-1572\u201d directory as shown below:\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml\n\n\n\n\n\n\n\n\n\nAfter we push the configuration, \nlogin\n to the \nleaf-3\n switch, and check if the server facing port has the access vlan configured with the below command:\n  \nshow run interface ethernet1/3\n\n\nThe output of above command is shown in below screenshot:\n\n\n\n\n\n\n\n\n\n\nCongratulation! You have created your first ansible playbook, automatically provisioned new VLANs and assigned port to new created VLANs using Ansible. Next we are going to create VXLAN Fabric using Ansible.", 
            "title": "First playbook"
        }, 
        {
            "location": "/task2-first-ansible/#step-1-using-atom-text-editor", 
            "text": "Open \u201cAtom\u201d text editor by double click the icon on desktop. Atom is the recommended text editor for this lab:     After opening ATOM,  Click   No, Never  to the \u201cRegister as default atom:// URI handlre\u201d message as show below:", 
            "title": "Step 1: Using \"Atom\" text Editor"
        }, 
        {
            "location": "/task2-first-ansible/#step-2-atom-folder", 
            "text": "After opening ATOM, there should be a folder in the left pane named \u201cLTRDCN-1572.\u201d   Right click  the pre-configured project folder  LTRDCN-1572  and select  New File       Name the new file  vlan_provision.yml  and hit enter. This will create the new file:     Also, on the lower bar of the ATOM, verify that  file grammar of  YAML  is selected instead of default \" Plain Text \".  If  YAML  is not selected, then you should  choose  it from the listed options.", 
            "title": "Step 2: Atom folder"
        }, 
        {
            "location": "/task2-first-ansible/#step-3-define-variables-tasks-for-playbook", 
            "text": "In this step, we are going to define the scope, variable for playbook and tasks    In the opened window for \u2018vlan_provision.yml\u2019 file, enter the below content.  NOTE: YAML is space sensitive. Hence be careful with the spaces in below section.      ---                     \n#Task 2: Simple playbook assign VLAN to server facing port\n  - hosts: leaf:jinja2_leaf\n    vars:\n      nxos_provider:\n        username:  {{ user }} \n        password:  {{ pwd }} \n        transport: nxapi\n        host:  {{ inventory_hostname }}   Note:   \u201c hosts: \u201d defines the scope of this playbook applies to all switches in group \u2018leaf\u2019 and \u2018jinja2_leaf\u2019 (within the  \"hosts\"  file created in pervious task).   Note  that you can review the IP addresses of the three (2) \u201cleaf\u201d and one (1) \u201cjinja2_leaf\u201d in the \u201c hosts \u201d file (configured in previous steps).  The IP addresses are:  jinja2_leaf: 198.18.4.104  leaf: 198.18.4.101  leaf: 198.18.4.103      \u201c vars \u201d defines one variable \u201c nxos_provider \u201d that will be used in this playbook.   \u201c nxos_provider \u201d is variable that includes all value that will be used for connection and autnentication.   This variable will be referenced in playbook via \u201c provider \u201d argument (that will be added in next step #4).", 
            "title": "Step 3: Define variables, tasks for playbook"
        }, 
        {
            "location": "/task2-first-ansible/#step-4-vlan-tasks-in-playbook", 
            "text": "Continue to add below tasks on the same  playbook file:       tasks:\n      - name: provision VLAN\n        nxos_config:\n          lines:  vlan {{item}} \n          provider:  {{nxos_provider}} \n        with_items:\n          -  140\n          -  141\n        tags: add vlans  Note:   Multiple plays can be defined in one playbook under \u201c tasks \u201d, each starts with \u201c-\u201c .    This kind of play creates multiple VLANs using  nxos_config  module.    The \u201clines\u201d option is used to pass only one configuration command.  This commands must be the exact same commands as found in the device running-config.  Though one or multiple configuration commands can be configured under the \u201clines\u201d option.   For multiple, ordered set of commands can be configured under this \u201cline\u201d section.     At the end of this play, we use \u201c tags \u201d to name the play \u201c add vlans \u201d. This is useful to run a specific part of the configuration without running whole playbook.    Below screenshot shows how playbook will look:   NOTE: Formatting is extremely important when working with Ansible. Ansible playbook would return errors if the spaces are not properly aligned or formatting is not correct    Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.    NOTE: Once the Save button is pressed, then at the lower part of ATOM app, you will see message about connecting to Ansibe host (198.18.134.150) and saving the vlan_provision.yml file", 
            "title": "Step 4: VLAN tasks in playbook"
        }, 
        {
            "location": "/task2-first-ansible/#step-5-execute-playbook", 
            "text": "After creating the playbook, it is now time to execute the playbook.    Before executing the playbook, we will verify the leaf switch if it has any vlan configuration present on it.   Login  to leaf-3 switch using the Mputty client, or any leaf switch in previous playbook (remember  hosts:  variable in the file), and execute  show vlan brief  command.    This command will show the vlans that currently exist on the leaf switch.  As you note from below screenshot, only the default VLAN (vlan number 1) is configured:      Now, go to mputty, login or launch a new ssh into Ansible node (198.18.134.150)     Use command  ansible-playbook vlan_provision.yml --tags \"add vlans\"  under folder \"LTRDCN-1572\" as shown below:  [root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml --tags \"add vlans\"   Note: If the playbook fails first time, re-run the playbook again. Make sure to save all the changes in the playbook first before executing the playbook in Ansible.    After playbook is run successfully, login into leaf 3 again and check if vlan 140 and vlan 141 appears. There would also be a log message on the screen indicating a configuration change was pushed to the device:", 
            "title": "Step 5: Execute playbook"
        }, 
        {
            "location": "/task2-first-ansible/#step-6-server-port-vlan-tasks-in-playbook", 
            "text": "We have just tested our first playbook with basic configuration (i.e, by adding 2 VLANS), now we are going to add more tasks in our existing playbook \u201c vlan_provision.yml \u201d in this step:   we will add new plays in the playbook to assign VLANs to server facing port. This time, we will configure VLAN towards the server facing ports  Go back to  ATOM  and  add  the following plays to the  existing playbook         - name: configure server facing port to L2\n        nxos_interface:\n          interface: eth1/3\n          mode: layer2\n          provider:  {{nxos_provider}} \n      - name: configure VLAN for server port\n        when: ( 101  in inventory_hostname) or ( 103  in inventory_hostname)\n        nxos_switchport:\n          interface: eth1/3\n          mode: access\n          access_vlan: 140\n          provider:  {{nxos_provider}} \n      - name: configure VLAN for server port\n        when: ( 102  in inventory_hostname) or ( 104  in inventory_hostname)\n        nxos_switchport:\n          interface: eth1/3\n          mode: access\n          access_vlan: 141\n          provider:  {{nxos_provider}}    Click   File  and  Save  on ATOM. This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.    Note: In this new play, we used nxos module \u201c nxos_interface \u201d and \u201c nxos_switchport \u201d.    \u201c nxos_interface \u201d provides the capability to manage the physical attributes of an interface  In this example, it is used to configure \u201clayer 2\u201d on interface Ethernet 1/3    \u201c nxos_switchport \u201d provides the capability to manage the Layer 2 switchport attributes  In this example, it is used to configuration it is used to configure mode access on Ethernet ports 1/3    We used \u201c when \u201d argument to provide little logic of the play.   In our example, the playbook assign VLAN 140 on leaf1 and leaf3 switches; assign VLAN 141 on leaf4 switch.", 
            "title": "Step 6: Server port VLAN tasks in playbook"
        }, 
        {
            "location": "/task2-first-ansible/#step-7-execute-playbook", 
            "text": "Now, we are going to execute the playbook with the command  ansible-playbook vlan_provision.yml    Before excuting the ansible playbook, you may log into switch (leaf1, leaf3 or leaf4) using MTPutty client, and check the existing configuration by executing the below command:\n   show run interface ethernet1/3    On MTPuttY, log back into (or launch a new ssh) into \u201cAnsible\u201d node    Execute command  ansible-playbook vlan_provision.yml  in the \u201cLTRDCN-1572\u201d directory as shown below:  [root@rhel7-tools LTRDCN-1572]# ansible-playbook vlan_provision.yml     After we push the configuration,  login  to the  leaf-3  switch, and check if the server facing port has the access vlan configured with the below command:\n   show run interface ethernet1/3  The output of above command is shown in below screenshot:      Congratulation! You have created your first ansible playbook, automatically provisioned new VLANs and assigned port to new created VLANs using Ansible. Next we are going to create VXLAN Fabric using Ansible.", 
            "title": "Step 7: Execute playbook"
        }, 
        {
            "location": "/task3-vxlan-jinja2/", 
            "text": "In this task, we are going to install Jinja2. It is one of the python template engines.  In this section, we use Jinja2 to create template for spine and leaf and configure VXLAN fabric using this Jinja2 templates. \n\n\n\n\nJinja2 template looks just like the NXOS configurations. We abstract the variables out of the configuration and use simple for loop to feed variables into the template. \n\n\n\n\nStep 1: Install jinja2\n\n\n\n\nOn the Ansible node, install jinja2 using \npip install jinja2\n command.  If it is already installed, we will get the message \u201crequirement is satisfied\u201d:\n[root@rhel7-tools ~]# pip install jinja2\nRequirement already satisfied (use --upgrade to upgrade): jinja2 in /usr/lib/python2.7/site-packages/Jinja2-2.8-py2.7.egg\nRequirement already satisfied (use --upgrade to upgrade): MarkupSafe in /usr/lib/python2.7/site-packages/MarkupSafe-0.23-py2.7.egg (from jinja2)\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Playbook for jinja2 Spine\n\n\nIn this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric. \n\n\n\n\n\n\nSwitch to \u201cAtom\u201d, \nright click\n on the folder 'LTRDDCN-1572' and create a new playbook named \njinja2_fabric.yml\n. Enter this file name and hit enter\n\n\n\n\n\n\n\n\nAlso, on the lower bar of the ATOM, verify that  file grammar of \nYAML\n is selected instead of default \"\nPlain Text\n\".  If \nYAML\n is not selected, then you should \nchoose\n it from the listed options.\n\n\n\n\n\n\nNow enter below data in this playbook:\n\n\n\n\n\n\n---\n  - hosts: jinja2_spine\n    connection: local\n    roles:\n    - jinja2_spine\n\n\n\n\n\n\n\n\n\nThe contents of the \njinja2_fabric.yml\n file should look like\n\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\nOn the MTputty, go back to the ssh session to Ansible Server node (198.18.134.150). \nVerify\n that the below 2 groups exists in the inventory filename \"\nhosts\n\" under the folder LTRDCN-1572\n\n\n[jinja2_spine]\n198.18.4.202 \n[jinja2_leaf]\n198.18.4.104\n\n\n\n\n\n\n\n\n\nStep 3: Create new roles and vars\n\n\nIn this section, we will create two new roles for provisioning Fabric with Jina2 template.\n\n\n\n\n\n\nOn the MTputty, go back to Ansible Server node (198.18.134.150), switch to \u2018\nroles\n\u2019 directory; \ncreate\n \u2018jinja2_spine\u2019 and \u2018jinja2_leaf\u2019 roles using ansible-galaxy using below commands: \n\n\n[root@rhel7-tools ~]# cd ~/LTRDCN-1572/\n[root@rhel7-tools LTRDCN-1572]# cd roles/\n[root@rhel7-tools roles]# ansible-galaxy init jinja2_spine\nansible-galaxy init jinja2_leaf\n\n\n\n\n\n\n\nBelow screenshot shows the output of above command:\n\n\n\n\n\n\n\n\nNote:\n \u2018\nansible-galaxy\n\u2019 will initialize the role structure and create necessary folders with default name like \u2018tasks\u2019, \u2018template\u2019, \u2018vars\u2019 etc.\n\n\n\n\n\n\nchange directory\n path to LTRDCN-1572/roles/jinja2_spine and check the content of local directory (\nls\n) as per below commands:\n\n\n[root@rhel7-tools]# cd ~/LTRDCN-1572/roles/jinja2_spine/\n[root@rhel7-tools jinja2_spine]# ls\n\n\n\n\n\n\n\nBelow screenshot shows the output of above file.  Note that various directories including tasks, templates, vars exists.  We will use these in later steps. \n\n\n\n\n\n\n\n\nNext:\n\n\n\n\n\n\nCreate\n empty jinja2 template files for spine and leaf under templates folder for each role by running below commands:\n\n\n[root@rhel7-tools roles]# cd ~/LTRDCN-1572/roles\n[root@rhel7-tools roles]# touch jinja2_spine/templates/spine.j2\n[root@rhel7-tools roles]# touch jinja2_leaf/templates/leaf.j2\n\n\n\n\n\n\n\nSwitch to \u201c\nAtom\n\u201d and sync the new created folders between Ansible node and Remote desktop by pressing \nRight Click\n on the folder \nLTRDDCN-1572\n, then open \nRemote Sync\n select \nDownload Folder\n as shown below:\n\n\n\n\n\n\n\n\n\n\nStep 4: Create variable file for \u201cjina2_spine\u201d role\n\n\n\u201c\nansible-galaxy\n\u201d automatically creates empty \u201c\nmain.yml\n\u201d file under \u201c\nvars\n\u201d folder. We can use \u201c\nAtom\n\u201d to edit the main.yml file to include the following variables that will be used in jinja2 template. \n\n\n\n\n\n\nSwitch to \nATOM\n, then open up the project folder \nLTRDCN-1572\n from the left pane and \nopen\n \nmain.yml\n file under \u201c\nroles/jinja2_spine/vars/\n\u201d as shown below:\n\n\n\n\n\n\n\n\nuse \u201c\nAtom\n\u201d to edit the \u201c\nmain.yml\n\u201d file to include the following variables that will be used in jinja2 template. \n\n\n\n\n\n\n---\n# vars file for jinja2_spine\n  nxos_provider:\n    username: \n{{ user }}\n\n    password: \n{{ pwd }}\n\n    timeout: 100\n    host: \n{{ inventory_hostname }}\n\n  asn: 65000\n  bgp_neighbors:\n  -  remote_as: 65000\n     neighbor: 192.168.0.8\n     update_source: Loopback0\n  -  remote_as: 65000\n     neighbor: 192.168.0.10\n     update_source: Loopback0\n  -  remote_as: 65000\n     neighbor: 192.168.0.11\n     update_source: Loopback0\n  L3_interfaces:\n  -  interface: Ethernet 1/1\n  -  interface: Ethernet 1/2\n  -  interface: Ethernet 1/3\n  -  interface: Ethernet 1/4\n  -  interface: loopback 0\n  -  interface: loopback 1\n  s1_loopback: 192.168.0.6\n  s2_loopback: 192.168.0.7\n\n\n\n\n\n\n\n\nContents of the \u2018\nmain.yml\n\u2019 file should look like below:\n\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\n\n\nStep 5: Create Jinja2 template for spine role\n\n\n\n\nOn \nATOM\n open up the project folder \nLTRDCN-1572\n from the left pane and \nopen\n \nspine.j2\n file under \u201c\nroles/jinja2_spine/templates\n\u201d as shown below:\n\n\n\n\nNOTE: if the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync.  If the \nspine.j2\n file appears in above folder then you can skip below 4 steps\n:\n\n\n\n\n\n\nChange Directory to folder LTRDCN-1572 on Ansible server (198.18.134.150) using below command:\n\n\ncd ~/LTRDC-1572\n\n\n\n\n\n\n\nfurther, change Directory (cd) to folder roles/jinja2_spine/templates using below command: \n\n\ncd roles/jinja2_spine/templates\n\n\n\n\n\n\n\nType \ntouch spine.j2\n\n\n\n\n\n\nAfter entering the command, go back to ATOM,  \nright click\n on folder \nLTRDCN-1572\n, scroll to choose option \nRemote Sync\n option and choose \nDownload Folder\n as shown below: \n\n\n\n\n\n\n\n\nNow that the file/folder appears properly on ATOM, go ahead and proceed with further steps:\n\n\n\n\n\n\nTo reduce the typo, you can download jina2 template from the \nbox\n folder spine.j2. Below is the link to this folder:\n\n\nhttps://bit.ly/LTRDCN-1572\n\n\n\n\n\n\nThe file would be under LTRDCN-1572/roles/jinja2_spine/templates/spine.j2 (link above) as shown in below screenshot:\n\n\n\n\n\n\n\n\n\n\n\n\nAfter the file is downloaded to the PC (on your RDP session), go to the downloads folder. Move (or copy) the file from the downloads folder and paste the file in the projects folder\n\n\nTFTP_Data (\\\\AD1) (X:)\n --\n  \nLTRDC-1572\n --\n \nroles\n --\n \njinja2_spine\n --\n \ntemplates\n\n\nas shown below:\n\n\n\n\n\n\n\n\nOn ATOM, go to \nFile\n then \nOpen File\u2026\n and browse to this \nspine.j2\n file that was just saved in \nX:\\LTRDCN-1572\\roles\\jinja2_spine\\templates\n as shown below:\n\n\n\n\n\n\n\n\nAfter opening \u201c\nspine.j2\n\u201d file from ATOM, go to \nFile\n \u2013-\n \nSave\n to push template file to Ansible node:\n\n\n\n\n\n\n\n\n\n\nStep 6: Create playbook for jinja2_spine role\n\n\nThe playbook for jinja2_spine roles has two tasks. First task uses ansible \ntemplate\n module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201c\nfile\n\u201d folder. Second task is push the configuration to switch. \n\n\n\u201c\nansible-galaxy\n\u201d automatically creates empty \u201c\nmain.yml\n\u201d file under \u201c\ntasks\n\u201d folder.  We are going to use \u201c\nAtom\n\u201d to edit the \nmain.yml\n file. \n\n\n\n\nOn ATOM, open up the project folder \nLTRDCN-1572\n and edit \nmain.yml\n file under \nroles/jinja2_spine/tasks/\n to include following: \n\n\n\n\n---\n# tasks file for jinja2_spine\n  - name: Generate Spine Config\n    template: src=spine.j2 dest=roles/jinja2_spine/files/{{inventory_hostname}}.cfg\n  - name: Push Spine Config\n    nxos_config:\n      src: roles/jinja2_spine/files/{{inventory_hostname}}.cfg\n      match: none\n      provider: \n{{ nxos_provider }}\n\n\n\n\n\n\n\n\n\nContents of the \u2018\nmain.yml\n\u2019 file should look like below:\n\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\nNOTE:\n In the above YAML file, ansible module named \u201c\nnxos_config\n\u201d is used.  This module performs below activities:\n\n\n\n\nIt uses source path of the file (\u201c\nsrc\n\u201d) that contains the configuration or configuration template to load into spine\n\n\nSince \u201c\nmatch\n\u201d option is set to none (yes), hence the module will not attempt to compare the source configuration with the running configuration on the remote device.\n\n\n\n\n\n\nStep 7: Run Jinja2_fabric playbook\n\n\nIn this section you will run the playbook created in step 2 (in this task 3), this will generate configuration file for Spine-2 switche from the template. \n\n\nThe playbook will also push the configuration file to Spine-2 switches. \n\n\n\n\n\n\nRun the ansible playbook by going to folder LTRDC-1572 and executing the below commands:\n\n\n[root@rhel7-tools roles]# cd ~/LTRDCN-1572/\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml\n\n\n\nNote: It will take few minutes to push configuration\n\n\nBelow screenshot shows the execution of above playbook:\n\n\n\n\n\n\n\n\nTo verify the execution of this playbook, you can:\n\n\n\n\nLogin\n to \nSpine-2 \nswitch (\non MTputty\n) to verify configuration has been pushed. Double click the spine-2 icon in the left pane on MTputty.  Login with credentials admin/C1sco12345\n\n\n\n\nExecute \nshow run bgp\n command on the switch to confirm the configurations have been provisioned (as shown below):\n\n\n\n\n\n\n\n\n\n\nStep 8: Modify playbook for Leaf\n\n\nIn this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric on leaf-4.   We are going to add jinja2_leaf this time to the already created playbook in step 2.\n\n\n\n\nSwitch to \u201c\nAtom\n\u201d, click on the folder \nLTRDDCN-1572\n, edit the existing playbook \njinja2_fabric.yml\n file for a role for leaf (named jinja2_leaf).  Add the below \n\n\n\n\n  - hosts: jinja2_leaf\n    connection: local\n    roles:\n      - jinja2_leaf\n\n\n\n\n\n\nBelow screenshot shows the contents of \njinja2_fabric.yml\n file in Atom after adding the above configs:\n\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\nStep 9: Variable file for jinja2_leaf role\n\n\n\n\nOn ATOM, open up the project folder \nLTRDCN-1572\n and edit \nmain.yml\n file under \nroles/jinja2_leaf/vars/\n to include following:\n\n\n\n\n---\n# vars file for jinja2_leaf\n  nxos_provider:\n    username: \n{{ user }}\n\n    password: \n{{ pwd }}\n\n    timeout: 100\n    host: \n{{ inventory_hostname }}\n\n  asn: 65000\n  bgp_neighbors:\n  -  remote_as: 65000\n     neighbor: 192.168.0.6\n     update_source: Loopback0\n  -  remote_as: 65000\n     neighbor: 192.168.0.7\n     update_source: Loopback0\n  rp_address: 192.168.0.100\n  L3_interfaces:\n  -  interface: Ethernet 1/1\n  -  interface: Ethernet 1/2\n  -  interface: loopback 0\n  -  interface: loopback 1\n  L2VNI:\n  -  vlan_id: 140\n     vni: 50140\n     ip_add: 172.21.140.1\n     mask: 24\n     vlan_name: L2-VNI-140-Tenant1\n     mcast: 239.0.0.140\n  -  vlan_id: 141\n     vni: 50141\n     ip_add: 172.21.141.1\n     mask: 24\n     vlan_name: L2-VNI-141-Tenant1\n     mcast: 239.0.0.141\n  L3VNI:\n  -  vlan_id: 999\n     vlan_name: L3-VNI-999-Tenant1\n     vni: 50999\n\n\n\n\n\n\n\n\nBelow screenshot shows the contents of jinja2_leaf\\vars\\main.yml file in Atom:\n\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\n\n\nStep 10: Jinja2 template for leaf role\n\n\n\n\nOn ATOM, open up the project folder \nLTRDCN-1572\n and open \nleaf.j2\n file under \u201c\nroles/jinja2_leaf/templates/\n\u201d\n\n\n\n\nNOTE: if the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync.  If the \nleaf.j2\n file appears in above folder then you can skip below 4 steps\n:\n\n\n\n\n\n\nOn MTputty, change Directory to folder LTRDCN-1572 on Ansible server (198.18.134.150) using below command:\n\n\ncd ~/LTRDC-1572\n\n\n\n\n\n\n\nfurther, change Directory (cd) to folder roles/jinja2_leaf/templates using below command:\n\n\ncd roles/jinja2_leaf/templates\n\n\n\n\n\n\n\nType \ntouch leaf.j2\n\n\n\n\nAfter entering the command, go back to ATOM,  \nright click\n on folder \nLTRDCN-1572\n, scroll to choose option \nRemote Sync\n option and choose \nDownload Folder\n\n\n\n\nNow that the file/folder appears properly on ATOM, go ahead and proceed with further steps:\n\n\n\n\n\n\nTo reduce the typo, you can download jina2 template from the \nbox\n folder leaf.j2. Below is the link to this folder:\n\n\n\n\n\n\nTo reduce the typo, you can download jina2 template from the \nbox\n folder leaf.j2. Below is the link to this folder:\n\n\nhttps://bit.ly/LTRDCN-1572\n\n\n\n\n\n\nThe file would be under LTRDCN-1572/roles/jinja2_leaf/templates/leaf.j2\n(link below) as shown in below screenshot:\n\n\n\n\n\n\n\n\n\n\n\n\nAfter the file is downloaded to the PC (on your RDP session), go to the downloads folder. Move (or copy) the file from the downloads folder and paste the file in the projects folder\n\n\nTFTP_Data (\\\\AD1) (X:)\n --\n  \nLTRDC-1572\n --\n \nroles\n --\n \njinja2_leaf\n --\n \ntemplates\n\n\nas shown below:\n\n\n\n\n\n\n\n\nOn ATOM, go to \nFile\n then \nOpen File\u2026\n and browse to this \nleaf.j2\n that was just saved in \nX:\\LTRDCN-1572\\roles\\jinja2_leaf\\templates\n as shown below screenshots:\n\n\n\n\n\n\n\n\n\n\nAfter opening \u201c\nleaf.j2\n\u201d file from ATOM, go to \nFile\n \u2013 \nSave\n to push template file to Ansible node:\n\n\n\n\n\n\n\n\nStep 11: Create playbook for jinja2_leaf role\n\n\nThe playbook for jinja2_leaf roles has two tasks. \n\n\n\n\nFirst task uses ansible template module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201cfile\u201d folder. \n\n\nSecond task is to push the configuration to switch. \n\n\n\n\n\u201c\nansible-galaxy\n\u201d automatically creates empty \u201c\nmain.yml\n\u201d file under \u201c\ntasks\n\u201d folder.  We are going to use \u201c\nAtom\n\u201d to edit this main.yml file. \n\n\n\n\nOn ATOM, open up the project folder \nLTRDCN-1572\n and edit \u201c\nmain.yml\n\u201d file under \u201c\nroles/jinja2_leaf/tasks/\n\u201d to include following:\n\n\n\n\n---\n# tasks file for jinja2_leaf\n  - name: Generate Leaf Config\n    template: src=leaf.j2 dest=roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\n  - name: Push Leaf Config\n    nxos_config:\n      src: roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\n      match: none\n      provider: \n{{ nxos_provider }}\n\n\n\n\n\nBelow screenshot shows how the contents of \njinja2_leaf/taks/main.yml\n file looks like in Atom:\n\n\n\n\n\n\nStep 12: Run Jinja2_fabric playbook\n\n\nIn this section you will run the playbook created in step 8, this will generate configuration file for Spine-2 and Leaf-4 switches. It will also push the configuration file to both switches. \n\n\n\n\n\n\nBefore running the ansible-playbook, you may \nlog\n into the leaf-4 (in MTputty SSH session) and verify that no bgp configurations exist by running \nshow running bgp\n command as shown below:\n\n\n\n\n\n\n\n\nNOTE: It might take couple of minutes for the configuration to be pushed to via the Ansible Server. It is working in the background. \n\n\n\n\n\n\nOn the Ansible node (in MTputty SSH session), run the command (\nansible-playbook jinja2_fabric.yml\n) to execute the playbook as shown below:\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml\n\n\n\nBelow screenshot shows the output of above command:\n\n\n\n\n\n\n\n\nAfter the configuration push is successful, \nlogin\n (on MTputty SSH session) to \nleaf-4\n switch to verify configuration has been pushed by running below command:\n\n\nshow running-config bgp\n\n\n\nThe output of above command is shown below:\n\n\n\n\n\n\n\n\n\n\nCongrats: you have successfully concluded this task by using jinja2 templates with Ansible for Cisco Nexus switches", 
            "title": "Jinja2 Fabric"
        }, 
        {
            "location": "/task3-vxlan-jinja2/#step-1-install-jinja2", 
            "text": "On the Ansible node, install jinja2 using  pip install jinja2  command.  If it is already installed, we will get the message \u201crequirement is satisfied\u201d: [root@rhel7-tools ~]# pip install jinja2\nRequirement already satisfied (use --upgrade to upgrade): jinja2 in /usr/lib/python2.7/site-packages/Jinja2-2.8-py2.7.egg\nRequirement already satisfied (use --upgrade to upgrade): MarkupSafe in /usr/lib/python2.7/site-packages/MarkupSafe-0.23-py2.7.egg (from jinja2)", 
            "title": "Step 1: Install jinja2"
        }, 
        {
            "location": "/task3-vxlan-jinja2/#step-2-playbook-for-jinja2-spine", 
            "text": "In this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric.     Switch to \u201cAtom\u201d,  right click  on the folder 'LTRDDCN-1572' and create a new playbook named  jinja2_fabric.yml . Enter this file name and hit enter     Also, on the lower bar of the ATOM, verify that  file grammar of  YAML  is selected instead of default \" Plain Text \".  If  YAML  is not selected, then you should  choose  it from the listed options.    Now enter below data in this playbook:    ---\n  - hosts: jinja2_spine\n    connection: local\n    roles:\n    - jinja2_spine    The contents of the  jinja2_fabric.yml  file should look like     Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.     On the MTputty, go back to the ssh session to Ansible Server node (198.18.134.150).  Verify  that the below 2 groups exists in the inventory filename \" hosts \" under the folder LTRDCN-1572  [jinja2_spine]\n198.18.4.202 \n[jinja2_leaf]\n198.18.4.104", 
            "title": "Step 2: Playbook for jinja2 Spine"
        }, 
        {
            "location": "/task3-vxlan-jinja2/#step-3-create-new-roles-and-vars", 
            "text": "In this section, we will create two new roles for provisioning Fabric with Jina2 template.    On the MTputty, go back to Ansible Server node (198.18.134.150), switch to \u2018 roles \u2019 directory;  create  \u2018jinja2_spine\u2019 and \u2018jinja2_leaf\u2019 roles using ansible-galaxy using below commands:   [root@rhel7-tools ~]# cd ~/LTRDCN-1572/\n[root@rhel7-tools LTRDCN-1572]# cd roles/\n[root@rhel7-tools roles]# ansible-galaxy init jinja2_spine ansible-galaxy init jinja2_leaf    Below screenshot shows the output of above command:     Note:  \u2018 ansible-galaxy \u2019 will initialize the role structure and create necessary folders with default name like \u2018tasks\u2019, \u2018template\u2019, \u2018vars\u2019 etc.    change directory  path to LTRDCN-1572/roles/jinja2_spine and check the content of local directory ( ls ) as per below commands:  [root@rhel7-tools]# cd ~/LTRDCN-1572/roles/jinja2_spine/\n[root@rhel7-tools jinja2_spine]# ls    Below screenshot shows the output of above file.  Note that various directories including tasks, templates, vars exists.  We will use these in later steps.      Next:    Create  empty jinja2 template files for spine and leaf under templates folder for each role by running below commands:  [root@rhel7-tools roles]# cd ~/LTRDCN-1572/roles\n[root@rhel7-tools roles]# touch jinja2_spine/templates/spine.j2\n[root@rhel7-tools roles]# touch jinja2_leaf/templates/leaf.j2    Switch to \u201c Atom \u201d and sync the new created folders between Ansible node and Remote desktop by pressing  Right Click  on the folder  LTRDDCN-1572 , then open  Remote Sync  select  Download Folder  as shown below:", 
            "title": "Step 3: Create new roles and vars"
        }, 
        {
            "location": "/task3-vxlan-jinja2/#step-4-create-variable-file-for-jina2_spine-role", 
            "text": "\u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c vars \u201d folder. We can use \u201c Atom \u201d to edit the main.yml file to include the following variables that will be used in jinja2 template.     Switch to  ATOM , then open up the project folder  LTRDCN-1572  from the left pane and  open   main.yml  file under \u201c roles/jinja2_spine/vars/ \u201d as shown below:     use \u201c Atom \u201d to edit the \u201c main.yml \u201d file to include the following variables that will be used in jinja2 template.     ---\n# vars file for jinja2_spine\n  nxos_provider:\n    username:  {{ user }} \n    password:  {{ pwd }} \n    timeout: 100\n    host:  {{ inventory_hostname }} \n  asn: 65000\n  bgp_neighbors:\n  -  remote_as: 65000\n     neighbor: 192.168.0.8\n     update_source: Loopback0\n  -  remote_as: 65000\n     neighbor: 192.168.0.10\n     update_source: Loopback0\n  -  remote_as: 65000\n     neighbor: 192.168.0.11\n     update_source: Loopback0\n  L3_interfaces:\n  -  interface: Ethernet 1/1\n  -  interface: Ethernet 1/2\n  -  interface: Ethernet 1/3\n  -  interface: Ethernet 1/4\n  -  interface: loopback 0\n  -  interface: loopback 1\n  s1_loopback: 192.168.0.6\n  s2_loopback: 192.168.0.7    Contents of the \u2018 main.yml \u2019 file should look like below:     Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Step 4: Create variable file for \u201cjina2_spine\u201d role"
        }, 
        {
            "location": "/task3-vxlan-jinja2/#step-5-create-jinja2-template-for-spine-role", 
            "text": "On  ATOM  open up the project folder  LTRDCN-1572  from the left pane and  open   spine.j2  file under \u201c roles/jinja2_spine/templates \u201d as shown below:   NOTE: if the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync.  If the  spine.j2  file appears in above folder then you can skip below 4 steps :    Change Directory to folder LTRDCN-1572 on Ansible server (198.18.134.150) using below command:  cd ~/LTRDC-1572    further, change Directory (cd) to folder roles/jinja2_spine/templates using below command:   cd roles/jinja2_spine/templates    Type  touch spine.j2    After entering the command, go back to ATOM,   right click  on folder  LTRDCN-1572 , scroll to choose option  Remote Sync  option and choose  Download Folder  as shown below:      Now that the file/folder appears properly on ATOM, go ahead and proceed with further steps:    To reduce the typo, you can download jina2 template from the  box  folder spine.j2. Below is the link to this folder:  https://bit.ly/LTRDCN-1572    The file would be under LTRDCN-1572/roles/jinja2_spine/templates/spine.j2 (link above) as shown in below screenshot:       After the file is downloaded to the PC (on your RDP session), go to the downloads folder. Move (or copy) the file from the downloads folder and paste the file in the projects folder  TFTP_Data (\\\\AD1) (X:)  --    LTRDC-1572  --   roles  --   jinja2_spine  --   templates  as shown below:     On ATOM, go to  File  then  Open File\u2026  and browse to this  spine.j2  file that was just saved in  X:\\LTRDCN-1572\\roles\\jinja2_spine\\templates  as shown below:     After opening \u201c spine.j2 \u201d file from ATOM, go to  File  \u2013-   Save  to push template file to Ansible node:", 
            "title": "Step 5: Create Jinja2 template for spine role"
        }, 
        {
            "location": "/task3-vxlan-jinja2/#step-6-create-playbook-for-jinja2_spine-role", 
            "text": "The playbook for jinja2_spine roles has two tasks. First task uses ansible  template  module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201c file \u201d folder. Second task is push the configuration to switch.   \u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder.  We are going to use \u201c Atom \u201d to edit the  main.yml  file.    On ATOM, open up the project folder  LTRDCN-1572  and edit  main.yml  file under  roles/jinja2_spine/tasks/  to include following:    ---\n# tasks file for jinja2_spine\n  - name: Generate Spine Config\n    template: src=spine.j2 dest=roles/jinja2_spine/files/{{inventory_hostname}}.cfg\n  - name: Push Spine Config\n    nxos_config:\n      src: roles/jinja2_spine/files/{{inventory_hostname}}.cfg\n      match: none\n      provider:  {{ nxos_provider }}     Contents of the \u2018 main.yml \u2019 file should look like below:     Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.     NOTE:  In the above YAML file, ansible module named \u201c nxos_config \u201d is used.  This module performs below activities:   It uses source path of the file (\u201c src \u201d) that contains the configuration or configuration template to load into spine  Since \u201c match \u201d option is set to none (yes), hence the module will not attempt to compare the source configuration with the running configuration on the remote device.", 
            "title": "Step 6: Create playbook for jinja2_spine role"
        }, 
        {
            "location": "/task3-vxlan-jinja2/#step-7-run-jinja2_fabric-playbook", 
            "text": "In this section you will run the playbook created in step 2 (in this task 3), this will generate configuration file for Spine-2 switche from the template.   The playbook will also push the configuration file to Spine-2 switches.     Run the ansible playbook by going to folder LTRDC-1572 and executing the below commands:  [root@rhel7-tools roles]# cd ~/LTRDCN-1572/\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml  Note: It will take few minutes to push configuration  Below screenshot shows the execution of above playbook:     To verify the execution of this playbook, you can:   Login  to  Spine-2  switch ( on MTputty ) to verify configuration has been pushed. Double click the spine-2 icon in the left pane on MTputty.  Login with credentials admin/C1sco12345   Execute  show run bgp  command on the switch to confirm the configurations have been provisioned (as shown below):", 
            "title": "Step 7: Run Jinja2_fabric playbook"
        }, 
        {
            "location": "/task3-vxlan-jinja2/#step-8-modify-playbook-for-leaf", 
            "text": "In this section, we will use Jina2 template and Ansible to provision the VXLAN Fabric on leaf-4.   We are going to add jinja2_leaf this time to the already created playbook in step 2.   Switch to \u201c Atom \u201d, click on the folder  LTRDDCN-1572 , edit the existing playbook  jinja2_fabric.yml  file for a role for leaf (named jinja2_leaf).  Add the below      - hosts: jinja2_leaf\n    connection: local\n    roles:\n      - jinja2_leaf   Below screenshot shows the contents of  jinja2_fabric.yml  file in Atom after adding the above configs:     Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Step 8: Modify playbook for Leaf"
        }, 
        {
            "location": "/task3-vxlan-jinja2/#step-9-variable-file-for-jinja2_leaf-role", 
            "text": "On ATOM, open up the project folder  LTRDCN-1572  and edit  main.yml  file under  roles/jinja2_leaf/vars/  to include following:   ---\n# vars file for jinja2_leaf\n  nxos_provider:\n    username:  {{ user }} \n    password:  {{ pwd }} \n    timeout: 100\n    host:  {{ inventory_hostname }} \n  asn: 65000\n  bgp_neighbors:\n  -  remote_as: 65000\n     neighbor: 192.168.0.6\n     update_source: Loopback0\n  -  remote_as: 65000\n     neighbor: 192.168.0.7\n     update_source: Loopback0\n  rp_address: 192.168.0.100\n  L3_interfaces:\n  -  interface: Ethernet 1/1\n  -  interface: Ethernet 1/2\n  -  interface: loopback 0\n  -  interface: loopback 1\n  L2VNI:\n  -  vlan_id: 140\n     vni: 50140\n     ip_add: 172.21.140.1\n     mask: 24\n     vlan_name: L2-VNI-140-Tenant1\n     mcast: 239.0.0.140\n  -  vlan_id: 141\n     vni: 50141\n     ip_add: 172.21.141.1\n     mask: 24\n     vlan_name: L2-VNI-141-Tenant1\n     mcast: 239.0.0.141\n  L3VNI:\n  -  vlan_id: 999\n     vlan_name: L3-VNI-999-Tenant1\n     vni: 50999    Below screenshot shows the contents of jinja2_leaf\\vars\\main.yml file in Atom:     Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Step 9: Variable file for jinja2_leaf role"
        }, 
        {
            "location": "/task3-vxlan-jinja2/#step-10-jinja2-template-for-leaf-role", 
            "text": "On ATOM, open up the project folder  LTRDCN-1572  and open  leaf.j2  file under \u201c roles/jinja2_leaf/templates/ \u201d   NOTE: if the file does not appear on the ATOM, then go ahead and execute below 4 steps to get it sync.  If the  leaf.j2  file appears in above folder then you can skip below 4 steps :    On MTputty, change Directory to folder LTRDCN-1572 on Ansible server (198.18.134.150) using below command:  cd ~/LTRDC-1572    further, change Directory (cd) to folder roles/jinja2_leaf/templates using below command:  cd roles/jinja2_leaf/templates    Type  touch leaf.j2   After entering the command, go back to ATOM,   right click  on folder  LTRDCN-1572 , scroll to choose option  Remote Sync  option and choose  Download Folder   Now that the file/folder appears properly on ATOM, go ahead and proceed with further steps:    To reduce the typo, you can download jina2 template from the  box  folder leaf.j2. Below is the link to this folder:    To reduce the typo, you can download jina2 template from the  box  folder leaf.j2. Below is the link to this folder:  https://bit.ly/LTRDCN-1572    The file would be under LTRDCN-1572/roles/jinja2_leaf/templates/leaf.j2\n(link below) as shown in below screenshot:       After the file is downloaded to the PC (on your RDP session), go to the downloads folder. Move (or copy) the file from the downloads folder and paste the file in the projects folder  TFTP_Data (\\\\AD1) (X:)  --    LTRDC-1572  --   roles  --   jinja2_leaf  --   templates  as shown below:     On ATOM, go to  File  then  Open File\u2026  and browse to this  leaf.j2  that was just saved in  X:\\LTRDCN-1572\\roles\\jinja2_leaf\\templates  as shown below screenshots:      After opening \u201c leaf.j2 \u201d file from ATOM, go to  File  \u2013  Save  to push template file to Ansible node:", 
            "title": "Step 10: Jinja2 template for leaf role"
        }, 
        {
            "location": "/task3-vxlan-jinja2/#step-11-create-playbook-for-jinja2_leaf-role", 
            "text": "The playbook for jinja2_leaf roles has two tasks.    First task uses ansible template module to generate configuration file based on jinja2 template created in last step. The configuration file is saved in \u201cfile\u201d folder.   Second task is to push the configuration to switch.    \u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder.  We are going to use \u201c Atom \u201d to edit this main.yml file.    On ATOM, open up the project folder  LTRDCN-1572  and edit \u201c main.yml \u201d file under \u201c roles/jinja2_leaf/tasks/ \u201d to include following:   ---\n# tasks file for jinja2_leaf\n  - name: Generate Leaf Config\n    template: src=leaf.j2 dest=roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\n  - name: Push Leaf Config\n    nxos_config:\n      src: roles/jinja2_leaf/files/{{inventory_hostname}}.cfg\n      match: none\n      provider:  {{ nxos_provider }}   Below screenshot shows how the contents of  jinja2_leaf/taks/main.yml  file looks like in Atom:", 
            "title": "Step 11: Create playbook for jinja2_leaf role"
        }, 
        {
            "location": "/task3-vxlan-jinja2/#step-12-run-jinja2_fabric-playbook", 
            "text": "In this section you will run the playbook created in step 8, this will generate configuration file for Spine-2 and Leaf-4 switches. It will also push the configuration file to both switches.     Before running the ansible-playbook, you may  log  into the leaf-4 (in MTputty SSH session) and verify that no bgp configurations exist by running  show running bgp  command as shown below:     NOTE: It might take couple of minutes for the configuration to be pushed to via the Ansible Server. It is working in the background.     On the Ansible node (in MTputty SSH session), run the command ( ansible-playbook jinja2_fabric.yml ) to execute the playbook as shown below:  [root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml  Below screenshot shows the output of above command:     After the configuration push is successful,  login  (on MTputty SSH session) to  leaf-4  switch to verify configuration has been pushed by running below command:  show running-config bgp  The output of above command is shown below:      Congrats: you have successfully concluded this task by using jinja2 templates with Ansible for Cisco Nexus switches", 
            "title": "Step 12: Run Jinja2_fabric playbook"
        }, 
        {
            "location": "/task4-vxlan-nxos/", 
            "text": "In this section, you will build remaining VXLAN fabric using Ansible NXOS modules.  We will configure BGP neighbors between spine and leaf switching by using Ansible NXOS modules. The following NXOS ansible modules are used:\n\n\n\n\n\n\n\n\nnxos_feature\n\n\nManage features on Nexus switches\n\n\n\n\n\n\n\n\n\n\nnxos_bgp\n\n\nManage BGP config\n\n\n\n\n\n\nnxos_bgp_neighbor\n\n\nManage BGP neighbor config\n\n\n\n\n\n\nNxos_bgp_af\n\n\nManage BGP address-famility config\n\n\n\n\n\n\nNxos_bgp_neighbor_af\n\n\nManage BGP neighbor address-famility config\n\n\n\n\n\n\n\n\nIn comparison to Jinja2, NXOS modules are more abstract from configurations. There is no need to have knowledge of NXOS CLI to use NXOS modules. You will follow the steps to configure BGP, Multicast, VXLAN and EVPN. In each step, you will use different Ansible NXOS modules to accomplish the step. \n\n\n\n\nAfter each step, you can login the switches to verify configuration changes. \n\n\n\n\nStep 1: Create new playbook\n\n\nAgain we will use roles structure to make the playbook more modular.  The roles included in the new playbook are \u201c\nspine\n\u201d and \u201c\nleaf\n\u201d.  \n\n\n\n\n\n\nSwitch to \u201cAtom\u201d, \nright click\n on the folder 'LTRDDCN-1572' and create a new playbook named \nnxos_fabric.yml\n. Enter this file name and hit enter\n\n\n\n\n\n\nIn the \nnxos_fabric.yml\n enter the content as shown in below screenshot:\n\n\n\n\n\n\n---\n\n- hosts: spine\n  connection: local\n  roles:\n    - spine\n\n- hosts: leaf\n  connection: local\n  roles:\n    - leaf  \n\n\n\n\n![](pic/4-1-1.png)\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\nStep 2: Create new roles and vars\n\n\n\n\n\n\nOn the MTputty, go back to Ansible Server node (198.18.134.150), switch to \u2018\nroles\n\u2019 directory; \ncreate\n \u2018spine\u2019 and \u2018leaf\u2019 roles using ansible-galaxy as per below commands: \n\n\n[root@rhel7-tools LTRDCN-1572]# cd ~/LTRDCN-1572/roles/\n[root@rhel7-tools roles]# ansible-galaxy init spine \n ansible-galaxy init leaf\n\n\n\nBelow screeshot shows the output of above command:\n\n\n\n\n\n\n\n\n\u200b \n\n*  Switch to \u201c\nAtom\n\u201d and sync the new created folders between Ansible node and Remote desktop by pressing \nRight Click\n on the folder \nLTRDDCN-1572\n, then open \nRemote Sync\n select \nDownload Folder\n as shown below:\n\n\n    ![](pic/4-2-2.png)\n\n\n\n\n\nStep 3: Build playbook for Spine role - tasks:\n\n\n\u201c\nansible-galaxy\n\u201d automatically creates empty \u201c\nmain.yml\n\u201d file under \u201c\ntasks\n\u201d folder.  We are going to use \u201c\nAtom\n\u201d to edit the \nmain.yml\n file. \n\n\n\n\nOn ATOM, open up the project folder \nLTRDCN-1572\n and edit \nmain.yml\n file under \nroles/spine/tasks/\n to include following: \n\n\n\n\n\n\nNote: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.\n\n\n\n\n---\n# tasks file for spine\n#task to configure bgp neighbor to all leaf switches\n       - name: Enable BGP\n         nxos_feature:\n           feature: bgp\n           provider: \n{{nxos_provider}}\n\n           state: enabled\n         tags: bgp\n       - name: Configure BGP AS\n         nxos_bgp:\n           asn: \n{{ asn }}\n\n           router_id: \n{{ router_id }}\n\n           provider: \n{{ nxos_provider }}\n\n           state: present\n         tags: bgp\n       - name: Configure BGP AF\n         nxos_bgp_af:\n           asn: \n{{ asn }}\n\n           afi: ipv4\n           safi: unicast\n           provider: \n{{ nxos_provider }}\n\n         tags: bgp\n       - name: Configure iBGP neighbors\n         nxos_bgp_neighbor:\n           asn: \n{{ asn }}\n\n           neighbor: \n{{ item.neighbor }}\n\n           remote_as: \n{{ item.remote_as }}\n\n           update_source: \n{{ item.update_source }}\n\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ bgp_neighbors }}\n\n         tags: bgp\n       - name: Configure iBGP neighbor AF\n         nxos_bgp_neighbor_af:\n           asn: \n{{ asn }}\n\n           neighbor: \n{{ item.neighbor }}\n\n           afi: ipv4\n           safi: unicast\n           route_reflector_client: \ntrue\n\n           send_community: both\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ bgp_neighbors }}\n\n         tags: bgp\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nNote\n, in the above task/main.yml file multiple NXOS ansible modules have been used:\n\n\n\n\n\n\n\u201c\nnxos_feature\n\u201d module provides the capability to manage features in NX-OS features.  It is used to enable bgp as a feature in above configurations\n\n\n\n\n\n\n\u201c\nnxos_bgp\n\u201d module provides the capability to manage BGP configuration in NX-OS.  Here it is used to configure bgp\n\n\n\n\n\n\n\u201c\nnxos_bgp_af\n\u201d module provides the capability to manage BGP Address-family configuration in NX-OS.  \n\n\n\n\n\n\n\u201c\nnxos_bgp_neighbor\n\u201d module is used to configure the BGP Neighbour in NX-OS.  \n\n\n\n\n\n\n\u201c\nnxos_bgp_neighbor_af\n\u201d module provides the capability to manage BGP Address-family Neighbour configuration in NX-OS.  \n\n\n\n\n\n\n\n\nStep 4:  Build playbook for Spine role - vars\n\n\n\u201c\nansible-galaxy\n\u201d automatically creates empty \u201c\nmain.yml\n\u201d file under \u201c\nvars\n\u201d folder. We can use \u201c\nAtom\n\u201d to edit the main.yml file\n\n\n\n\nSwitch to \nATOM\n, then open up the project folder \nLTRDCN-1572\n from the left pane and \nopen\n \nmain.yml\n file under \u201c\nroles/spine/vars/\n\u201d and enter below content:\n\n\n\n\n---\n# vars file for spine\n  nxos_provider:\n    username: \n{{ user }}\n\n    password: \n{{ pwd }}\n\n    transport: nxapi\n    timeout: 30\n    host: \n{{ inventory_hostname }}\n\n\n  asn: 65000\n\n  bgp_neighbors:\n  - { remote_as: 65000, neighbor: 192.168.0.8, update_source: Loopback0 }\n  - { remote_as: 65000, neighbor: 192.168.0.10, update_source: Loopback0 }\n  - { remote_as: 65000, neighbor: 192.168.0.11, update_source: Loopback0 }\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\nStep 5:  Build playbook for leaf role - tasks:\n\n\n\u201c\nansible-galaxy\n\u201d automatically creates empty \u201c\nmain.yml\n\u201d file under \u201c\ntasks\n\u201d folder. We can use \u201c\nAtom\n\u201d to edit the main.yml file\n\n\n\n\nOn ATOM, open up the project folder \nLTRDCN-1572\n and edit \nmain.yml\n file under \nroles/leaf/tasks/\n to include following: \n\n\n\n\n---\n# tasks file for leaf\n#task to configure bgp neighbor to all spine switches\n       - name: Enable BGP\n         nxos_feature:\n           feature: bgp\n           provider: \n{{nxos_provider}}\n\n           state: enabled\n         tags: bgp\n       - name: Configure BGP AS\n         nxos_bgp:\n           asn: \n{{ asn }}\n\n           router_id: \n{{ router_id }}\n\n           provider: \n{{ nxos_provider }}\n\n           state: present\n         tags: bgp\n       - name: Configure BGP AF\n         nxos_bgp_af:\n           asn: \n{{ asn }}\n\n           afi: ipv4\n           safi: unicast\n           provider: \n{{ nxos_provider }}\n\n         tags: bgp\n       - name: Configure iBGP neighbors\n         nxos_bgp_neighbor:\n           asn: \n{{ asn }}\n\n           neighbor: \n{{ item.neighbor }}\n\n           remote_as: \n{{ item.remote_as }}\n\n           update_source: \n{{ item.update_source }}\n\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ bgp_neighbors }}\n\n         tags: bgp\n       - name: Configure iBGP neighbor AF\n         nxos_bgp_neighbor_af:\n           asn: \n{{ asn }}\n\n           neighbor: \n{{ item.neighbor }}\n\n           afi: ipv4\n           safi: unicast\n           send_community: both\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ bgp_neighbors }}\n\n         tags: bgp\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\nStep 6: Build playbook for leaf role - vars\n\n\n\u201c\nansible-galaxy\n\u201d automatically creates empty \u201c\nmain.yml\n\u201d file under \u201c\nvars\n\u201d folder. We can use \u201c\nAtom\n\u201d to edit the main.yml file\n\n\n\n\nSwitch to \nATOM\n, then open up the project folder \nLTRDCN-1572\n from the left pane and \nopen\n \nmain.yml\n file under \u201c\nroles/leaf/vars/\n\u201d and enter below content:\n\n\n\n\n---\n# vars file for leaf\n  nxos_provider:\n    username: \n{{ user }}\n\n    password: \n{{ pwd }}\n\n    transport: nxapi\n    timeout: 30\n    host: \n{{ inventory_hostname }}\n\n\n  asn: 65000\n\n  bgp_neighbors:\n  - { remote_as: 65000, neighbor: 192.168.0.6, update_source: Loopback0 }\n  - { remote_as: 65000, neighbor: 192.168.0.7, update_source: Loopback0 }\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\nStep 7: Execute playbook\n\n\n\n\n\n\nOn the Ansible node (in MTputty SSH session), run the command (\nansible-playbook nxos_fabric.yml --tags \"bgp\"\n) to execute the playbook as shown below:\n\n\n[root@rhel7-tools LTRDCN-1572]# cd ~/LTRDCN-1572\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"bgp\"\n\n\n\nBelow screenshots shows the execution of above playbook:\n\n\n\n  \n\n\n\n\n\n\nAfter the configuration push is successful, \nlogin\n (on MTputty SSH session) to \nleaf-1, leaf-3 or leaf-4\n or \nspine-1\n switch to verify configuration has been pushed by running below command and BGP neighbours are operational:\n\n\nshow ip bgp summary\n\n\n\nThe output of above command is shown below:\n\n\n\n\n\n\n\n\n\n\n\n\nStep 8: Configure Multicast using Ansible NXOS\n\n\nIn this section, we will be configuring underlay multicast to support BUM traffic in the VXLAN fabric. The NXOS modules we will be using in this section are \nnxos_feature    Manage fatures on Nexus switchs \nnxos_pim_interface  Manage PIM interface configuration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnxos_pim_rp_address\n\n\nManage static RP configuration\n\n\n\n\n\n\nnxos_config\n\n\nManage NXOS arbitrary configuration command\n\n\n\n\n\n\nnxos_interface_ospf\n\n\nManage configuration OSPF interface instance\n\n\n\n\n\n\nnxos_interface\n\n\nManage physical attribute of interface\n\n\n\n\n\n\n\n\n\n\nNote: there is no nxos ansible module for Anycast RP, we will use nxos_config to push the Anycast RP configuration\n\n\n\n\nEdit playbook for spine role\n\n\n\n\nUse \n\u201cAtom\u201d\n to edit the \n\u201cmain.yml\u201d\n file. Open up the project folder \n\u201cLTRDCN-1572\u201d\n and open \n\u201cmain.yml\u201d\n file under \n\u201croles/spine/tasks/\u201d\n and save the below tasks at the end the file: \n\n\n\n\n\n\nNote: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.\n \n\n\n\n\n#task to enable pim and configure anycast rp for underlay multicast\n       - name: Enable PIM\n         nxos_feature:\n           feature: pim\n           provider: \n{{nxos_provider}}\n\n           state: enabled\n         tags: multicast\n       - name: Configure Anycast RP interfce\n         nxos_interface:\n           interface: loopback1\n           provider: \n{{ nxos_provider }}\n\n         tags: multicast\n       - name: Configure IP Address on New LP1\n         nxos_ip_interface:\n            interface: loopback1\n            version: v4\n            addr: \n{{ loopback1 }}\n\n            mask: 32\n            provider: \n{{ nxos_provider }}\n\n         tags: multicast\n       - name: Configure PIM int\n         nxos_pim_interface:\n           interface: \n{{ item.interface }}\n\n           sparse: true\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{L3_interfaces}}\n\n         tags: multicast\n       - name: Enable OSPF on New LP1\n         nxos_interface_ospf:\n           interface: loopback1\n           ospf: 1\n           area: 0\n           provider: \n{{ nxos_provider }}\n\n         tags: multicast\n       - name: Configure PIM RP\n         nxos_pim_rp_address:\n           rp_address: \n{{ loopback1 }}\n\n           provider: \n{{ nxos_provider }}\n\n         tags: multicast\n       - name: Configure Anycast RP\n         nxos_config:\n           lines:\n            - \nip pim anycast-rp {{ loopback1 }} {{ s1_loopback }}\n\n            - \nip pim anycast-rp {{ loopback1 }} {{ s2_loopback }}\n\n           provider: \n{{ nxos_provider }}\n\n         tags: multicast\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nEdit variable file for Spine role\n\n\n\n\nUse \n\u201cAtom\u201d\n to edit the main.yml file. Open up the project folder \n\u201cLTRDCN-1572\u201d\n and open main.yml file under \n\u201croles/spine/vars/\u201d\n\n\n\n\n  L3_interfaces:\n  - { interface: Ethernet1/1 }\n  - { interface: Ethernet1/2 }\n  - { interface: Ethernet1/3 }\n  - { interface: Ethernet1/4 }\n  - { interface: loopback0 }\n  - { interface: loopback1 }\n  s1_loopback: 192.168.0.6\n  s2_loopback: 192.168.0.7\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nEdit playbook for leaf role\n\n\n\n\nuse \n\u201cAtom\u201d\n to edit the main.yml file. Open up the project folder \n\u201cLTRDCN-1572\u201d\n and open main.yml file under \n\u201croles/leaf/tasks/\u201d\n.   On Atom, Make sure to click \nFile-\nSave\n after entering the below data in this file so it is pushed to Ansible server:\n\n\n\n\n\n\nNote: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.\n \n\n\n\n\n#task to enable PIM for underlay multicast\n       - name: Enable PIM\n         nxos_feature:\n           feature: pim\n           provider: \n{{nxos_provider}}\n\n           state: enabled\n         tags: multicast\n       - name: Configure PIM int\n         nxos_pim_interface:\n           interface: \n{{ item.interface }}\n\n           sparse: true\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{L3_interfaces}}\n\n         tags: multicast\n       - name: Configure PIM RP\n         nxos_pim_rp_address:\n           rp_address: \n{{ rp_address }}\n\n           provider: \n{{ nxos_provider }}\n\n         tags: multicast\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nEdit variable file for leaf role\n\n\n\n\nUse \n\u201cAtom\u201d\n to edit the main.yml file. Open up the project folder \n\u201cLTRDCN-1572\u201d\n and open main.yml file under \n\u201croles/leaf/vars/\u201d\n.  On Atom, Make sure to click \nFile-\nSave\n after entering the below data in this file so it is pushed to Ansible server:\n\n\n\n\n  rp_address: 192.168.0.100\n  L3_interfaces:\n  - { interface: Ethernet1/1 }\n  - { interface: Ethernet1/2 }\n  - { interface: loopback0 }\n  - { interface: loopback1 }\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nRun the playbook and verify configuration changes\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"multicast\"\n\n\n\nBelow screenshots shows the output of above command:\n\n\n\n\n\n\n\n\nNote: login to any leaf (leaf1, leaf3, leaf4) or spine-1 switch to verify multicast configuration and PIM neighbors by executing command:\n \nshow ip pim nei\n\n\n\n\n\n\nBelow screenshot shows the output of above command \nshow ip pim nei\n from Spine-1\n\n\n\n\n\n\n\n\nStep 9: Configure VXLAN using Ansible\n\n\nIn this section, we will be configuring VXLAN on leaf and spine switches. The NXOS modules we will be using in this section are \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnxos_feature\n\n\nManages features on Nexus switches\n\n\n\n\n\n\nnxos_evpn_global\n\n\nHandles EVPN control plane for VXLAN\n\n\n\n\n\n\nnxos_vlan\n\n\nManages VLAN resources and attributes\n\n\n\n\n\n\nnxos_vrf\n\n\nManages global VRF configuration\n\n\n\n\n\n\nnxos_vrf_af\n\n\nManages VRF address falimily\n\n\n\n\n\n\nnxos_overlay_global\n\n\nConfiguration anycast gateway MAC\n\n\n\n\n\n\nnxos_vxlan_vtep\n\n\nManages VXLAN Network Virtualization Endpoint\n\n\n\n\n\n\nnxos_vxlan_vtep_vni\n\n\nCreates Virtual Network Identifier member\n\n\n\n\n\n\n\n\nEdit playbook for spine role\n\n\n\n\nUse \n\u201cAtom\u201d\n to edit the \u201cmain.yml\u201d file. Open up the project folder \n\u201cLTRDCN-1572\u201d\n and open \u201cmain.yml\u201d file under \n\u201croles/spine/tasks/\u201d\n.   \n\n\n\n\n\n\nNote: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.\n \n\n\n\n\n#task to configure vxlan fabric\n       - name: Enable VXLAN Feature\n         nxos_feature:\n            feature: \n{{item}}\n\n            provider: \n{{nxos_provider }}\n\n            state: enabled\n         with_items:\n            - nv overlay\n            - vn-segment-vlan-based\n         tags: vxlan\n       - name: Enable NV Overlay\n         nxos_evpn_global:\n           nv_overlay_evpn: true\n           provider: \n{{ nxos_provider }}\n\n         tags: vxlan\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nEdit variable file for Spine role\n\n\nNo new variable required for Spine \n\n\nEdit playbook for leaf role\n\n\n\n\nuse \n\u201cAtom\u201d\n to edit the main.yml file. Open up the project folder \n\u201cLTRDCN-1572\u201d\n and open \u201cmain.yml\u201d file under \n\u201croles/leaf/tasks/\u201d\n.  Add the below content in the file in addition to existing content, and then make sure to click \n\u201cFile\u201d-\n\u201cSave\u201d\n on Atom, so that the updated file is pushed to Ansible server.\n\n\n\n\n\n\nNote: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.\n \n\n\n\n\n#task to configure VXLAN fabric\n       - name: Enable VXLAN Feature\n         nxos_feature:\n           feature: \n{{ item }}\n\n           provider: \n{{nxos_provider}}\n\n           state: enabled\n         with_items:\n          - nv overlay\n          - vn-segment-vlan-based\n         tags: vxlan\n       - name: Enable NV Overlay\n         nxos_evpn_global:\n           nv_overlay_evpn: true\n           provider: \n{{ nxos_provider }}\n\n         tags: vxlan\n       - name: Configure VLAN to VNI\n         nxos_vlan:\n           vlan_id: \n{{ item.vlan_id }}\n\n           mapped_vni: \n{{ item.vni }}\n\n           name: \n{{ item.vlan_name }}\n\n           provider: \n{{ nxos_provider }}\n\n         with_items:\n         - \n{{ L2VNI }}\n\n         - \n{{ L3VNI }}\n\n         tags: vxlan\n       - name: Configure Tenant VRF\n         nxos_vrf:\n           vrf: Tenant-1\n           rd:  auto\n           vni: \n{{ L3VNI[0].vni }}\n\n           provider: \n{{ nxos_provider }}\n\n         tags: vxlan\n       - name: Configure VRF AF\n         nxos_vrf_af:\n           vrf: Tenant-1\n           route_target_both_auto_evpn: true\n           afi: ipv4\n           provider: \n{{ nxos_provider }}\n\n         tags: vxlan\n       - name: Configure Anycast GW\n         nxos_overlay_global:\n           anycast_gateway_mac: 0000.2222.3333\n           provider: \n{{ nxos_provider }}\n\n         tags: vxlan\n       - name: Configure L2VNI\n         nxos_interface:\n           interface: vlan\n{{ item.vlan_id }}\n\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ L2VNI }}\n\n         tags: vxlan\n       - name: Configure L3VNI\n         nxos_interface:\n           interface: vlan\n{{ L3VNI[0].vlan_id }}\n\n           provider: \n{{ nxos_provider }}\n\n         tags: vxlan\n       - name: Assign interface to Tenant VRF\n         nxos_vrf_interface:\n           vrf: Tenant-1\n           interface: \nvlan{{ item.vlan_id }}\n\n           provider: \n{{ nxos_provider }}\n\n         with_items:\n         - \n{{ L2VNI }}\n\n         - \n{{ L3VNI }}\n\n         tags: vxlan\n       - name: Configure SVI IP\n         nxos_ip_interface:\n           interface: \nvlan{{ item.vlan_id }}\n\n           addr: \n{{ item.ip_add }}\n\n           mask: \n{{ item.mask }}\n\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ L2VNI }}\n\n         tags: vxlan\n       - name: Configure L2VNI SVI\n         nxos_interface:\n           interface: vlan\n{{ item.vlan_id }}\n\n           fabric_forwarding_anycast_gateway: true\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ L2VNI }}\n\n         tags: vxlan\n       - name: Configure L3VNI SVI\n         nxos_interface:\n           interface: vlan\n{{ L3VNI[0].vlan_id }}\n\n           ip_forward: enable\n           provider: \n{{ nxos_provider }}\n\n         tags: vxlan\n       - name: Configure VTEP Tunnel\n         nxos_vxlan_vtep:\n            interface: nve1\n            shutdown: \nfalse\n\n            source_interface: Loopback1\n            host_reachability: \ntrue\n\n            provider: \n{{ nxos_provider }}\n\n         tags: vxlan\n       - name: Configure L2VNI to VTEP\n         nxos_vxlan_vtep_vni:\n            interface: nve1\n            vni: \n{{ item.vni }}\n\n            multicast_group: \n{{ item.mcast }}\n\n            provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ L2VNI }}\n\n         tags: vxlan\n       - name: Configure L3VNI to VTEP\n         nxos_vxlan_vtep_vni:\n            interface: nve1\n            vni: \n{{ L3VNI[0].vni }}\n\n            assoc_vrf: true\n            provider: \n{{ nxos_provider }}\n\n         tags: vxlan\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nEdit variable file for leaf role\n\n\n\n\nUse \n\u201cAtom\u201d\n to edit the main.yml file. Open up the project folder \n\u201cLTRDCN-1572\u201d\n and open main.yml file under \n\u201croles/leaf/vars/\u201d\n.  Add the below content in the file in addition to existing content, and then make sure to click \n\u201cFile\u201d-\n\u201cSave\u201d\n on Atom, so that the updated file is pushed to Ansible server.\n\n\n\n\n  L2VNI:\n  - { vlan_id: 140, vni: 50140, ip_add: 172.21.140.1, mask: 24, vlan_name: L2-VNI-140-Tenant1, mcast: 239.0.0.140 }\n  - { vlan_id: 141, vni: 50141, ip_add: 172.21.141.1, mask: 24, vlan_name: L2-VNI-141-Tenant1, mcast: 239.0.0.141 }\n  L3VNI:\n  - { vlan_id: 999, vlan_name: L3-VNI-999-Tenant1, vni: 50999 }\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nRun the playbook and verify configuration changes\n\n\n\n\n\n\nOn Ansible node (via SSH connection on MTputty), run the below command:\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"vxlan\"\n\n\n\n\n\n\n\nBelow is a partial screenshot for the output of above command:\n\n\n\n\n\n\n\n\nAfter finishing this task: \nlogin\n to any leaf switch (on MTPutty) to verify VXLAN configuration and the VNI by issuing the command:  \nshow nve vni\n\n\nBelow screenshot shows the output of above command from Leaf-4:\n\n\n \n\n\n\n\n\n\n\n\nStep 10: Configure EVPN using Ansible\n\n\nIn this section, we will be configuring BGP EVPN on leaf and spine switches. The NXOS modules we will be using in this section are \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnxos_bgp_af\n\n\nManage BGP address-famility config\n\n\n\n\n\n\nnxos_bgp_neighbor_af\n\n\nManage BGP neighbor address-famility config\n\n\n\n\n\n\nnxos_evpn_vni\n\n\nManage Cisco EVPN VXLAN Network Identifier\n\n\n\n\n\n\n\n\nEdit playbook for spine role\n\n\n\n\nUse \n\u201cAtom\u201d\n to edit the main.yml file. Open up the project folder \n\u201cLTRDCN-1572\u201d\n and open \u201cmain.yml\u201d file under \n\u201croles/spine/tasks/\u201d\n.  Add the below content in the file in addition to existing content,\n\n\nThen make sure to click \n\u201cFile\u201d-\n\u201cSave\u201d\n on Atom, so that the updated file is pushed to Ansible server. \n\n\n\n\n\n\nNote: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.\n \n\n\n\n\n# task to configure BGP EVPN\n       - name: Configure BGP EVPN\n         nxos_bgp_af:\n           asn: \n{{ asn }}\n\n           afi: l2vpn\n           safi: evpn\n           provider: \n{{ nxos_provider }}\n\n         tags: evpn\n       - name: Configure iBGP neighbor EVPN AF\n         nxos_bgp_neighbor_af:\n           asn: \n{{ asn }}\n\n           neighbor: \n{{ item.neighbor }}\n\n           afi: l2vpn\n           safi: evpn\n           route_reflector_client: \ntrue\n\n           send_community: both\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ bgp_neighbors }}\n\n         tags: evpn\n\n\n\n\nEdit variable file for Spine role\n\n\nNo new variables required for Spine \n\n\nEdit playbook for leaf role\n\n\n\n\nuse \n\u201cAtom\u201d\n to edit the \u201cmain.yml\u201d file. Open up the project folder \n\u201cLTRDCN-1572\u201d\n and open \u201cmain.yml\u201d file under \n\u201croles/leaf/tasks/\u201d\n.  Add the below content in the file in addition to existing content, and then make sure to click \n\u201cFile\u201d-\n\u201cSave\u201d\n on Atom, so that the updated file is pushed to Ansible server.\n\n\n\n\n\n\nNote: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.\n \n\n\n\n\n#task to configure BGP EVPN\n       - name: Configure BGP EVPN\n         nxos_bgp_af:\n           asn: \n{{ asn }}\n\n           afi: l2vpn\n           safi: evpn\n           provider: \n{{ nxos_provider }}\n\n         tags: evpn\n       - name: Configure iBGP neighbor EVPN AF\n         nxos_bgp_neighbor_af:\n           asn: \n{{ asn }}\n\n           neighbor: \n{{ item.neighbor }}\n\n           afi: l2vpn\n           safi: evpn\n           send_community: both\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ bgp_neighbors }}\n\n         tags: evpn\n       - name: Configure L2VNI RD/RT\n         nxos_evpn_vni:\n          vni: \n{{ item.vni }}\n\n          route_distinguisher: auto\n          route_target_both: auto\n          provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ L2VNI }}\n\n         tags: evpn\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\nEdit variable file for leaf role\n\n\nNo new variables required for Leaf\n\n\nRun the playbook and verify configuration changes\n\n\n\n\n\n\nOn the Ansible server (using MTPutty) run this playbook by running the below command:\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"evpn\"\n\n\n\nBelow screenshot shows the output of above playbook:\n\n\n\n\n\n\n\n\nAfter successful execution of the playbook: \nlogin\n to \nany leaf or spine\n switch to verify BGP EVPN configuration and evpn negibor by using command: \n\n\nshow bgp l2vpn evpn summary\n\n\n\nBelow screenshot shows the output of above command from leaf-4 switch.  As expected it shows the spine-1 and spine-2 as neighbors:\n\n\n\n\n\n\n\n\n\n\nStep 11: Run nxos_fabric playbook\n\n\n\n\nUp on this point, you have run the playbook during each step. You could re-run the whole playbook without giving any tags, but no new changes should be maded to the switches. \n[root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml\n\n\n\n\n\n\n\n\n\nCongratulation!\n You have just built VXLAN fabric using ansible + Jinja2 template and ansible + NXOS modules. \n\n\n\n\nLet\u2019s verify the VXLAN bridging and VXLAN routing from servers that are pre-configured in following VLANs and IPs \n\n\n\n\n\n\n\n\n\n\nServer Name\n\n\nConnect to switch\n\n\nIn VLAN\n\n\nIP of server\n\n\n\n\n\n\n\n\n\n\nServer-1\n\n\nLeaf-1\n\n\n140\n\n\n172.21.140.10\n\n\n\n\n\n\nServer-3\n\n\nLeaf-3\n\n\n140\n\n\n172.21.140.11\n\n\n\n\n\n\nServer-4\n\n\nLeaf-4\n\n\n141\n\n\n172.21.141.11\n\n\n\n\n\n\n\n\n\n\n\n\nSwitch to MTPuTTY and connect to server-1 with root/C1sco12345\n\n\n\n\n\n\nPing default gateway from server-1 \n\n\n\n\n\n\n[root@server-1 ~]# ping 172.21.140.1\nPING 172.21.140.1 (172.21.140.1) 56(84) bytes of data.\n64 bytes from 172.21.140.1: icmp_seq=2 ttl=255 time=15.7 ms\n64 bytes from 172.21.140.1: icmp_seq=3 ttl=255 time=4.11 ms\n\n\n\n\n\n\nPing server 3 and server 4 from server-1 (in same VLAN and inter-VLAN respectively):\n\n\n\n\n[[root@server-1 ~]# ping 172.21.140.11\nPING 172.21.140.11 (172.21.140.11) 56(84) bytes of data.\n64 bytes from 172.21.140.11: icmp_seq=1 ttl=64 time=1032 ms\n64 bytes from 172.21.140.11: icmp_seq=2 ttl=64 time=35.7 ms\n64 bytes from 172.21.140.11: icmp_seq=3 ttl=64 time=14.4 ms\n^C\n--- 172.21.140.11 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2112ms\nrtt min/avg/max/mdev = 14.431/360.839/1032.335/474.899 ms, pipe 2\n[root@server-1 ~]# ping 172.21.141.11\nPING 172.21.141.11 (172.21.141.11) 56(84) bytes of data.\n64 bytes from 172.21.141.11: icmp_seq=994 ttl=62 time=30.2 ms\n64 bytes from 172.21.141.11: icmp_seq=995 ttl=62 time=16.1 ms\n64 bytes from 172.21.141.11: icmp_seq=996 ttl=62 time=18.0 ms", 
            "title": "NXOS Fabric"
        }, 
        {
            "location": "/task4-vxlan-nxos/#step-1-create-new-playbook", 
            "text": "Again we will use roles structure to make the playbook more modular.  The roles included in the new playbook are \u201c spine \u201d and \u201c leaf \u201d.      Switch to \u201cAtom\u201d,  right click  on the folder 'LTRDDCN-1572' and create a new playbook named  nxos_fabric.yml . Enter this file name and hit enter    In the  nxos_fabric.yml  enter the content as shown in below screenshot:    ---\n\n- hosts: spine\n  connection: local\n  roles:\n    - spine\n\n- hosts: leaf\n  connection: local\n  roles:\n    - leaf    ![](pic/4-1-1.png)   Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Step 1: Create new playbook"
        }, 
        {
            "location": "/task4-vxlan-nxos/#step-2-create-new-roles-and-vars", 
            "text": "On the MTputty, go back to Ansible Server node (198.18.134.150), switch to \u2018 roles \u2019 directory;  create  \u2018spine\u2019 and \u2018leaf\u2019 roles using ansible-galaxy as per below commands:   [root@rhel7-tools LTRDCN-1572]# cd ~/LTRDCN-1572/roles/\n[root@rhel7-tools roles]# ansible-galaxy init spine   ansible-galaxy init leaf  Below screeshot shows the output of above command:     \u200b  \n*  Switch to \u201c Atom \u201d and sync the new created folders between Ansible node and Remote desktop by pressing  Right Click  on the folder  LTRDDCN-1572 , then open  Remote Sync  select  Download Folder  as shown below:      ![](pic/4-2-2.png)", 
            "title": "Step 2: Create new roles and vars"
        }, 
        {
            "location": "/task4-vxlan-nxos/#step-3-build-playbook-for-spine-role-tasks", 
            "text": "\u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder.  We are going to use \u201c Atom \u201d to edit the  main.yml  file.    On ATOM, open up the project folder  LTRDCN-1572  and edit  main.yml  file under  roles/spine/tasks/  to include following:     Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.   ---\n# tasks file for spine\n#task to configure bgp neighbor to all leaf switches\n       - name: Enable BGP\n         nxos_feature:\n           feature: bgp\n           provider:  {{nxos_provider}} \n           state: enabled\n         tags: bgp\n       - name: Configure BGP AS\n         nxos_bgp:\n           asn:  {{ asn }} \n           router_id:  {{ router_id }} \n           provider:  {{ nxos_provider }} \n           state: present\n         tags: bgp\n       - name: Configure BGP AF\n         nxos_bgp_af:\n           asn:  {{ asn }} \n           afi: ipv4\n           safi: unicast\n           provider:  {{ nxos_provider }} \n         tags: bgp\n       - name: Configure iBGP neighbors\n         nxos_bgp_neighbor:\n           asn:  {{ asn }} \n           neighbor:  {{ item.neighbor }} \n           remote_as:  {{ item.remote_as }} \n           update_source:  {{ item.update_source }} \n           provider:  {{ nxos_provider }} \n         with_items:  {{ bgp_neighbors }} \n         tags: bgp\n       - name: Configure iBGP neighbor AF\n         nxos_bgp_neighbor_af:\n           asn:  {{ asn }} \n           neighbor:  {{ item.neighbor }} \n           afi: ipv4\n           safi: unicast\n           route_reflector_client:  true \n           send_community: both\n           provider:  {{ nxos_provider }} \n         with_items:  {{ bgp_neighbors }} \n         tags: bgp   Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.    Note , in the above task/main.yml file multiple NXOS ansible modules have been used:    \u201c nxos_feature \u201d module provides the capability to manage features in NX-OS features.  It is used to enable bgp as a feature in above configurations    \u201c nxos_bgp \u201d module provides the capability to manage BGP configuration in NX-OS.  Here it is used to configure bgp    \u201c nxos_bgp_af \u201d module provides the capability to manage BGP Address-family configuration in NX-OS.      \u201c nxos_bgp_neighbor \u201d module is used to configure the BGP Neighbour in NX-OS.      \u201c nxos_bgp_neighbor_af \u201d module provides the capability to manage BGP Address-family Neighbour configuration in NX-OS.", 
            "title": "Step 3: Build playbook for Spine role - tasks:"
        }, 
        {
            "location": "/task4-vxlan-nxos/#step-4-build-playbook-for-spine-role-vars", 
            "text": "\u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c vars \u201d folder. We can use \u201c Atom \u201d to edit the main.yml file   Switch to  ATOM , then open up the project folder  LTRDCN-1572  from the left pane and  open   main.yml  file under \u201c roles/spine/vars/ \u201d and enter below content:   ---\n# vars file for spine\n  nxos_provider:\n    username:  {{ user }} \n    password:  {{ pwd }} \n    transport: nxapi\n    timeout: 30\n    host:  {{ inventory_hostname }} \n\n  asn: 65000\n\n  bgp_neighbors:\n  - { remote_as: 65000, neighbor: 192.168.0.8, update_source: Loopback0 }\n  - { remote_as: 65000, neighbor: 192.168.0.10, update_source: Loopback0 }\n  - { remote_as: 65000, neighbor: 192.168.0.11, update_source: Loopback0 }   Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Step 4:  Build playbook for Spine role - vars"
        }, 
        {
            "location": "/task4-vxlan-nxos/#step-5-build-playbook-for-leaf-role-tasks", 
            "text": "\u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c tasks \u201d folder. We can use \u201c Atom \u201d to edit the main.yml file   On ATOM, open up the project folder  LTRDCN-1572  and edit  main.yml  file under  roles/leaf/tasks/  to include following:    ---\n# tasks file for leaf\n#task to configure bgp neighbor to all spine switches\n       - name: Enable BGP\n         nxos_feature:\n           feature: bgp\n           provider:  {{nxos_provider}} \n           state: enabled\n         tags: bgp\n       - name: Configure BGP AS\n         nxos_bgp:\n           asn:  {{ asn }} \n           router_id:  {{ router_id }} \n           provider:  {{ nxos_provider }} \n           state: present\n         tags: bgp\n       - name: Configure BGP AF\n         nxos_bgp_af:\n           asn:  {{ asn }} \n           afi: ipv4\n           safi: unicast\n           provider:  {{ nxos_provider }} \n         tags: bgp\n       - name: Configure iBGP neighbors\n         nxos_bgp_neighbor:\n           asn:  {{ asn }} \n           neighbor:  {{ item.neighbor }} \n           remote_as:  {{ item.remote_as }} \n           update_source:  {{ item.update_source }} \n           provider:  {{ nxos_provider }} \n         with_items:  {{ bgp_neighbors }} \n         tags: bgp\n       - name: Configure iBGP neighbor AF\n         nxos_bgp_neighbor_af:\n           asn:  {{ asn }} \n           neighbor:  {{ item.neighbor }} \n           afi: ipv4\n           safi: unicast\n           send_community: both\n           provider:  {{ nxos_provider }} \n         with_items:  {{ bgp_neighbors }} \n         tags: bgp   Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Step 5:  Build playbook for leaf role - tasks:"
        }, 
        {
            "location": "/task4-vxlan-nxos/#step-6-build-playbook-for-leaf-role-vars", 
            "text": "\u201c ansible-galaxy \u201d automatically creates empty \u201c main.yml \u201d file under \u201c vars \u201d folder. We can use \u201c Atom \u201d to edit the main.yml file   Switch to  ATOM , then open up the project folder  LTRDCN-1572  from the left pane and  open   main.yml  file under \u201c roles/leaf/vars/ \u201d and enter below content:   ---\n# vars file for leaf\n  nxos_provider:\n    username:  {{ user }} \n    password:  {{ pwd }} \n    transport: nxapi\n    timeout: 30\n    host:  {{ inventory_hostname }} \n\n  asn: 65000\n\n  bgp_neighbors:\n  - { remote_as: 65000, neighbor: 192.168.0.6, update_source: Loopback0 }\n  - { remote_as: 65000, neighbor: 192.168.0.7, update_source: Loopback0 }   Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Step 6: Build playbook for leaf role - vars"
        }, 
        {
            "location": "/task4-vxlan-nxos/#step-7-execute-playbook", 
            "text": "On the Ansible node (in MTputty SSH session), run the command ( ansible-playbook nxos_fabric.yml --tags \"bgp\" ) to execute the playbook as shown below:  [root@rhel7-tools LTRDCN-1572]# cd ~/LTRDCN-1572\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"bgp\"  Below screenshots shows the execution of above playbook:        After the configuration push is successful,  login  (on MTputty SSH session) to  leaf-1, leaf-3 or leaf-4  or  spine-1  switch to verify configuration has been pushed by running below command and BGP neighbours are operational:  show ip bgp summary  The output of above command is shown below:", 
            "title": "Step 7: Execute playbook"
        }, 
        {
            "location": "/task4-vxlan-nxos/#step-8-configure-multicast-using-ansible-nxos", 
            "text": "In this section, we will be configuring underlay multicast to support BUM traffic in the VXLAN fabric. The NXOS modules we will be using in this section are \nnxos_feature    Manage fatures on Nexus switchs \nnxos_pim_interface  Manage PIM interface configuration           nxos_pim_rp_address  Manage static RP configuration    nxos_config  Manage NXOS arbitrary configuration command    nxos_interface_ospf  Manage configuration OSPF interface instance    nxos_interface  Manage physical attribute of interface      Note: there is no nxos ansible module for Anycast RP, we will use nxos_config to push the Anycast RP configuration", 
            "title": "Step 8: Configure Multicast using Ansible NXOS"
        }, 
        {
            "location": "/task4-vxlan-nxos/#edit-playbook-for-spine-role", 
            "text": "Use  \u201cAtom\u201d  to edit the  \u201cmain.yml\u201d  file. Open up the project folder  \u201cLTRDCN-1572\u201d  and open  \u201cmain.yml\u201d  file under  \u201croles/spine/tasks/\u201d  and save the below tasks at the end the file:     Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.     #task to enable pim and configure anycast rp for underlay multicast\n       - name: Enable PIM\n         nxos_feature:\n           feature: pim\n           provider:  {{nxos_provider}} \n           state: enabled\n         tags: multicast\n       - name: Configure Anycast RP interfce\n         nxos_interface:\n           interface: loopback1\n           provider:  {{ nxos_provider }} \n         tags: multicast\n       - name: Configure IP Address on New LP1\n         nxos_ip_interface:\n            interface: loopback1\n            version: v4\n            addr:  {{ loopback1 }} \n            mask: 32\n            provider:  {{ nxos_provider }} \n         tags: multicast\n       - name: Configure PIM int\n         nxos_pim_interface:\n           interface:  {{ item.interface }} \n           sparse: true\n           provider:  {{ nxos_provider }} \n         with_items:  {{L3_interfaces}} \n         tags: multicast\n       - name: Enable OSPF on New LP1\n         nxos_interface_ospf:\n           interface: loopback1\n           ospf: 1\n           area: 0\n           provider:  {{ nxos_provider }} \n         tags: multicast\n       - name: Configure PIM RP\n         nxos_pim_rp_address:\n           rp_address:  {{ loopback1 }} \n           provider:  {{ nxos_provider }} \n         tags: multicast\n       - name: Configure Anycast RP\n         nxos_config:\n           lines:\n            -  ip pim anycast-rp {{ loopback1 }} {{ s1_loopback }} \n            -  ip pim anycast-rp {{ loopback1 }} {{ s2_loopback }} \n           provider:  {{ nxos_provider }} \n         tags: multicast   Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Edit playbook for spine role"
        }, 
        {
            "location": "/task4-vxlan-nxos/#edit-variable-file-for-spine-role", 
            "text": "Use  \u201cAtom\u201d  to edit the main.yml file. Open up the project folder  \u201cLTRDCN-1572\u201d  and open main.yml file under  \u201croles/spine/vars/\u201d     L3_interfaces:\n  - { interface: Ethernet1/1 }\n  - { interface: Ethernet1/2 }\n  - { interface: Ethernet1/3 }\n  - { interface: Ethernet1/4 }\n  - { interface: loopback0 }\n  - { interface: loopback1 }\n  s1_loopback: 192.168.0.6\n  s2_loopback: 192.168.0.7   Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Edit variable file for Spine role"
        }, 
        {
            "location": "/task4-vxlan-nxos/#edit-playbook-for-leaf-role", 
            "text": "use  \u201cAtom\u201d  to edit the main.yml file. Open up the project folder  \u201cLTRDCN-1572\u201d  and open main.yml file under  \u201croles/leaf/tasks/\u201d .   On Atom, Make sure to click  File- Save  after entering the below data in this file so it is pushed to Ansible server:    Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.     #task to enable PIM for underlay multicast\n       - name: Enable PIM\n         nxos_feature:\n           feature: pim\n           provider:  {{nxos_provider}} \n           state: enabled\n         tags: multicast\n       - name: Configure PIM int\n         nxos_pim_interface:\n           interface:  {{ item.interface }} \n           sparse: true\n           provider:  {{ nxos_provider }} \n         with_items:  {{L3_interfaces}} \n         tags: multicast\n       - name: Configure PIM RP\n         nxos_pim_rp_address:\n           rp_address:  {{ rp_address }} \n           provider:  {{ nxos_provider }} \n         tags: multicast   Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Edit playbook for leaf role"
        }, 
        {
            "location": "/task4-vxlan-nxos/#edit-variable-file-for-leaf-role", 
            "text": "Use  \u201cAtom\u201d  to edit the main.yml file. Open up the project folder  \u201cLTRDCN-1572\u201d  and open main.yml file under  \u201croles/leaf/vars/\u201d .  On Atom, Make sure to click  File- Save  after entering the below data in this file so it is pushed to Ansible server:     rp_address: 192.168.0.100\n  L3_interfaces:\n  - { interface: Ethernet1/1 }\n  - { interface: Ethernet1/2 }\n  - { interface: loopback0 }\n  - { interface: loopback1 }   Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Edit variable file for leaf role"
        }, 
        {
            "location": "/task4-vxlan-nxos/#run-the-playbook-and-verify-configuration-changes", 
            "text": "[root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"multicast\"  Below screenshots shows the output of above command:     Note: login to any leaf (leaf1, leaf3, leaf4) or spine-1 switch to verify multicast configuration and PIM neighbors by executing command:\n  show ip pim nei    Below screenshot shows the output of above command  show ip pim nei  from Spine-1", 
            "title": "Run the playbook and verify configuration changes"
        }, 
        {
            "location": "/task4-vxlan-nxos/#step-9-configure-vxlan-using-ansible", 
            "text": "In this section, we will be configuring VXLAN on leaf and spine switches. The NXOS modules we will be using in this section are            nxos_feature  Manages features on Nexus switches    nxos_evpn_global  Handles EVPN control plane for VXLAN    nxos_vlan  Manages VLAN resources and attributes    nxos_vrf  Manages global VRF configuration    nxos_vrf_af  Manages VRF address falimily    nxos_overlay_global  Configuration anycast gateway MAC    nxos_vxlan_vtep  Manages VXLAN Network Virtualization Endpoint    nxos_vxlan_vtep_vni  Creates Virtual Network Identifier member", 
            "title": "Step 9: Configure VXLAN using Ansible"
        }, 
        {
            "location": "/task4-vxlan-nxos/#edit-playbook-for-spine-role_1", 
            "text": "Use  \u201cAtom\u201d  to edit the \u201cmain.yml\u201d file. Open up the project folder  \u201cLTRDCN-1572\u201d  and open \u201cmain.yml\u201d file under  \u201croles/spine/tasks/\u201d .       Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.     #task to configure vxlan fabric\n       - name: Enable VXLAN Feature\n         nxos_feature:\n            feature:  {{item}} \n            provider:  {{nxos_provider }} \n            state: enabled\n         with_items:\n            - nv overlay\n            - vn-segment-vlan-based\n         tags: vxlan\n       - name: Enable NV Overlay\n         nxos_evpn_global:\n           nv_overlay_evpn: true\n           provider:  {{ nxos_provider }} \n         tags: vxlan   Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Edit playbook for spine role"
        }, 
        {
            "location": "/task4-vxlan-nxos/#edit-variable-file-for-spine-role_1", 
            "text": "No new variable required for Spine", 
            "title": "Edit variable file for Spine role"
        }, 
        {
            "location": "/task4-vxlan-nxos/#edit-playbook-for-leaf-role_1", 
            "text": "use  \u201cAtom\u201d  to edit the main.yml file. Open up the project folder  \u201cLTRDCN-1572\u201d  and open \u201cmain.yml\u201d file under  \u201croles/leaf/tasks/\u201d .  Add the below content in the file in addition to existing content, and then make sure to click  \u201cFile\u201d- \u201cSave\u201d  on Atom, so that the updated file is pushed to Ansible server.    Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.     #task to configure VXLAN fabric\n       - name: Enable VXLAN Feature\n         nxos_feature:\n           feature:  {{ item }} \n           provider:  {{nxos_provider}} \n           state: enabled\n         with_items:\n          - nv overlay\n          - vn-segment-vlan-based\n         tags: vxlan\n       - name: Enable NV Overlay\n         nxos_evpn_global:\n           nv_overlay_evpn: true\n           provider:  {{ nxos_provider }} \n         tags: vxlan\n       - name: Configure VLAN to VNI\n         nxos_vlan:\n           vlan_id:  {{ item.vlan_id }} \n           mapped_vni:  {{ item.vni }} \n           name:  {{ item.vlan_name }} \n           provider:  {{ nxos_provider }} \n         with_items:\n         -  {{ L2VNI }} \n         -  {{ L3VNI }} \n         tags: vxlan\n       - name: Configure Tenant VRF\n         nxos_vrf:\n           vrf: Tenant-1\n           rd:  auto\n           vni:  {{ L3VNI[0].vni }} \n           provider:  {{ nxos_provider }} \n         tags: vxlan\n       - name: Configure VRF AF\n         nxos_vrf_af:\n           vrf: Tenant-1\n           route_target_both_auto_evpn: true\n           afi: ipv4\n           provider:  {{ nxos_provider }} \n         tags: vxlan\n       - name: Configure Anycast GW\n         nxos_overlay_global:\n           anycast_gateway_mac: 0000.2222.3333\n           provider:  {{ nxos_provider }} \n         tags: vxlan\n       - name: Configure L2VNI\n         nxos_interface:\n           interface: vlan {{ item.vlan_id }} \n           provider:  {{ nxos_provider }} \n         with_items:  {{ L2VNI }} \n         tags: vxlan\n       - name: Configure L3VNI\n         nxos_interface:\n           interface: vlan {{ L3VNI[0].vlan_id }} \n           provider:  {{ nxos_provider }} \n         tags: vxlan\n       - name: Assign interface to Tenant VRF\n         nxos_vrf_interface:\n           vrf: Tenant-1\n           interface:  vlan{{ item.vlan_id }} \n           provider:  {{ nxos_provider }} \n         with_items:\n         -  {{ L2VNI }} \n         -  {{ L3VNI }} \n         tags: vxlan\n       - name: Configure SVI IP\n         nxos_ip_interface:\n           interface:  vlan{{ item.vlan_id }} \n           addr:  {{ item.ip_add }} \n           mask:  {{ item.mask }} \n           provider:  {{ nxos_provider }} \n         with_items:  {{ L2VNI }} \n         tags: vxlan\n       - name: Configure L2VNI SVI\n         nxos_interface:\n           interface: vlan {{ item.vlan_id }} \n           fabric_forwarding_anycast_gateway: true\n           provider:  {{ nxos_provider }} \n         with_items:  {{ L2VNI }} \n         tags: vxlan\n       - name: Configure L3VNI SVI\n         nxos_interface:\n           interface: vlan {{ L3VNI[0].vlan_id }} \n           ip_forward: enable\n           provider:  {{ nxos_provider }} \n         tags: vxlan\n       - name: Configure VTEP Tunnel\n         nxos_vxlan_vtep:\n            interface: nve1\n            shutdown:  false \n            source_interface: Loopback1\n            host_reachability:  true \n            provider:  {{ nxos_provider }} \n         tags: vxlan\n       - name: Configure L2VNI to VTEP\n         nxos_vxlan_vtep_vni:\n            interface: nve1\n            vni:  {{ item.vni }} \n            multicast_group:  {{ item.mcast }} \n            provider:  {{ nxos_provider }} \n         with_items:  {{ L2VNI }} \n         tags: vxlan\n       - name: Configure L3VNI to VTEP\n         nxos_vxlan_vtep_vni:\n            interface: nve1\n            vni:  {{ L3VNI[0].vni }} \n            assoc_vrf: true\n            provider:  {{ nxos_provider }} \n         tags: vxlan   Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Edit playbook for leaf role"
        }, 
        {
            "location": "/task4-vxlan-nxos/#edit-variable-file-for-leaf-role_1", 
            "text": "Use  \u201cAtom\u201d  to edit the main.yml file. Open up the project folder  \u201cLTRDCN-1572\u201d  and open main.yml file under  \u201croles/leaf/vars/\u201d .  Add the below content in the file in addition to existing content, and then make sure to click  \u201cFile\u201d- \u201cSave\u201d  on Atom, so that the updated file is pushed to Ansible server.     L2VNI:\n  - { vlan_id: 140, vni: 50140, ip_add: 172.21.140.1, mask: 24, vlan_name: L2-VNI-140-Tenant1, mcast: 239.0.0.140 }\n  - { vlan_id: 141, vni: 50141, ip_add: 172.21.141.1, mask: 24, vlan_name: L2-VNI-141-Tenant1, mcast: 239.0.0.141 }\n  L3VNI:\n  - { vlan_id: 999, vlan_name: L3-VNI-999-Tenant1, vni: 50999 }   Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Edit variable file for leaf role"
        }, 
        {
            "location": "/task4-vxlan-nxos/#run-the-playbook-and-verify-configuration-changes_1", 
            "text": "On Ansible node (via SSH connection on MTputty), run the below command:  [root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"vxlan\"    Below is a partial screenshot for the output of above command:     After finishing this task:  login  to any leaf switch (on MTPutty) to verify VXLAN configuration and the VNI by issuing the command:   show nve vni  Below screenshot shows the output of above command from Leaf-4:", 
            "title": "Run the playbook and verify configuration changes"
        }, 
        {
            "location": "/task4-vxlan-nxos/#step-10-configure-evpn-using-ansible", 
            "text": "In this section, we will be configuring BGP EVPN on leaf and spine switches. The NXOS modules we will be using in this section are            nxos_bgp_af  Manage BGP address-famility config    nxos_bgp_neighbor_af  Manage BGP neighbor address-famility config    nxos_evpn_vni  Manage Cisco EVPN VXLAN Network Identifier", 
            "title": "Step 10: Configure EVPN using Ansible"
        }, 
        {
            "location": "/task4-vxlan-nxos/#edit-playbook-for-spine-role_2", 
            "text": "Use  \u201cAtom\u201d  to edit the main.yml file. Open up the project folder  \u201cLTRDCN-1572\u201d  and open \u201cmain.yml\u201d file under  \u201croles/spine/tasks/\u201d .  Add the below content in the file in addition to existing content,  Then make sure to click  \u201cFile\u201d- \u201cSave\u201d  on Atom, so that the updated file is pushed to Ansible server.     Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.     # task to configure BGP EVPN\n       - name: Configure BGP EVPN\n         nxos_bgp_af:\n           asn:  {{ asn }} \n           afi: l2vpn\n           safi: evpn\n           provider:  {{ nxos_provider }} \n         tags: evpn\n       - name: Configure iBGP neighbor EVPN AF\n         nxos_bgp_neighbor_af:\n           asn:  {{ asn }} \n           neighbor:  {{ item.neighbor }} \n           afi: l2vpn\n           safi: evpn\n           route_reflector_client:  true \n           send_community: both\n           provider:  {{ nxos_provider }} \n         with_items:  {{ bgp_neighbors }} \n         tags: evpn", 
            "title": "Edit playbook for spine role"
        }, 
        {
            "location": "/task4-vxlan-nxos/#edit-variable-file-for-spine-role_2", 
            "text": "No new variables required for Spine", 
            "title": "Edit variable file for Spine role"
        }, 
        {
            "location": "/task4-vxlan-nxos/#edit-playbook-for-leaf-role_2", 
            "text": "use  \u201cAtom\u201d  to edit the \u201cmain.yml\u201d file. Open up the project folder  \u201cLTRDCN-1572\u201d  and open \u201cmain.yml\u201d file under  \u201croles/leaf/tasks/\u201d .  Add the below content in the file in addition to existing content, and then make sure to click  \u201cFile\u201d- \u201cSave\u201d  on Atom, so that the updated file is pushed to Ansible server.    Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.     #task to configure BGP EVPN\n       - name: Configure BGP EVPN\n         nxos_bgp_af:\n           asn:  {{ asn }} \n           afi: l2vpn\n           safi: evpn\n           provider:  {{ nxos_provider }} \n         tags: evpn\n       - name: Configure iBGP neighbor EVPN AF\n         nxos_bgp_neighbor_af:\n           asn:  {{ asn }} \n           neighbor:  {{ item.neighbor }} \n           afi: l2vpn\n           safi: evpn\n           send_community: both\n           provider:  {{ nxos_provider }} \n         with_items:  {{ bgp_neighbors }} \n         tags: evpn\n       - name: Configure L2VNI RD/RT\n         nxos_evpn_vni:\n          vni:  {{ item.vni }} \n          route_distinguisher: auto\n          route_target_both: auto\n          provider:  {{ nxos_provider }} \n         with_items:  {{ L2VNI }} \n         tags: evpn   Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.", 
            "title": "Edit playbook for leaf role"
        }, 
        {
            "location": "/task4-vxlan-nxos/#edit-variable-file-for-leaf-role_2", 
            "text": "No new variables required for Leaf", 
            "title": "Edit variable file for leaf role"
        }, 
        {
            "location": "/task4-vxlan-nxos/#run-the-playbook-and-verify-configuration-changes_2", 
            "text": "On the Ansible server (using MTPutty) run this playbook by running the below command:  [root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml --tags \"evpn\"  Below screenshot shows the output of above playbook:     After successful execution of the playbook:  login  to  any leaf or spine  switch to verify BGP EVPN configuration and evpn negibor by using command:   show bgp l2vpn evpn summary  Below screenshot shows the output of above command from leaf-4 switch.  As expected it shows the spine-1 and spine-2 as neighbors:", 
            "title": "Run the playbook and verify configuration changes"
        }, 
        {
            "location": "/task4-vxlan-nxos/#step-11-run-nxos_fabric-playbook", 
            "text": "Up on this point, you have run the playbook during each step. You could re-run the whole playbook without giving any tags, but no new changes should be maded to the switches.  [root@rhel7-tools LTRDCN-1572]# ansible-playbook nxos_fabric.yml     Congratulation!  You have just built VXLAN fabric using ansible + Jinja2 template and ansible + NXOS modules.    Let\u2019s verify the VXLAN bridging and VXLAN routing from servers that are pre-configured in following VLANs and IPs       Server Name  Connect to switch  In VLAN  IP of server      Server-1  Leaf-1  140  172.21.140.10    Server-3  Leaf-3  140  172.21.140.11    Server-4  Leaf-4  141  172.21.141.11       Switch to MTPuTTY and connect to server-1 with root/C1sco12345    Ping default gateway from server-1     [root@server-1 ~]# ping 172.21.140.1\nPING 172.21.140.1 (172.21.140.1) 56(84) bytes of data.\n64 bytes from 172.21.140.1: icmp_seq=2 ttl=255 time=15.7 ms\n64 bytes from 172.21.140.1: icmp_seq=3 ttl=255 time=4.11 ms   Ping server 3 and server 4 from server-1 (in same VLAN and inter-VLAN respectively):   [[root@server-1 ~]# ping 172.21.140.11\nPING 172.21.140.11 (172.21.140.11) 56(84) bytes of data.\n64 bytes from 172.21.140.11: icmp_seq=1 ttl=64 time=1032 ms\n64 bytes from 172.21.140.11: icmp_seq=2 ttl=64 time=35.7 ms\n64 bytes from 172.21.140.11: icmp_seq=3 ttl=64 time=14.4 ms\n^C\n--- 172.21.140.11 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2112ms\nrtt min/avg/max/mdev = 14.431/360.839/1032.335/474.899 ms, pipe 2\n[root@server-1 ~]# ping 172.21.141.11\nPING 172.21.141.11 (172.21.141.11) 56(84) bytes of data.\n64 bytes from 172.21.141.11: icmp_seq=994 ttl=62 time=30.2 ms\n64 bytes from 172.21.141.11: icmp_seq=995 ttl=62 time=16.1 ms\n64 bytes from 172.21.141.11: icmp_seq=996 ttl=62 time=18.0 ms", 
            "title": "Step 11: Run nxos_fabric playbook"
        }, 
        {
            "location": "/task5-day2-operation/", 
            "text": "Task 5: Day 2 operation using Ansible\n\n\nIn this section, we will use automation to perform following day 2 operation tasks. \n\n\n\n\nBackup running configurations on all leaf and spine switches \n\n\nVerify underlay ospf, bgp and pim neighbors \n\n\nVerify overlay nve peer, host route, bgp update \n\n\nBaseline configuration comparison \n\n\nAdd new VNIs into the existing fabric\n\n\n\n\n\n\nStep 1: Backup running configurations\n\n\nIn this section, you will use \nios_config\n module to backup running configuration on each switch, the backup file will be saved to a local \n\u201cbackup\u201d\n folder.  The backup argument create a full backup of the current running-config of each switch.  The backup file is written to the backup folder in the playbook root directory. If the directory does not exist, it is created.\n\n\n\n\n\n\nOn \nAtom\n, open up the project folder \n\u201cLTRDCN-1572\u201d\n and create new file under \nLTRDCN-1572\n. Name the new file \nget_config.yml\n.\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\n---\n  - hosts: spine,leaf,jinja2_leaf,jinja2_spine\n    connection: local\n    vars:\n      ios_provider:\n        transport: nxapi\n        username: \n{{ user }}\n\n        password: \n{{ pwd }}\n\n        host: \n{{ inventory_hostname }}\n\n    tasks:\n      - name: save running\n        nxos_config:\n           provider: \n{{ ios_provider }}\n\n           backup: yes\n           timeout: 20\n\n\n\n\n\n\n\n\n\nOn the Ansible node (using MTputty via SSH), run \n\u201cget_config.yml\u201d\n playbook and verify the backup configurations in \n\u201cbackup\u201d\n folder by using below commands:\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook get_config.yml\n[root@rhel7-tools LTRDCN-1572]# ls -lrt\n[root@rhel7-tools LTRDCN-1572]# ls backup\n\n\n\nYou may further view the contents of the files under backup folder by using cat, less or more commands.  Below screenshot shows the output of above commands\n\n\n\n\n\n\n\n\n\n\nStep 2: Verify underlay and overlay\n\n\nIn this step, you will verify underlay and overlay operation using ansible playbook. The playbook will be applied to all leaf switches to verify the below commands:\n\n\nUnderlay\n \n\n\n-   show ip ospf neighbor\n-   show ip bgp sum\n-   show ip pim neighbor\n\n\n\nOverlay\n\n\n-   show nve vni\n-   show nve peer\n-   show ip route vrf Tenant-1\n-   show bgp l2vpn evpn\n-   show l2route evpn mac-ip all\n\n\n\n\n\nSwitch to \u201cAtom\u201d, \nright click\n on the folder \nLTRDDCN-1572\n and create a new playbook named \nverify_fabric.yml\n. Enter this file name and hit enter.\n\n\n\n\n---\n  - hosts: leaf, jinja2_leaf\n    connection: local\n    gather_facts: false\n    vars:\n      ios_provider:\n         username: \n{{ user }}\n\n         password: \n{{ pwd }}\n\n         timeout: 30\n         host: \n{{ inventory_hostname }}\n\n    tasks:\n      - name: verify underlay\n        register: underlay_output\n        ios_command:\n          provider: \n{{ ios_provider }}\n\n          commands:\n            - show ip ospf neighbors\n            - show ip bgp sum\n            - show ip pim neighbor\n        tags: underlay\n      - debug: var=underlay_output.stdout_lines\n        tags: underlay\n#      - copy: content=\n{{underlay_output | to_nice_json}}\n dest=\nverify/{{inventory_hostname}}_underlay\n\n      - name: Verify Overlay\n        register: overlay_output\n        ios_command:\n          provider: \n{{ ios_provider }}\n\n          commands:\n            - show nve vni\n            - show nve peer\n            - show ip route vrf Tenant-1\n            - show bgp l2vpn evpn\n            - show l2route evpn mac-ip all\n        tags: overlay\n      - debug: var=overlay_output.stdout_lines\n        tags: overlay\n\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\nOn the Ansible node (via MTPutty), run \nverify_fabric.yml\n playbook and verify the output for \nunderlay\n by executing below command (using respective tag): \n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags \"underlay\"\n\n\n\n\n\n\n\n\n\nThe output shows ospf, bgp and pim neighbors for all leaf switches\n\n\n\n\n\n\n\n\nBelow screenshot shows the partial output of above command:\n\n\n\n\n\n\n\n\nHere is a log of execution of above command:\n\n\n [root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags \nunderlay\n\n\nPLAY [leaf, jinja2_leaf] ************************************************************************************************************************************************\n\nTASK [verify underlay] **************************************************************************************************************************************************\n [WARNING]: argument username has been deprecated and will be removed in a future version\n\n [WARNING]: argument timeout has been deprecated and will be removed in a future version\n\n [WARNING]: argument password has been deprecated and will be removed in a future version\n\nok: [198.18.4.101]\nok: [198.18.4.104]\nok: [198.18.4.103]\n\nTASK [debug] ************************************************************************************************************************************************************\nok: [198.18.4.101] =\n {\n    \nunderlay_output.stdout_lines\n: [\n        [\n            \nOSPF Process ID 1 VRF default\n,\n            \n Total number of neighbors: 2\n,\n            \n Neighbor ID     Pri State            Up Time  Address         Interface\n,\n            \n 192.168.0.6       1 FULL/ -          1d03h    10.0.0.21       Eth1/1 \n,\n            \n 192.168.0.7       1 FULL/ -          1d03h    10.0.128.5      Eth1/2\n\n        ],\n        [\n            \nBGP summary information for VRF default, address family IPv4 Unicast\n,\n            \nBGP router identifier 192.168.0.8, local AS number 65000\n,\n            \nBGP table version is 8, IPv4 Unicast config peers 2, capable peers 2\n,\n            \n0 network entries and 0 paths using 0 bytes of memory\n,\n            \nBGP attribute entries [0/0], BGP AS path entries [0/0]\n,\n            \nBGP community entries [0/0], BGP clusterlist entries [0/0]\n,\n            \n,\n            \nNeighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\n,\n            \n192.168.0.6     4 65000      52      54        8    0    0 00:24:18 0         \n,\n            \n192.168.0.7     4 65000      47      49        8    0    0 00:30:45 0\n\n        ],\n        [\n            \nPIM Neighbor Status for VRF \\\ndefault\\\n,\n            \nNeighbor        Interface            Uptime    Expires   DR       Bidir-  BFD\n,\n            \n                                                         Priority Capable State\n,\n            \n10.0.0.21       Ethernet1/1          00:15:51  00:01:43  1        yes     n/a\n,\n            \n10.0.128.5      Ethernet1/2          00:15:45  00:01:23  1        yes     n/a\n\n        ]\n    ]\n}\nok: [198.18.4.104] =\n {\n    \nunderlay_output.stdout_lines\n: [\n        [\n            \nOSPF Process ID 1 VRF default\n,\n            \n Total number of neighbors: 2\n,\n            \n Neighbor ID     Pri State            Up Time  Address         Interface\n,\n            \n 192.168.0.6       1 FULL/ -          10:20:18 10.0.128.1      Eth1/1 \n,\n            \n 192.168.0.7       1 FULL/ -          10:20:15 10.0.128.17     Eth1/2\n\n        ],\n        [\n            \nBGP summary information for VRF default, address family IPv4 Unicast\n,\n            \nBGP router identifier 192.168.0.11, local AS number 65000\n,\n            \nBGP table version is 4, IPv4 Unicast config peers 2, capable peers 2\n,\n            \n0 network entries and 0 paths using 0 bytes of memory\n,\n            \nBGP attribute entries [0/0], BGP AS path entries [0/0]\n,\n            \nBGP community entries [0/0], BGP clusterlist entries [0/0]\n,\n            \n,\n            \nNeighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\n,\n            \n192.168.0.6     4 65000      32      32        4    0    0 00:24:03 0         \n,\n            \n192.168.0.7     4 65000      34      34        4    0    0 00:25:16 0\n\n        ],\n        [\n            \nPIM Neighbor Status for VRF \\\ndefault\\\n,\n            \nNeighbor        Interface            Uptime    Expires   DR       Bidir-  BFD\n,\n            \n                                                         Priority Capable State\n,\n            \n10.0.128.1      Ethernet1/1          00:19:15  00:01:31  1        yes     n/a\n,\n            \n10.0.128.17     Ethernet1/2          00:25:43  00:01:18  1        yes     n/a\n\n        ]\n    ]\n}\nok: [198.18.4.103] =\n {\n    \nunderlay_output.stdout_lines\n: [\n        [\n            \nOSPF Process ID 1 VRF default\n,\n            \n Total number of neighbors: 2\n,\n            \n Neighbor ID     Pri State            Up Time  Address         Interface\n,\n            \n 192.168.0.6       1 FULL/ -          1d03h    10.0.0.29       Eth1/1 \n,\n            \n 192.168.0.7       1 FULL/ -          1d03h    10.0.128.13     Eth1/2\n\n        ],\n        [\n            \nBGP summary information for VRF default, address family IPv4 Unicast\n,\n            \nBGP router identifier 192.168.0.10, local AS number 65000\n,\n            \nBGP table version is 8, IPv4 Unicast config peers 2, capable peers 2\n,\n            \n0 network entries and 0 paths using 0 bytes of memory\n,\n            \nBGP attribute entries [0/0], BGP AS path entries [0/0]\n,\n            \nBGP community entries [0/0], BGP clusterlist entries [0/0]\n,\n            \n,\n            \nNeighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd\n,\n            \n192.168.0.6     4 65000      52      53        8    0    0 00:24:19 0         \n,\n            \n192.168.0.7     4 65000      47      49        8    0    0 00:30:47 0\n\n        ],\n        [\n            \nPIM Neighbor Status for VRF \\\ndefault\\\n,\n            \nNeighbor        Interface            Uptime    Expires   DR       Bidir-  BFD\n,\n            \n                                                         Priority Capable State\n,\n            \n10.0.0.29       Ethernet1/1          00:15:53  00:01:23  1        yes     n/a\n,\n            \n10.0.128.13     Ethernet1/2          00:15:47  00:01:37  1        yes     n/a\n\n        ]\n    ]\n}\n\nPLAY RECAP **************************************************************************************************************************************************************\n198.18.4.101               : ok=2    changed=0    unreachable=0    failed=0\n198.18.4.103               : ok=2    changed=0    unreachable=0    failed=0\n198.18.4.104               : ok=2    changed=0    unreachable=0    failed=0\n\n\n\n\nNext:\n\n\n\n\nRun \nverify_fabric.yml\n playbook and verify the output for \noverlay\n using the respective tag in the command (as shown below):\n\n\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags \noverlay\n\n\n\n\n\n\n\nThe output shows nve tunnel peer, host route in bgp EVPN from all leaf switches\n\n\n\n\n\n\n\n\nBelow screenshot of the partial output of above command:\n\n\n\n\n\n\n\n\nBelow shows the complete log output of execution of above playbook command.   Verify the output for \nvne vni\n status, \nvne\n dynamic neighbors, type host \nmac+ip evpn route update for each L2VNI, l2fib\n information.\n\n\n\n\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags \noverlay\n\n\nPLAY [leaf, jinja2_leaf] **********************************************************************************************************************************************\n\nTASK [Verify Overlay] *************************************************************************************************************************************************\n [WARNING]: argument username has been deprecated and will be removed in a future version\n\n [WARNING]: argument timeout has been deprecated and will be removed in a future version\n\n [WARNING]: argument password has been deprecated and will be removed in a future version\n\nok: [198.18.4.101]\nok: [198.18.4.104]\nok: [198.18.4.103]\n\nTASK [debug] **********************************************************************************************************************************************************\nok: [198.18.4.101] =\n {\n    \noverlay_output.stdout_lines\n: [\n        [\n            \nCodes: CP - Control Plane        DP - Data Plane          \n,\n            \n       UC - Unconfigured         SA - Suppress ARP        \n,\n            \n       SU - Suppress Unknown Unicast\n,\n            \n \n,\n            \nInterface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags\n,\n            \n--------- -------- ----------------- ----- ---- ------------------ -----\n,\n            \nnve1      50140    239.0.0.140       Up    CP   L2 [140]                  \n,\n            \nnve1      50141    239.0.0.141       Up    CP   L2 [141]                  \n,\n            \nnve1      50999    n/a               Up    CP   L3 [Tenant-1]\n\n        ],\n        [\n            \nInterface Peer-IP          State LearnType Uptime   Router-Mac       \n,\n            \n--------- ---------------  ----- --------- -------- -----------------\n,\n            \nnve1      192.168.0.110    Up    CP        00:03:19 000c.2939.f53f   \n,\n            \nnve1      192.168.0.111    Up    CP        00:01:12 000c.2951.176f\n\n        ],\n        [\n            \nIP Route Table for VRF \\\nTenant-1\\\n,\n            \n'*' denotes best ucast next-hop\n,\n            \n'**' denotes best mcast next-hop\n,\n            \n'[x/y]' denotes [preference/metric]\n,\n            \n'%\nstring\n' in via output denotes VRF \nstring\n,\n            \n,\n            \n172.21.140.0/24, ubest/mbest: 1/0, attached\n,\n            \n    *via 172.21.140.1, Vlan140, [0/0], 00:05:39, direct\n,\n            \n172.21.140.1/32, ubest/mbest: 1/0, attached\n,\n            \n    *via 172.21.140.1, Vlan140, [0/0], 00:05:39, local\n,\n            \n172.21.140.10/32, ubest/mbest: 1/0, attached\n,\n            \n    *via 172.21.140.10, Vlan140, [190/0], 00:05:33, hmm\n,\n            \n172.21.140.11/32, ubest/mbest: 1/0\n,\n            \n    *via 192.168.0.110%default, [200/0], 00:01:50, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006e encap: VXLAN\n,\n            \n \n,\n            \n172.21.141.0/24, ubest/mbest: 1/0, attached\n,\n            \n    *via 172.21.141.1, Vlan141, [0/0], 00:05:38, direct\n,\n            \n172.21.141.1/32, ubest/mbest: 1/0, attached\n,\n            \n    *via 172.21.141.1, Vlan141, [0/0], 00:05:38, local\n,\n            \n172.21.141.11/32, ubest/mbest: 1/0\n,\n            \n    *via 192.168.0.111%default, [200/0], 00:01:12, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006f encap: VXLAN\n\n        ],\n        [\n            \nBGP routing table information for VRF default, address family L2VPN EVPN\n,\n            \nBGP table version is 35, Local Router ID is 192.168.0.8\n,\n            \nStatus: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid, \n-best\n,\n            \nPath type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected\n,\n            \nOrigin codes: i - IGP, e - EGP, ? - incomplete, | - multipath, \n - backup\n,\n            \n,\n            \n   Network            Next Hop            Metric     LocPrf     Weight Path\n,\n            \nRoute Distinguisher: 192.168.0.8:32907    (L2VNI 50140)\n,\n            \n*\ni[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\n,\n            \n                      192.168.0.110                     100          0 i\n,\n            \n*\nl[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272\n,\n            \n                      192.168.0.18                      100      32768 i\n,\n            \n*\ni[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\n,\n            \n                      192.168.0.110                     100          0 i\n,\n            \n,\n            \nRoute Distinguisher: 192.168.0.8:32908    (L2VNI 50141)\n,\n            \n*\ni[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\n,\n            \n                      192.168.0.111                     100          0 i\n,\n            \n*\ni[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\n,\n            \n                      192.168.0.111                     100          0 i\n,\n            \n,\n            \nRoute Distinguisher: 192.168.0.10:32907\n,\n            \n* i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216\n,\n            \n                      192.168.0.110                     100          0 i\n,\n            \n*\ni                   192.168.0.110                     100          0 i\n,\n            \n*\ni[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\n,\n            \n                      192.168.0.110                     100          0 i\n,\n            \n* i                   192.168.0.110                     100          0 i\n,\n            \n,\n            \nRoute Distinguisher: 192.168.0.11:32908\n,\n            \n* i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216\n,\n            \n                      192.168.0.111                     100          0 i\n,\n            \n*\ni                   192.168.0.111                     100          0 i\n,\n            \n*\ni[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\n,\n            \n                      192.168.0.111                     100          0 i\n,\n            \n* i                   192.168.0.111                     100          0 i\n,\n            \n,\n            \nRoute Distinguisher: 192.168.0.8:3    (L3VNI 50999)\n,\n            \n*\ni[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272\n,\n            \n                      192.168.0.111                     100          0 i\n,\n            \n*\ni[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272\n,\n            \n                      192.168.0.110                     100          0 i\n\n        ],\n        [\n            \nFlags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \n,\n            \n(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\n,\n            \n(Ps):Peer Sync (Ro):Re-Originated \n,\n            \nTopology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops      \n,\n            \n----------- -------------- ------ ---------- --------------- ---------------\n,\n            \n140         0050.56a0.7630 HMM    --            0          172.21.140.10  Local          \n,\n            \n140         0050.56a0.b5d1 BGP    --            0          172.21.140.11  192.168.0.110  \n,\n            \n141         000c.2979.f00d BGP    --            1          172.21.141.11  192.168.0.111\n\n        ]\n    ]\n}\n\n\n\n\n\n\nStep 3: Baseline configuration comparison\n\n\nIn this section we will compare the running configuration with baseline configuration for configuration compliance check. The configuration file that we backed in tak 1 will be used as baseline configuration. \n\n\nIn this playbook, you will use \n\u201clookup\u201d\n module to find the backup filename generated in Step 1. Then you will use \ndiff_against\n function in \nnxos_config\n module to compare running configuration. \n\n\n\n\nOn Atom, Open up the project folder \nLTRDCN-1572\n and create new file under \n\u201cLTRDCN-1572\u201d\n. Name the new file \nverify_config.yml\n and enter below data in this playbook:\n\n\n\n\n---\n   - hosts: jinja2_leaf,leaf,jinja2_spine,spine\n     connection: local\n     gather_facts: flase\n     vars:\n       nxos_provider:\n          transport: nxapi\n          username: \n{{ user }}\n\n          password: \n{{ pwd }}\n\n          timeout: 30\n          host: \n{{ inventory_hostname }}\n\n       filename: \n{{ lookup('pipe', 'ls backup/{{ inventory_hostname}}_config.*')}}\n\n     tasks:\n       - name: configure compliance\n         register: diff_config\n#         when: (inventory_hostname in groups['leaf']) or (inventory_hostname in groups['jinja2_leaf'])\n         nxos_config:\n           provider: \n{{ nxos_provider }}\n\n           diff_against: intended\n           intended_config: \n{{ lookup('file', '{{filename}}') }}\n\n\n\n\n\n\n\n\n\nClick\n \nFile\n and \nSave\n . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c\nremote-sync\n\u201d package. \n\n\n\n\n\n\nBefore\n you run this playbook, SSH into leaf-4 to make some configuration change as shown below.\n\n\n\n\n\n\nLeaf-4# conf t\nEnter configuration commands, one per line. End with CNTL/Z.\nLeaf-4(config)# no router bgp 65000\nLeaf-4(config)# copy run start\n[########################################] 100%\nLeaf-4(config)# end\n\n\n\n\n\n\nOn the Ansible server (via MTputty SSH session), run the playbook for configuration compliance check by executing below command:\n\n\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook --diff verify_config.yml\n\n\n\n\n\n\nThe delta between current running config and base line config are highlighted in \nRED\n from the result\n\n\n\n\n\n\n\n\nBelow partial screenshot shows the output of above command:\n\n\n\n\n\n\n\n\nBring leaf-4 back to the baseline config by executing below command:\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml --limit=198.18.4.104\n\n\n\n\n\n\n\nBelow screenshot shows the output of above command.  You can also log into leaf-4 and verify that bgp configurations are back:\n\n\n\n\n\n\n\n\n\n\nStep 4: Add new VNI\n\n\nIn this section, we will introduce following new VNI into the VXLAN fabric. \n\n\n\n\n\n\n\n\nVLAN ID\n\n\nVLAN Name\n\n\nVNI\n\n\nIP_Add\n\n\nmask\n\n\nMcast\n\n\n\n\n\n\n\n\n\n\n200\n\n\nL2-VNI-200-Tenant1\n\n\n50200\n\n\n172.21.200.1\n\n\n24\n\n\n239.0.0.200\n\n\n\n\n\n\n201\n\n\nL2-VNI-201-Tenant1\n\n\n50201\n\n\n172.21.201.1\n\n\n24\n\n\n239.0.0.201\n\n\n\n\n\n\n\n\n\n\n\n\nFirst we will creat a new role, and name it \n\u201cvni_provision\u201d\n under folder roles using \nansible-galaxy\n using below commands on the Ansible node (using MTputty via SSH connection): \n\n\n[root@rhel7-tools LTRDCN-1572]# cd roles/\n[root@rhel7-tools roles]# ansible-galaxy init vni_provision\n\n\n\n\n\n\n\nVerify vni_provision was created successfully\n\n\n\n\n\n\nAnsible-galaxy init will create new role with base role structure and empty \nmain.yml\n file as role requires.  \n\n\n\n\n\n\nSwitch to \n\u201cAtom\u201d\n and sync the new created folders between Ansible node and remote desktop. Right click on project folder \n\u201cLTRDCN-1572\u201d\n, open \n\u201cRemote Sync\u201d\n select \n\u201cDownload Folder\u201d\n\n\n\n\n\n\n\n\nEdit variable file \nmain.yml\n for \n\u201cvni_provision\u201d\n role under \u201c\n/root/LTRDCN-1572/roles/vni_provision/vars\n\u201d and enter below data.  Make sure to click \nFile\n and \nSave\n on Atom to push this to Ansible server:\n\n\n\n\n\n\n---\n# vars file for vni_provision\n  nxos_provider:\n    username: \n{{ user }}\n\n    password: \n{{ pwd }}\n\n    transport: nxapi\n    timeout: 30\n    host: \n{{ inventory_hostname }}\n\n\n  L2VNI:\n  - { vlan_id: 200, vni: 50200, ip_add: 172.21.200.1, mask: 24, vlan_name: L2-VNI-200-Tenant1, mcast: 239.0.0.200 }\n  - { vlan_id: 201, vni: 50201, ip_add: 172.21.201.1, mask: 24, vlan_name: L2-VNI-201-Tenant1, mcast: 239.0.0.201 }\n\n\n\n\n\n\n\n\nEdit playbook file \nmail.yml\n for \n\u201cvni_provision\u201d\n role under \n\u201c/root/LTRDCN-1572/roles/vni_provision/tasks\u201d\n and enter below data.   Make sure to click \nFile\n and \nSave\n on Atom to push this to Ansible server:\n\n\n\n\n\n\nNote: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.\n \n\n\n\n\n---\n# tasks file for vni_provision\n       - name: Configure VLAN to VNI\n         nxos_vlan:\n           vlan_id: \n{{ item.vlan_id }}\n\n           mapped_vni: \n{{ item.vni }}\n\n           name: \n{{ item.vlan_name }}\n\n           provider: \n{{ nxos_provider }}\n\n         with_items:\n         - \n{{ L2VNI }}\n\n       - name: Configure L2VNI\n         nxos_interface:\n           interface: vlan\n{{ item.vlan_id }}\n\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ L2VNI }}\n\n       - name: Assign interface to Tenant VRF\n         nxos_vrf_interface:\n           vrf: Tenant-1\n           interface: \nvlan{{ item.vlan_id }}\n\n           provider: \n{{ nxos_provider }}\n\n         with_items:\n         - \n{{ L2VNI }}\n\n       - name: Configure SVI IP\n         nxos_ip_interface:\n           interface: \nvlan{{ item.vlan_id }}\n\n           addr: \n{{ item.ip_add }}\n\n           mask: \n{{ item.mask }}\n\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ L2VNI }}\n\n       - name: Configure L2VNI SVI\n         nxos_interface:\n           interface: vlan\n{{ item.vlan_id }}\n\n           fabric_forwarding_anycast_gateway: true\n           provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ L2VNI }}\n\n       - name: Configure L2VNI to VTEP\n         nxos_vxlan_vtep_vni:\n            interface: nve1\n            vni: \n{{ item.vni }}\n\n            multicast_group: \n{{ item.mcast }}\n\n            provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ L2VNI }}\n\n       - name: Configure L2VNI RD/RT\n         nxos_evpn_vni:\n          vni: \n{{ item.vni }}\n\n          route_distinguisher: auto\n          route_target_both: auto\n          provider: \n{{ nxos_provider }}\n\n         with_items: \n{{ L2VNI }}\n\n\n\n\n\nthis is shown in below screenshot:\n\n\n\n\n\n\nSwitch to \u201cAtom\u201d  create new playbook \n\u2018vni_provision.yml\u2019\n under project folder \nLTRDCN-1572\n and enter below data.  Make sure to click \nFile\n and \nSave\n on Atom to push this to Ansible server:\n\n\n\n\n---\n- hosts: leaf,jinja2_leaf\n  connection: local\n  roles:\n    - vni_provision\n\n\n\n\n\n\n\n\nRun playbook \nvni_provision.yml\n to add new VNIs on the fabric\n\n\n[root@rhel7-tools LTRDCN-1572]# ansible-playbook vni_provision.yml\n\n\n\nBelow screenshot shows the output of above command:\n\n\n\n\n\n\n\n\nSwitch to \nMTPutty\n and connect to \nleaf-4\n (SSH connection0, verify the change on leaf switches using command \nshow nve vni\n. Notice the new created L2VNI. \n\n\n\n\n\n\n\n\nCongratulation! You have completed VXLAN Fabric Lab.", 
            "title": "Day2 task"
        }, 
        {
            "location": "/task5-day2-operation/#task-5-day-2-operation-using-ansible", 
            "text": "In this section, we will use automation to perform following day 2 operation tasks.    Backup running configurations on all leaf and spine switches   Verify underlay ospf, bgp and pim neighbors   Verify overlay nve peer, host route, bgp update   Baseline configuration comparison   Add new VNIs into the existing fabric", 
            "title": "Task 5: Day 2 operation using Ansible"
        }, 
        {
            "location": "/task5-day2-operation/#step-1-backup-running-configurations", 
            "text": "In this section, you will use  ios_config  module to backup running configuration on each switch, the backup file will be saved to a local  \u201cbackup\u201d  folder.  The backup argument create a full backup of the current running-config of each switch.  The backup file is written to the backup folder in the playbook root directory. If the directory does not exist, it is created.    On  Atom , open up the project folder  \u201cLTRDCN-1572\u201d  and create new file under  LTRDCN-1572 . Name the new file  get_config.yml .    Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.     ---\n  - hosts: spine,leaf,jinja2_leaf,jinja2_spine\n    connection: local\n    vars:\n      ios_provider:\n        transport: nxapi\n        username:  {{ user }} \n        password:  {{ pwd }} \n        host:  {{ inventory_hostname }} \n    tasks:\n      - name: save running\n        nxos_config:\n           provider:  {{ ios_provider }} \n           backup: yes\n           timeout: 20    On the Ansible node (using MTputty via SSH), run  \u201cget_config.yml\u201d  playbook and verify the backup configurations in  \u201cbackup\u201d  folder by using below commands:  [root@rhel7-tools LTRDCN-1572]# ansible-playbook get_config.yml\n[root@rhel7-tools LTRDCN-1572]# ls -lrt\n[root@rhel7-tools LTRDCN-1572]# ls backup  You may further view the contents of the files under backup folder by using cat, less or more commands.  Below screenshot shows the output of above commands", 
            "title": "Step 1: Backup running configurations"
        }, 
        {
            "location": "/task5-day2-operation/#step-2-verify-underlay-and-overlay", 
            "text": "In this step, you will verify underlay and overlay operation using ansible playbook. The playbook will be applied to all leaf switches to verify the below commands:  Underlay    -   show ip ospf neighbor\n-   show ip bgp sum\n-   show ip pim neighbor  Overlay  -   show nve vni\n-   show nve peer\n-   show ip route vrf Tenant-1\n-   show bgp l2vpn evpn\n-   show l2route evpn mac-ip all   Switch to \u201cAtom\u201d,  right click  on the folder  LTRDDCN-1572  and create a new playbook named  verify_fabric.yml . Enter this file name and hit enter.   ---\n  - hosts: leaf, jinja2_leaf\n    connection: local\n    gather_facts: false\n    vars:\n      ios_provider:\n         username:  {{ user }} \n         password:  {{ pwd }} \n         timeout: 30\n         host:  {{ inventory_hostname }} \n    tasks:\n      - name: verify underlay\n        register: underlay_output\n        ios_command:\n          provider:  {{ ios_provider }} \n          commands:\n            - show ip ospf neighbors\n            - show ip bgp sum\n            - show ip pim neighbor\n        tags: underlay\n      - debug: var=underlay_output.stdout_lines\n        tags: underlay\n#      - copy: content= {{underlay_output | to_nice_json}}  dest= verify/{{inventory_hostname}}_underlay \n      - name: Verify Overlay\n        register: overlay_output\n        ios_command:\n          provider:  {{ ios_provider }} \n          commands:\n            - show nve vni\n            - show nve peer\n            - show ip route vrf Tenant-1\n            - show bgp l2vpn evpn\n            - show l2route evpn mac-ip all\n        tags: overlay\n      - debug: var=overlay_output.stdout_lines\n        tags: overlay    Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.     On the Ansible node (via MTPutty), run  verify_fabric.yml  playbook and verify the output for  underlay  by executing below command (using respective tag):   [root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags \"underlay\"     The output shows ospf, bgp and pim neighbors for all leaf switches     Below screenshot shows the partial output of above command:     Here is a log of execution of above command:   [root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags  underlay \n\nPLAY [leaf, jinja2_leaf] ************************************************************************************************************************************************\n\nTASK [verify underlay] **************************************************************************************************************************************************\n [WARNING]: argument username has been deprecated and will be removed in a future version\n\n [WARNING]: argument timeout has been deprecated and will be removed in a future version\n\n [WARNING]: argument password has been deprecated and will be removed in a future version\n\nok: [198.18.4.101]\nok: [198.18.4.104]\nok: [198.18.4.103]\n\nTASK [debug] ************************************************************************************************************************************************************\nok: [198.18.4.101] =  {\n     underlay_output.stdout_lines : [\n        [\n             OSPF Process ID 1 VRF default ,\n              Total number of neighbors: 2 ,\n              Neighbor ID     Pri State            Up Time  Address         Interface ,\n              192.168.0.6       1 FULL/ -          1d03h    10.0.0.21       Eth1/1  ,\n              192.168.0.7       1 FULL/ -          1d03h    10.0.128.5      Eth1/2 \n        ],\n        [\n             BGP summary information for VRF default, address family IPv4 Unicast ,\n             BGP router identifier 192.168.0.8, local AS number 65000 ,\n             BGP table version is 8, IPv4 Unicast config peers 2, capable peers 2 ,\n             0 network entries and 0 paths using 0 bytes of memory ,\n             BGP attribute entries [0/0], BGP AS path entries [0/0] ,\n             BGP community entries [0/0], BGP clusterlist entries [0/0] ,\n             ,\n             Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd ,\n             192.168.0.6     4 65000      52      54        8    0    0 00:24:18 0          ,\n             192.168.0.7     4 65000      47      49        8    0    0 00:30:45 0 \n        ],\n        [\n             PIM Neighbor Status for VRF \\ default\\ ,\n             Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD ,\n                                                                      Priority Capable State ,\n             10.0.0.21       Ethernet1/1          00:15:51  00:01:43  1        yes     n/a ,\n             10.0.128.5      Ethernet1/2          00:15:45  00:01:23  1        yes     n/a \n        ]\n    ]\n}\nok: [198.18.4.104] =  {\n     underlay_output.stdout_lines : [\n        [\n             OSPF Process ID 1 VRF default ,\n              Total number of neighbors: 2 ,\n              Neighbor ID     Pri State            Up Time  Address         Interface ,\n              192.168.0.6       1 FULL/ -          10:20:18 10.0.128.1      Eth1/1  ,\n              192.168.0.7       1 FULL/ -          10:20:15 10.0.128.17     Eth1/2 \n        ],\n        [\n             BGP summary information for VRF default, address family IPv4 Unicast ,\n             BGP router identifier 192.168.0.11, local AS number 65000 ,\n             BGP table version is 4, IPv4 Unicast config peers 2, capable peers 2 ,\n             0 network entries and 0 paths using 0 bytes of memory ,\n             BGP attribute entries [0/0], BGP AS path entries [0/0] ,\n             BGP community entries [0/0], BGP clusterlist entries [0/0] ,\n             ,\n             Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd ,\n             192.168.0.6     4 65000      32      32        4    0    0 00:24:03 0          ,\n             192.168.0.7     4 65000      34      34        4    0    0 00:25:16 0 \n        ],\n        [\n             PIM Neighbor Status for VRF \\ default\\ ,\n             Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD ,\n                                                                      Priority Capable State ,\n             10.0.128.1      Ethernet1/1          00:19:15  00:01:31  1        yes     n/a ,\n             10.0.128.17     Ethernet1/2          00:25:43  00:01:18  1        yes     n/a \n        ]\n    ]\n}\nok: [198.18.4.103] =  {\n     underlay_output.stdout_lines : [\n        [\n             OSPF Process ID 1 VRF default ,\n              Total number of neighbors: 2 ,\n              Neighbor ID     Pri State            Up Time  Address         Interface ,\n              192.168.0.6       1 FULL/ -          1d03h    10.0.0.29       Eth1/1  ,\n              192.168.0.7       1 FULL/ -          1d03h    10.0.128.13     Eth1/2 \n        ],\n        [\n             BGP summary information for VRF default, address family IPv4 Unicast ,\n             BGP router identifier 192.168.0.10, local AS number 65000 ,\n             BGP table version is 8, IPv4 Unicast config peers 2, capable peers 2 ,\n             0 network entries and 0 paths using 0 bytes of memory ,\n             BGP attribute entries [0/0], BGP AS path entries [0/0] ,\n             BGP community entries [0/0], BGP clusterlist entries [0/0] ,\n             ,\n             Neighbor        V    AS MsgRcvd MsgSent   TblVer  InQ OutQ Up/Down  State/PfxRcd ,\n             192.168.0.6     4 65000      52      53        8    0    0 00:24:19 0          ,\n             192.168.0.7     4 65000      47      49        8    0    0 00:30:47 0 \n        ],\n        [\n             PIM Neighbor Status for VRF \\ default\\ ,\n             Neighbor        Interface            Uptime    Expires   DR       Bidir-  BFD ,\n                                                                      Priority Capable State ,\n             10.0.0.29       Ethernet1/1          00:15:53  00:01:23  1        yes     n/a ,\n             10.0.128.13     Ethernet1/2          00:15:47  00:01:37  1        yes     n/a \n        ]\n    ]\n}\n\nPLAY RECAP **************************************************************************************************************************************************************\n198.18.4.101               : ok=2    changed=0    unreachable=0    failed=0\n198.18.4.103               : ok=2    changed=0    unreachable=0    failed=0\n198.18.4.104               : ok=2    changed=0    unreachable=0    failed=0  Next:   Run  verify_fabric.yml  playbook and verify the output for  overlay  using the respective tag in the command (as shown below):   [root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags  overlay    The output shows nve tunnel peer, host route in bgp EVPN from all leaf switches     Below screenshot of the partial output of above command:     Below shows the complete log output of execution of above playbook command.   Verify the output for  vne vni  status,  vne  dynamic neighbors, type host  mac+ip evpn route update for each L2VNI, l2fib  information.    [root@rhel7-tools LTRDCN-1572]# ansible-playbook verify_fabric.yml --tags  overlay \n\nPLAY [leaf, jinja2_leaf] **********************************************************************************************************************************************\n\nTASK [Verify Overlay] *************************************************************************************************************************************************\n [WARNING]: argument username has been deprecated and will be removed in a future version\n\n [WARNING]: argument timeout has been deprecated and will be removed in a future version\n\n [WARNING]: argument password has been deprecated and will be removed in a future version\n\nok: [198.18.4.101]\nok: [198.18.4.104]\nok: [198.18.4.103]\n\nTASK [debug] **********************************************************************************************************************************************************\nok: [198.18.4.101] =  {\n     overlay_output.stdout_lines : [\n        [\n             Codes: CP - Control Plane        DP - Data Plane           ,\n                    UC - Unconfigured         SA - Suppress ARP         ,\n                    SU - Suppress Unknown Unicast ,\n               ,\n             Interface VNI      Multicast-group   State Mode Type [BD/VRF]      Flags ,\n             --------- -------- ----------------- ----- ---- ------------------ ----- ,\n             nve1      50140    239.0.0.140       Up    CP   L2 [140]                   ,\n             nve1      50141    239.0.0.141       Up    CP   L2 [141]                   ,\n             nve1      50999    n/a               Up    CP   L3 [Tenant-1] \n        ],\n        [\n             Interface Peer-IP          State LearnType Uptime   Router-Mac        ,\n             --------- ---------------  ----- --------- -------- ----------------- ,\n             nve1      192.168.0.110    Up    CP        00:03:19 000c.2939.f53f    ,\n             nve1      192.168.0.111    Up    CP        00:01:12 000c.2951.176f \n        ],\n        [\n             IP Route Table for VRF \\ Tenant-1\\ ,\n             '*' denotes best ucast next-hop ,\n             '**' denotes best mcast next-hop ,\n             '[x/y]' denotes [preference/metric] ,\n             '% string ' in via output denotes VRF  string ,\n             ,\n             172.21.140.0/24, ubest/mbest: 1/0, attached ,\n                 *via 172.21.140.1, Vlan140, [0/0], 00:05:39, direct ,\n             172.21.140.1/32, ubest/mbest: 1/0, attached ,\n                 *via 172.21.140.1, Vlan140, [0/0], 00:05:39, local ,\n             172.21.140.10/32, ubest/mbest: 1/0, attached ,\n                 *via 172.21.140.10, Vlan140, [190/0], 00:05:33, hmm ,\n             172.21.140.11/32, ubest/mbest: 1/0 ,\n                 *via 192.168.0.110%default, [200/0], 00:01:50, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006e encap: VXLAN ,\n               ,\n             172.21.141.0/24, ubest/mbest: 1/0, attached ,\n                 *via 172.21.141.1, Vlan141, [0/0], 00:05:38, direct ,\n             172.21.141.1/32, ubest/mbest: 1/0, attached ,\n                 *via 172.21.141.1, Vlan141, [0/0], 00:05:38, local ,\n             172.21.141.11/32, ubest/mbest: 1/0 ,\n                 *via 192.168.0.111%default, [200/0], 00:01:12, bgp-65000, internal, tag 65000 (evpn) segid: 50999 tunnelid: 0xc0a8006f encap: VXLAN \n        ],\n        [\n             BGP routing table information for VRF default, address family L2VPN EVPN ,\n             BGP table version is 35, Local Router ID is 192.168.0.8 ,\n             Status: s-suppressed, x-deleted, S-stale, d-dampened, h-history, *-valid,  -best ,\n             Path type: i-internal, e-external, c-confed, l-local, a-aggregate, r-redist, I-injected ,\n             Origin codes: i - IGP, e - EGP, ? - incomplete, | - multipath,   - backup ,\n             ,\n                Network            Next Hop            Metric     LocPrf     Weight Path ,\n             Route Distinguisher: 192.168.0.8:32907    (L2VNI 50140) ,\n             * i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216 ,\n                                   192.168.0.110                     100          0 i ,\n             * l[2]:[0]:[0]:[48]:[0050.56a0.7630]:[32]:[172.21.140.10]/272 ,\n                                   192.168.0.18                      100      32768 i ,\n             * i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272 ,\n                                   192.168.0.110                     100          0 i ,\n             ,\n             Route Distinguisher: 192.168.0.8:32908    (L2VNI 50141) ,\n             * i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216 ,\n                                   192.168.0.111                     100          0 i ,\n             * i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272 ,\n                                   192.168.0.111                     100          0 i ,\n             ,\n             Route Distinguisher: 192.168.0.10:32907 ,\n             * i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[0]:[0.0.0.0]/216 ,\n                                   192.168.0.110                     100          0 i ,\n             * i                   192.168.0.110                     100          0 i ,\n             * i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272 ,\n                                   192.168.0.110                     100          0 i ,\n             * i                   192.168.0.110                     100          0 i ,\n             ,\n             Route Distinguisher: 192.168.0.11:32908 ,\n             * i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[0]:[0.0.0.0]/216 ,\n                                   192.168.0.111                     100          0 i ,\n             * i                   192.168.0.111                     100          0 i ,\n             * i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272 ,\n                                   192.168.0.111                     100          0 i ,\n             * i                   192.168.0.111                     100          0 i ,\n             ,\n             Route Distinguisher: 192.168.0.8:3    (L3VNI 50999) ,\n             * i[2]:[0]:[0]:[48]:[000c.2979.f00d]:[32]:[172.21.141.11]/272 ,\n                                   192.168.0.111                     100          0 i ,\n             * i[2]:[0]:[0]:[48]:[0050.56a0.b5d1]:[32]:[172.21.140.11]/272 ,\n                                   192.168.0.110                     100          0 i \n        ],\n        [\n             Flags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link  ,\n             (Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear ,\n             (Ps):Peer Sync (Ro):Re-Originated  ,\n             Topology    Mac Address    Prod   Flags         Seq No     Host IP         Next-Hops       ,\n             ----------- -------------- ------ ---------- --------------- --------------- ,\n             140         0050.56a0.7630 HMM    --            0          172.21.140.10  Local           ,\n             140         0050.56a0.b5d1 BGP    --            0          172.21.140.11  192.168.0.110   ,\n             141         000c.2979.f00d BGP    --            1          172.21.141.11  192.168.0.111 \n        ]\n    ]\n}", 
            "title": "Step 2: Verify underlay and overlay"
        }, 
        {
            "location": "/task5-day2-operation/#step-3-baseline-configuration-comparison", 
            "text": "In this section we will compare the running configuration with baseline configuration for configuration compliance check. The configuration file that we backed in tak 1 will be used as baseline configuration.   In this playbook, you will use  \u201clookup\u201d  module to find the backup filename generated in Step 1. Then you will use  diff_against  function in  nxos_config  module to compare running configuration.    On Atom, Open up the project folder  LTRDCN-1572  and create new file under  \u201cLTRDCN-1572\u201d . Name the new file  verify_config.yml  and enter below data in this playbook:   ---\n   - hosts: jinja2_leaf,leaf,jinja2_spine,spine\n     connection: local\n     gather_facts: flase\n     vars:\n       nxos_provider:\n          transport: nxapi\n          username:  {{ user }} \n          password:  {{ pwd }} \n          timeout: 30\n          host:  {{ inventory_hostname }} \n       filename:  {{ lookup('pipe', 'ls backup/{{ inventory_hostname}}_config.*')}} \n     tasks:\n       - name: configure compliance\n         register: diff_config\n#         when: (inventory_hostname in groups['leaf']) or (inventory_hostname in groups['jinja2_leaf'])\n         nxos_config:\n           provider:  {{ nxos_provider }} \n           diff_against: intended\n           intended_config:  {{ lookup('file', '{{filename}}') }}     Click   File  and  Save  . This will save the playbook, and also ftp the playbook to Ansible server using pre-configured \u201c remote-sync \u201d package.     Before  you run this playbook, SSH into leaf-4 to make some configuration change as shown below.    Leaf-4# conf t\nEnter configuration commands, one per line. End with CNTL/Z.\nLeaf-4(config)# no router bgp 65000\nLeaf-4(config)# copy run start\n[########################################] 100%\nLeaf-4(config)# end   On the Ansible server (via MTputty SSH session), run the playbook for configuration compliance check by executing below command:   [root@rhel7-tools LTRDCN-1572]# ansible-playbook --diff verify_config.yml   The delta between current running config and base line config are highlighted in  RED  from the result     Below partial screenshot shows the output of above command:     Bring leaf-4 back to the baseline config by executing below command:  [root@rhel7-tools LTRDCN-1572]# ansible-playbook jinja2_fabric.yml --limit=198.18.4.104    Below screenshot shows the output of above command.  You can also log into leaf-4 and verify that bgp configurations are back:", 
            "title": "Step 3: Baseline configuration comparison"
        }, 
        {
            "location": "/task5-day2-operation/#step-4-add-new-vni", 
            "text": "In this section, we will introduce following new VNI into the VXLAN fabric.      VLAN ID  VLAN Name  VNI  IP_Add  mask  Mcast      200  L2-VNI-200-Tenant1  50200  172.21.200.1  24  239.0.0.200    201  L2-VNI-201-Tenant1  50201  172.21.201.1  24  239.0.0.201       First we will creat a new role, and name it  \u201cvni_provision\u201d  under folder roles using  ansible-galaxy  using below commands on the Ansible node (using MTputty via SSH connection):   [root@rhel7-tools LTRDCN-1572]# cd roles/\n[root@rhel7-tools roles]# ansible-galaxy init vni_provision    Verify vni_provision was created successfully    Ansible-galaxy init will create new role with base role structure and empty  main.yml  file as role requires.      Switch to  \u201cAtom\u201d  and sync the new created folders between Ansible node and remote desktop. Right click on project folder  \u201cLTRDCN-1572\u201d , open  \u201cRemote Sync\u201d  select  \u201cDownload Folder\u201d     Edit variable file  main.yml  for  \u201cvni_provision\u201d  role under \u201c /root/LTRDCN-1572/roles/vni_provision/vars \u201d and enter below data.  Make sure to click  File  and  Save  on Atom to push this to Ansible server:    ---\n# vars file for vni_provision\n  nxos_provider:\n    username:  {{ user }} \n    password:  {{ pwd }} \n    transport: nxapi\n    timeout: 30\n    host:  {{ inventory_hostname }} \n\n  L2VNI:\n  - { vlan_id: 200, vni: 50200, ip_add: 172.21.200.1, mask: 24, vlan_name: L2-VNI-200-Tenant1, mcast: 239.0.0.200 }\n  - { vlan_id: 201, vni: 50201, ip_add: 172.21.201.1, mask: 24, vlan_name: L2-VNI-201-Tenant1, mcast: 239.0.0.201 }    Edit playbook file  mail.yml  for  \u201cvni_provision\u201d  role under  \u201c/root/LTRDCN-1572/roles/vni_provision/tasks\u201d  and enter below data.   Make sure to click  File  and  Save  on Atom to push this to Ansible server:    Note: It is recommended to write your playbook and learn from mistakes, but file is also available in the box folder if you prefer to reuse.     ---\n# tasks file for vni_provision\n       - name: Configure VLAN to VNI\n         nxos_vlan:\n           vlan_id:  {{ item.vlan_id }} \n           mapped_vni:  {{ item.vni }} \n           name:  {{ item.vlan_name }} \n           provider:  {{ nxos_provider }} \n         with_items:\n         -  {{ L2VNI }} \n       - name: Configure L2VNI\n         nxos_interface:\n           interface: vlan {{ item.vlan_id }} \n           provider:  {{ nxos_provider }} \n         with_items:  {{ L2VNI }} \n       - name: Assign interface to Tenant VRF\n         nxos_vrf_interface:\n           vrf: Tenant-1\n           interface:  vlan{{ item.vlan_id }} \n           provider:  {{ nxos_provider }} \n         with_items:\n         -  {{ L2VNI }} \n       - name: Configure SVI IP\n         nxos_ip_interface:\n           interface:  vlan{{ item.vlan_id }} \n           addr:  {{ item.ip_add }} \n           mask:  {{ item.mask }} \n           provider:  {{ nxos_provider }} \n         with_items:  {{ L2VNI }} \n       - name: Configure L2VNI SVI\n         nxos_interface:\n           interface: vlan {{ item.vlan_id }} \n           fabric_forwarding_anycast_gateway: true\n           provider:  {{ nxos_provider }} \n         with_items:  {{ L2VNI }} \n       - name: Configure L2VNI to VTEP\n         nxos_vxlan_vtep_vni:\n            interface: nve1\n            vni:  {{ item.vni }} \n            multicast_group:  {{ item.mcast }} \n            provider:  {{ nxos_provider }} \n         with_items:  {{ L2VNI }} \n       - name: Configure L2VNI RD/RT\n         nxos_evpn_vni:\n          vni:  {{ item.vni }} \n          route_distinguisher: auto\n          route_target_both: auto\n          provider:  {{ nxos_provider }} \n         with_items:  {{ L2VNI }}   this is shown in below screenshot:    Switch to \u201cAtom\u201d  create new playbook  \u2018vni_provision.yml\u2019  under project folder  LTRDCN-1572  and enter below data.  Make sure to click  File  and  Save  on Atom to push this to Ansible server:   ---\n- hosts: leaf,jinja2_leaf\n  connection: local\n  roles:\n    - vni_provision    Run playbook  vni_provision.yml  to add new VNIs on the fabric  [root@rhel7-tools LTRDCN-1572]# ansible-playbook vni_provision.yml  Below screenshot shows the output of above command:     Switch to  MTPutty  and connect to  leaf-4  (SSH connection0, verify the change on leaf switches using command  show nve vni . Notice the new created L2VNI.", 
            "title": "Step 4: Add new VNI"
        }, 
        {
            "location": "/task5-day2-operation/#congratulation-you-have-completed-vxlan-fabric-lab", 
            "text": "", 
            "title": "Congratulation! You have completed VXLAN Fabric Lab."
        }, 
        {
            "location": "/appendixA-F5/", 
            "text": "Appendix A: L4-L7 insertion\n\n\nIn this section, you will insert F5 BIG-IP load balancer into the fabric. \n- First send out a http request from server-4 to a VIP, and you will notice no http service enabled on VIP. \n- Run the playbook to enable http service on the VIP with server-1, server-2, server-3 and server-4 in the server pool.\n- After successful execute the playbook, you will notice http request to VIP is load balanced across three servers. \n\n\n\n\nStep 1: Power on F5 Virtual Machine\n\n\n\n\n\n\nopen \nVMware vSphere Client\n to login ESXi \n(198.18.133.33)\n using crendential \nroot/C1sco12345\n\n\n\n\n\n\n\n\nOpen up the VM under host \n198.18.133.33\n, and power on VM \nF5_LTM\n as shown below:\n\n\n\n\n\n\n\n\n\n\nStep 2: Install Ansible pre-reqs\n\n\n\n\nSwitch to \nMTPuTTY\n install prerequisites packages on \nAnsible node\n by using below \npip install\n commands:\n[root@rhel7-tools LTRDCN-1572]# pip install f5-sdk\n[root@rhel7-tools LTRDCN-1572]# pip install setuptools --upgrade\n[root@rhel7-tools LTRDCN-1572]# pip install bigsuds\n[root@rhel7-tools LTRDCN-1572]# pip install netaddr\n\n\n\n\n\n\n\n\n\nStep 3: Check curl\n\n\n\n\nSwitch to \n\u2018MTPuTTY\u2019\n ssh into server-4 and send http request to VIP 172.21.140.100 from server-4 using \ncurl\n commands as shown below:\n[root@server-4 ~]# curl http://172.21.140.100\ncurl: (7) couldn't connect to host`\n[root@server-4 ~]#\n\n\n\n\n\n\n\n\n\nStep 4:\n\n\n\n\nSwitch to Atom, create new file name \n\u2018f5ltm.yml\u2019\n under project folder \nLTRDCN-1572\n and enter below data.  Make sure to click \nFile\n and \nSave\n to ftp this data to Ansible server:\n\n\n\n\n---\n- name: Configurating BIG-IP\n  hosts: localhost\n  gather_facts: false\n\n  tasks:\n  - name: Configure server facing port to L2\n    nxos_interface:\n      interface: eth1/4\n      mode: layer2\n      username: \n{{ user }}\n\n      password: \n{{ pwd }}\n\n      transport: nxapi\n      host: \n198.18.4.104\n\n  - name: Configure VLAN for F5 port\n    nxos_switchport:\n      interface: eth1/4\n      mode: access\n      access_vlan: 140\n      username: \n{{ user }}\n\n      password: \n{{ pwd }}\n\n      transport: nxapi\n      host: \n198.18.4.104\n\n  - name: Configure VLANs on the BIG-IP\n    bigip_vlan:\n     server: \n198.18.4.10\n\n     user: \nadmin\n\n     password: \nadmin\n\n     validate_certs: False\n     name: \nExternal\n\n     tag: \n140\n\n     untagged_interface: \n1.1\n\n     validate_certs: \nno\n\n    delegate_to: localhost\n  - name: Configure SELF-IPs on the BIG-IP\n    bigip_selfip:\n     server: \n198.18.4.10\n\n     user: \nadmin\n\n     password: \nadmin\n\n     validate_certs: False\n     name: \n172.21.140.50\n\n     address: \n172.21.140.50\n\n     netmask: \n255.255.255.0\n\n     vlan: \nExternal\n\n     allow_service: \ndefault\n\n     validate_certs: \nno\n\n    delegate_to: localhost\n  - name: Create static route\n    bigip_static_route:\n      server: \n198.18.4.10\n\n      user: \nadmin\n\n      password: \nadmin\n\n      validate_certs: False\n      destination: 0.0.0.0\n      netmask: 0.0.0.0\n      gateway_address: 172.21.140.1\n      name: \ndefult\n\n    delegate_to: localhost\n  - name: Create nodes\n    bigip_node:\n      server: \n198.18.4.10\n\n      user: \nadmin\n\n      password: \nadmin\n\n      host: \n{{item}}\n\n      name: \n{{item}}\n\n      validate_certs: \nno\n\n    with_items:\n      - 172.21.140.10\n      - 172.21.140.11\n      - 172.21.141.10\n      - 172.21.141.11\n    delegate_to: localhost\n\n  - name: Create pool\n    bigip_pool:\n      server: \n198.18.4.10\n\n      user: \nadmin\n\n      password: \nadmin\n\n      name: \nweb-pool\n\n      lb_method: \nround-robin\n\n      monitors: \n/Common/http\n\n      monitor_type: \nand_list\n\n      validate_certs: \nno\n\n    delegate_to: localhost\n\n  - name: Add Pool members\n    bigip_pool_member:\n      server: \n198.18.4.10\n\n      user: \nadmin\n\n      password: \nadmin\n\n      name: \n{{item}}\n\n      host: \n{{item}}\n\n      port: \n80\n\n      pool: \nweb-pool\n\n      validate_certs: \nno\n\n    with_items:\n      - 172.21.140.10\n      - 172.21.140.11\n      - 172.21.141.10\n      - 172.21.141.11\n    delegate_to: localhost\n\n  - name: Add Virtual Server\n    bigip_virtual_server:\n      server: \n198.18.4.10\n\n      user: \nadmin\n\n      password: \nadmin\n\n      name: \nhttp-virtualserver\n\n      destination: \n172.21.140.100\n\n      port: \n80\n\n      enabled_vlans: \nALL\n\n      all_profiles:\n      - http\n      pool: \nweb-pool\n\n      snat: \nAutomap\n\n      validate_certs: \nno\n\n    delegate_to: localhost\n\n\n\n\n\n\nStep 5: Run playbook\n\n\n\n\nOn the \nAnsible node\n (using MTputty SSH), run playbook \nf5ltm.yml\n to provision VIP (172.21.140.100) on F5 and also put all four servers into the server pool using \nansible-playbook f5ltm.yml\n: \n\n\n\n\n \n\n\n\n\nStep 6: Check load balancer\n\n\n\n\nSwitch to \n\u2018MTPuTTY\u2019\n and login server-4, and run \ncurl http://172.21.140.100\n command multiple times, you will see the request is load balanced to differernt servers in the sever pool. \n\n\n\n\n[root@server-4 ~]# curl http://172.21.140.100\n\nhtml\nbody\nh1\nIt works!\n/h1\n\n\np\n Server-1 172.21.140.10 \n/p\n\n\np\nThis is the default web page for this server.\n/p\n\n\np\nThe web server software is running but no content has been added, yet.\n/p\n\n\n/body\n/html\n\n\n[root@server-4 ~]# curl http://172.21.140.100\n\nhtml\nbody\nh1\nIt works!\n/h1\n\n\np\n Server-3 172.21.140.11 \n/p\n\n\np\nThis is the default web page for this server.\n/p\n\n\np\nThe web server software is running but no content has been added, yet.\n/p\n\n\n/body\n/html\n\n\n[root@server-4 ~]# curl http://172.21.140.100\n\nhtml\nbody\nh1\nIt works!\n/h1\n\n\np\n Server-4 172.21.141.11 \n/p\n\n\np\nThis is the default web page for this server.\n/p\n\n\np\nThe web server software is running but no content has been added, yet.\n/p\n\n\n/body\n/html", 
            "title": "F5"
        }, 
        {
            "location": "/appendixA-F5/#appendix-a-l4-l7-insertion", 
            "text": "In this section, you will insert F5 BIG-IP load balancer into the fabric. \n- First send out a http request from server-4 to a VIP, and you will notice no http service enabled on VIP. \n- Run the playbook to enable http service on the VIP with server-1, server-2, server-3 and server-4 in the server pool.\n- After successful execute the playbook, you will notice http request to VIP is load balanced across three servers.", 
            "title": "Appendix A: L4-L7 insertion"
        }, 
        {
            "location": "/appendixA-F5/#step-1-power-on-f5-virtual-machine", 
            "text": "open  VMware vSphere Client  to login ESXi  (198.18.133.33)  using crendential  root/C1sco12345     Open up the VM under host  198.18.133.33 , and power on VM  F5_LTM  as shown below:", 
            "title": "Step 1: Power on F5 Virtual Machine"
        }, 
        {
            "location": "/appendixA-F5/#step-2-install-ansible-pre-reqs", 
            "text": "Switch to  MTPuTTY  install prerequisites packages on  Ansible node  by using below  pip install  commands: [root@rhel7-tools LTRDCN-1572]# pip install f5-sdk\n[root@rhel7-tools LTRDCN-1572]# pip install setuptools --upgrade\n[root@rhel7-tools LTRDCN-1572]# pip install bigsuds\n[root@rhel7-tools LTRDCN-1572]# pip install netaddr", 
            "title": "Step 2: Install Ansible pre-reqs"
        }, 
        {
            "location": "/appendixA-F5/#step-3-check-curl", 
            "text": "Switch to  \u2018MTPuTTY\u2019  ssh into server-4 and send http request to VIP 172.21.140.100 from server-4 using  curl  commands as shown below: [root@server-4 ~]# curl http://172.21.140.100\ncurl: (7) couldn't connect to host`\n[root@server-4 ~]#", 
            "title": "Step 3: Check curl"
        }, 
        {
            "location": "/appendixA-F5/#step-4", 
            "text": "Switch to Atom, create new file name  \u2018f5ltm.yml\u2019  under project folder  LTRDCN-1572  and enter below data.  Make sure to click  File  and  Save  to ftp this data to Ansible server:   ---\n- name: Configurating BIG-IP\n  hosts: localhost\n  gather_facts: false\n\n  tasks:\n  - name: Configure server facing port to L2\n    nxos_interface:\n      interface: eth1/4\n      mode: layer2\n      username:  {{ user }} \n      password:  {{ pwd }} \n      transport: nxapi\n      host:  198.18.4.104 \n  - name: Configure VLAN for F5 port\n    nxos_switchport:\n      interface: eth1/4\n      mode: access\n      access_vlan: 140\n      username:  {{ user }} \n      password:  {{ pwd }} \n      transport: nxapi\n      host:  198.18.4.104 \n  - name: Configure VLANs on the BIG-IP\n    bigip_vlan:\n     server:  198.18.4.10 \n     user:  admin \n     password:  admin \n     validate_certs: False\n     name:  External \n     tag:  140 \n     untagged_interface:  1.1 \n     validate_certs:  no \n    delegate_to: localhost\n  - name: Configure SELF-IPs on the BIG-IP\n    bigip_selfip:\n     server:  198.18.4.10 \n     user:  admin \n     password:  admin \n     validate_certs: False\n     name:  172.21.140.50 \n     address:  172.21.140.50 \n     netmask:  255.255.255.0 \n     vlan:  External \n     allow_service:  default \n     validate_certs:  no \n    delegate_to: localhost\n  - name: Create static route\n    bigip_static_route:\n      server:  198.18.4.10 \n      user:  admin \n      password:  admin \n      validate_certs: False\n      destination: 0.0.0.0\n      netmask: 0.0.0.0\n      gateway_address: 172.21.140.1\n      name:  defult \n    delegate_to: localhost\n  - name: Create nodes\n    bigip_node:\n      server:  198.18.4.10 \n      user:  admin \n      password:  admin \n      host:  {{item}} \n      name:  {{item}} \n      validate_certs:  no \n    with_items:\n      - 172.21.140.10\n      - 172.21.140.11\n      - 172.21.141.10\n      - 172.21.141.11\n    delegate_to: localhost\n\n  - name: Create pool\n    bigip_pool:\n      server:  198.18.4.10 \n      user:  admin \n      password:  admin \n      name:  web-pool \n      lb_method:  round-robin \n      monitors:  /Common/http \n      monitor_type:  and_list \n      validate_certs:  no \n    delegate_to: localhost\n\n  - name: Add Pool members\n    bigip_pool_member:\n      server:  198.18.4.10 \n      user:  admin \n      password:  admin \n      name:  {{item}} \n      host:  {{item}} \n      port:  80 \n      pool:  web-pool \n      validate_certs:  no \n    with_items:\n      - 172.21.140.10\n      - 172.21.140.11\n      - 172.21.141.10\n      - 172.21.141.11\n    delegate_to: localhost\n\n  - name: Add Virtual Server\n    bigip_virtual_server:\n      server:  198.18.4.10 \n      user:  admin \n      password:  admin \n      name:  http-virtualserver \n      destination:  172.21.140.100 \n      port:  80 \n      enabled_vlans:  ALL \n      all_profiles:\n      - http\n      pool:  web-pool \n      snat:  Automap \n      validate_certs:  no \n    delegate_to: localhost", 
            "title": "Step 4:"
        }, 
        {
            "location": "/appendixA-F5/#step-5-run-playbook", 
            "text": "On the  Ansible node  (using MTputty SSH), run playbook  f5ltm.yml  to provision VIP (172.21.140.100) on F5 and also put all four servers into the server pool using  ansible-playbook f5ltm.yml :", 
            "title": "Step 5: Run playbook"
        }, 
        {
            "location": "/appendixA-F5/#step-6-check-load-balancer", 
            "text": "Switch to  \u2018MTPuTTY\u2019  and login server-4, and run  curl http://172.21.140.100  command multiple times, you will see the request is load balanced to differernt servers in the sever pool.    [root@server-4 ~]# curl http://172.21.140.100 html body h1 It works! /h1  p  Server-1 172.21.140.10  /p  p This is the default web page for this server. /p  p The web server software is running but no content has been added, yet. /p  /body /html \n\n[root@server-4 ~]# curl http://172.21.140.100 html body h1 It works! /h1  p  Server-3 172.21.140.11  /p  p This is the default web page for this server. /p  p The web server software is running but no content has been added, yet. /p  /body /html \n\n[root@server-4 ~]# curl http://172.21.140.100 html body h1 It works! /h1  p  Server-4 172.21.141.11  /p  p This is the default web page for this server. /p  p The web server software is running but no content has been added, yet. /p  /body /html", 
            "title": "Step 6: Check load balancer"
        }, 
        {
            "location": "/appendixB-Upgrade/", 
            "text": "Appendix B: Software compliance check and remediation\n\n\nIn this section, we will run software version compliance check using Ansible. For fabric switch that is not running on standard software version, we will perform software upgrade and bring all fabric switches into the standard version.  \n\n\nIn this playbook, we will use \u201cnxos_facts\u201d to find the software version on each fabric switch. Then we will compare with standard software version, 7.0(3)I7(4) in this lab. For fabric switch that is not running on standard version, the playbook will upgrade and reboot the switch. \n\n\nOn\nAtom,\n open up the project folder \u201cLTRDCN-1572\u201d and create new file under \u201cLTRDCN-1572\u201d. Name the new file \n\u201ccode_upgrade.yml\u201d.\n \n\n\n---\n#Appendix code upgrade\n  - hosts: spine,leaf,jinja2_leaf,jinja2_spine\n    vars:\n      - standard: 7.0(3)I7(4)\n      - image_file: nxos.7.0.3.I7.4.bin\n      - nxos_provider:\n          username: \n{{ user }}\n\n          password: \n{{ pwd }}\n\n          transport: network_cli\n          timeout: 30\n          host: \n{{ inventory_hostname }}\n\n    tasks:\n      - name: \nsoftware complaince check\n\n        nxos_facts:\n          gather_subset: all\n          provider: \n{{nxos_provider}}\n\n      - name: \nchange to standard code\n\n        block:\n          - debug: msg=\n{{ansible_net_hostname}} is not running standard {{standard}}\n\n          - nxos_feature:\n              feature: scp-server\n              provider: \n{{nxos_provider}}\n\n              state: enabled\n          - name: \nupload image file\n\n            nxos_file_copy:\n              local_file: \n/root/downloads/{{image_file}}\n\n              provider: \n{{nxos_provider}}\n\n          - name: \nchange boot statement\n\n            nxos_config:\n              lines: boot nxos bootflash:{{image_file}}\n              save_when: modified\n              provider: \n{{nxos_provider}}\n\n          - name: \nreload switch\n\n            ios_command:\n              commands:\n                - command: reload\n                  prompt: '(y/n)?'\n                  answer: 'y'\n              username: \n{{ user }}\n\n              password: \n{{ pwd }}\n\n        when: ansible_net_version != standard\n        rescue:\n          - debug:\n              msg: \n{{ansible_net_hostname}} is reloading\n\n          - name: Wait For Device To Come Back Up\n            wait_for:\n              port: 22\n              state: started\n              timeout: 900\n              delay: 60\n              host: \n{{ inventory_hostname }}\n\n        always:\n          - debug:\n              msg: \nAll devices are running {{standard}}\n\n\n\n\n\nOn the Ansible server, run the playbook for software compliance check and code upgrade. \n\n\nIt is expected to see timeout error message when playbook reloads the switch.\n\n\n\n\nIt is expected to see timeout error message when playbook reloads the switch.\n\n\n\n\n\n\n\n\nSwitch will take 20 mins to bootup; up to this point, you have completed all tasks.* \n\n\n\n\nCongratulation! You have completed the whole lab.", 
            "title": "code upgrade"
        }, 
        {
            "location": "/appendixB-Upgrade/#appendix-b-software-compliance-check-and-remediation", 
            "text": "In this section, we will run software version compliance check using Ansible. For fabric switch that is not running on standard software version, we will perform software upgrade and bring all fabric switches into the standard version.    In this playbook, we will use \u201cnxos_facts\u201d to find the software version on each fabric switch. Then we will compare with standard software version, 7.0(3)I7(4) in this lab. For fabric switch that is not running on standard version, the playbook will upgrade and reboot the switch.   On Atom,  open up the project folder \u201cLTRDCN-1572\u201d and create new file under \u201cLTRDCN-1572\u201d. Name the new file  \u201ccode_upgrade.yml\u201d.    ---\n#Appendix code upgrade\n  - hosts: spine,leaf,jinja2_leaf,jinja2_spine\n    vars:\n      - standard: 7.0(3)I7(4)\n      - image_file: nxos.7.0.3.I7.4.bin\n      - nxos_provider:\n          username:  {{ user }} \n          password:  {{ pwd }} \n          transport: network_cli\n          timeout: 30\n          host:  {{ inventory_hostname }} \n    tasks:\n      - name:  software complaince check \n        nxos_facts:\n          gather_subset: all\n          provider:  {{nxos_provider}} \n      - name:  change to standard code \n        block:\n          - debug: msg= {{ansible_net_hostname}} is not running standard {{standard}} \n          - nxos_feature:\n              feature: scp-server\n              provider:  {{nxos_provider}} \n              state: enabled\n          - name:  upload image file \n            nxos_file_copy:\n              local_file:  /root/downloads/{{image_file}} \n              provider:  {{nxos_provider}} \n          - name:  change boot statement \n            nxos_config:\n              lines: boot nxos bootflash:{{image_file}}\n              save_when: modified\n              provider:  {{nxos_provider}} \n          - name:  reload switch \n            ios_command:\n              commands:\n                - command: reload\n                  prompt: '(y/n)?'\n                  answer: 'y'\n              username:  {{ user }} \n              password:  {{ pwd }} \n        when: ansible_net_version != standard\n        rescue:\n          - debug:\n              msg:  {{ansible_net_hostname}} is reloading \n          - name: Wait For Device To Come Back Up\n            wait_for:\n              port: 22\n              state: started\n              timeout: 900\n              delay: 60\n              host:  {{ inventory_hostname }} \n        always:\n          - debug:\n              msg:  All devices are running {{standard}}   On the Ansible server, run the playbook for software compliance check and code upgrade.   It is expected to see timeout error message when playbook reloads the switch.   It is expected to see timeout error message when playbook reloads the switch.     Switch will take 20 mins to bootup; up to this point, you have completed all tasks.*", 
            "title": "Appendix B: Software compliance check and remediation"
        }, 
        {
            "location": "/appendixB-Upgrade/#congratulation-you-have-completed-the-whole-lab", 
            "text": "", 
            "title": "Congratulation! You have completed the whole lab."
        }
    ]
}